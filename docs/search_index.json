[["index.html", "R Course Chapter 1 Introduction to R and Basic Programming Concepts 1.1 Overview of R Programming 1.2 Introduction to the R ecosystem 1.3 Setting Up R Environment 1.4 Hands-on Exercise 1.5 Solution", " R Course YOUR NAME HERE 2024-10-03 Chapter 1 Introduction to R and Basic Programming Concepts 1.1 Overview of R Programming 1.1.1 Introduction to R R is a programming language particularly designed for statistics and data analysis. It was invented the statisticians Robert Gentleman and Ross Ihaka in 1990 and now its the one of the most widely used in data science community. The language is open-source, this has allowed extensive customization and adaptability for research and data analysis. Here are some of the Key features of R; Statistical Computing: R has various tools specifically for statistics like time-series, clustering, classification ,and linear and non-linear models. Data Wrangling and Analysis: R has extensive libraries like dplyr and ggplot for data manipulation and visualizations respectively. Users are able to clean, transform and present the data inform of charts in a meaningful way for better insights with the help of these libraries. Reproducibility: R language allows reproducible research such that the code and the results can be stored inform of scripts, markdown, notebooks and bookdown. Work done in R can be easily shared and reviewed enabling easy collaboration and transparency in work. Comprehensive Ecosystem: R has the CRAN(Comprehensive R Archive Network) that hosts numerous packages for different tasks like machine learning, data visualization, finances, bioinformatics and more. Versatility: R can be used with other tools like other programming languages, for instance Python, SQL etc. This allows seamless integration in the wide range of data ecosystems. 1.1.2 Importance of R in Data Analysis This are some of the features that sets R apart in the data community; Large Community and Support: There is a large online community for data scientist and other researchers who use R. The community has contributed to the development of various packages , continuous improvements, support and extensive documentation. This has made it possible and easy for new users and experienced users to find solution for the complex tasks. Versatility: Data Analysis in R is applicable in various industries like health, finance, government and academia. This allows it to handle diverse data types and analysis needs. Specialization on Statistical Analysis: Most programming languages were designed to be general purpose but R was designed specifically for statistics. This ensures its users have access to the most relevant and important tools in the data analysis tasks. Advanced Visualization: R has one of the best visualization capabilities like dashboards apps, interactive charts. Also, it can be used to generate the most aesthetically pleasing chart crucial for effective communication and decision making. 1.1.3 Application of R in Various Industries R is widely applicable in various industries due to its robust statistical capabilities, open source nature and the extensive library how is important in different industries; R is the go-to tool in the field of academia and research especially in data analysis, social and environmental science research(psychology, economics and climate change), reproducible research and statistical research. In the Manufacturing industries, R can be used to optimize operations like supply chain, quality control and cost reduction. The Governments can use R for data driven research like demographics, economic policy analysis, health and social policy.Forecasts can be made using R that will enable the research. The finance and banking industries use R for complex tasks like algorithmic trading, portfolio optimization, credit scoring and risk management. R is applicable is essential in the health industry in areas like clinical trials and bioinformatics, tracking the spread of diseases(epidemiology) and help in health policy making. R is also applicable in many other industries like marketing, sports and entertainment to name but a few. As industries continue to embrace data-driven strategies, R’s role in analytics and decision-making will only continue to grow. 1.2 Introduction to the R ecosystem 1.2.1 R Studio Rstudio is an integrated development environment(IDE) for R. It includes a console, syntax-highlighting editor that supports direct code execution as well as tools for plotting history, debugging and work space management. R studio is an open source software from posit and can be freely be downloaded from https://posit.co/download/rstudio-desktop/. This link has all the installation files for Mac, Linux and Windows. You will download the installation file and install based on your computer operating system. 1.2.2 CRAN CRAN in full is Comprehensive R Archive Network that is a central repository for R and its packages. It stores a collection R function data and compiled code. It is an essential part in R and acts as a hub due to the open source nature of R. It supports the growth and adoption of R by making powerful tools easily accessible to users and developers alike. The CRAN repo can be accessed here. 1.3 Setting Up R Environment 1.3.1 Installation of R and RStudio Follow the steps below to download and install R; Visit here to visit the CRAN web interface. Download the precompiled binaries according to your computer operating system If you are using linux(Debian/Ubuntu) run the below command to install R on the terminal. sudo apt update sudo apt upgrade sudo apt install r-base Open the downloaded R file and follow instructions to install R on Mac and Window. After installing R on Mac, run the below command on terminal to verify its installation. R q() to quit the R console on the terminal. The process of R studio installation is essentially the same as R;- Visit the R studio downloads page to download R according to your computer’s operating system. For linux, move the downloaded file to the home/&lt;user&gt; directory and open the terminal. Run sudo dpkg -i rstudio-1.2.5033-amd64.deb to install RStudio. Remember to replace the rstudio-1.2.5033-amd64.deb with the downloaded filepath. On Windows and Mac, open the downloaded the downloaded file, open it and follow the instructions to install it. Open the RStudio and DO YOUR MAGIC!!! 1.3.2 Introduction to RStudio Interface The image above is for R code in R Studio. Each quadrant has its own function. text editor: serves as the primary interface for writing and editing R scripts, Markdown documents, and other text-based files stand input/output (console/terminal/jobs): It is the bottom left quadrant in R Studio that serves as a command-line interface where you can directly interact with R such as code execution and output display. all data and its properties quadrants: It is the top right quadrant in Rstudio that is typically the “Environment” pane, which provides information and tools for managing your R environment such as data importation, display of variables. plots and file directory structure: The bottom right quadrant in RStudio typically houses the “Files”, “Plots”, “Packages”, “Help”, and “Viewer” tabs. 1.3.3 Basic R Syntax and Commands R syntax has specific rules that govern how code is written in R. Understanding these concepts makes a programmer to write clear and error-free code. These are some of the syntax rules that govern how code is written in R. Case sensitivity The R syntax distinguishes between the uppercase letter and the lowercase letter. A variable written in uppercase is different as the variable written in lowercase even the words mean the same. For instance let’s take the variable “age” and store it 15, and another variable “AGE” and store it 27 and see their outputs. # Different variables due to case sensitivity age &lt;- 15 AGE &lt;- 27 print(age) # Outputs: 15 ## [1] 15 print(AGE) # Outputs: 27 ## [1] 27 You can see that above here the variable age and AGE are treated as different. Assignment Operators R has several different methods to assign a value to a variable. They include; Equal Sign(‘=’): This assignment operator is always used when assigning the function arguments but can used in general assignments. age = 21 #variable `age` is assigned to 21 Left Arrow(‘&lt;-’): This is the common type of assignment operator. The direction of the arrow points to the variable age &lt;- 10 # variable &#39;age&#39; assigned to 10 Right Arrow(‘-&gt;’): This is just like the right arrow operator however the direction of the arrow is reversed. The arrow should still point to the direction of the variable. 27 -&gt; age # arrow points to the direction of the variable, age Use of symbols The R language has several symbols that have specific meaning important when writing code. The Hash(‘#’): Is used for comments. R ignores any text that follows # in the same line # This is a comment Dollar Sign (’$`): Used to access elements of a list or column values in a data frame df &lt;- data.frame(a = 1:3, b = 4:6) df$a # Accesses column &#39;a&#39; in the data frame ## [1] 1 2 3 Square brackets ([]): They are used for indexing vectors, lists, matrices and data frames v &lt;- c(10, 20, 30) v[3] # Accesses the third element (20) ## [1] 30 Curly braces ({}): Used to group multiple expressions, for instance in loops and conditional statements. In this case we will use a if else conditional statement to show how curly braces are used x &lt;- 2 if (x &gt; 0) { print(&quot;Positive number&quot;) } ## [1] &quot;Positive number&quot; Left arrow (‘&lt;-’) and the right arrow (‘-&gt;’): As mentioned earlier, these are assignment operators Brackets ‘()’: Used for function calls and grouping expressions sum(1, 2, 3) ## [1] 6 There are more symbols used in R especially in mathematical operators and advanced concepts that will be later introduced in the course. Reserved Words R has set of reserved words that have special meaning like identifiers and function names. These words cannot be used as variable names. if, else, repeat, while, function, TRUE, FALSE, NA and NULL are just but a few examples of reserved words. Whitespace R generally ignores whitespace (spaces, tabs, newlines) between elements, except within character strings or where it would change the meaning of the code. Proper use of whitespace improves code readability. x &lt;- 5 + 3 # Valid y &lt;-5+3 # Also valid, but less readable 1.4 Hands-on Exercise Solve the following equations; 34 + 54 76 - 10 25 * 25 Assign variable d and f to values 42 and 14 respectively and solve d/f 1.5 Solution Solve the following equations; 34 + 54 76 - 10 25 * 25 # a. `34 + 54` 34 + 54 ## [1] 88 # b. `76 - 10` 76-10 ## [1] 66 # c. `25 * 25` 25 * 25 ## [1] 625 Assign variable d and f to values 42 and 14 respectively and solve d/f # Assign variables d &lt;- 42 f &lt;- 14 # solve d/f d/f ## [1] 3 ________________________________________________________________________________ "],["basic-data-types-and-structures.html", "Chapter 2 Basic Data Types and Structures 2.1 Data types 2.2 Data Structures", " Chapter 2 Basic Data Types and Structures 2.1 Data types There are different kinds of values in R that can be manipulated in variables in R. class()function is used to check the data type of a value or a variable. Different data types include; Numeric These represent numeric values such as integers and decimals. They are used for mathematical expressions and quantitative data analysis. The below code finds the data type of variable a which is assigned 23.5 and returns numeric. a=23.5 class(a) #check the data type of a ## [1] &quot;numeric&quot; a whole number without without a decimal is also numeric for instance 45, 8, 0 and 73. Run the code chunks below to inspect to find the code of each value class(45) ## [1] &quot;numeric&quot; class(8) ## [1] &quot;numeric&quot; class(0) ## [1] &quot;numeric&quot; class(73) ## [1] &quot;numeric&quot; Activity Answer the questions below; Find the data type of 98.03 using class() function. Assign the value 98.03 to variable height and find data type of height. # I. Find the data type of 98.03 using class() function # CODE HERE # II. Assign the value `98.03` to variable `height` and find its data type # CODE HERE Integers They represent whole numbers without any any decimals and are a subclass of numeric. L is added at the end of a whole number to indicate that it is an integer. a=23L #add L to show it is an integer class(a) ## [1] &quot;integer&quot; Lets store age as an integer. Note the ‘L’ after the number 27 age = 27L class(age) ## [1] &quot;integer&quot; Activity Answer the questions below; Find the data type of any whole number using class() function. Remember to add L after the digits There are 27 goats in a field, assign the quantity of goats to a variable goats and find the data type of the variable goats. # I. Find the data type of any whole number using `class()` function. **Remember to add `L` after the digits** # CODE HERE # II. There are 27 goats in a field, assign the quantity of goats to a variable `goats` and find the data type of the variable `goats`. # CODE HERE Characters They represent text strings such as names, sentences and labels. They are enclosed in ” or ’. a=&quot;DNA&quot; class(a) ## [1] &quot;character&quot; Lets use name as a character name = &quot;Pragya&quot; class(name) ## [1] &quot;character&quot; for an object item = &quot;car&quot; # &quot;car&quot; is stored in a variable item class(item) ## [1] &quot;character&quot; Character data types can have empty spaces in between, for instance; fullname = &quot;Salman Khan&quot; class(fullname) ## [1] &quot;character&quot; Activity In the code cell below; Find the data type of the value \"school\" using the class() function. Assign your first name to a variable firstname and find its data type. Remember to enclose it in quotation marks Assign your full names to a variable full_name and find its data type. For instance if your name is “Vipin Patel” assign it like;full_name = \"Vipin Patel\" and find its data type. Remember to enclose the value in quotation marks since its a character data type # I. Find the data type of the value `&quot;school&quot; # CODE HERE # Assign your first name to variable first name and find its data type # CODE HERE # Assign your full names to a variable full_name and finds data type # CODE HERE Logical They represent boolean values which has only distinct value; TRUE or FALSE. a=TRUE #logical data types is either TRUE or FALSE only class(a) ## [1] &quot;logical&quot; changing it to FALSE b = FALSE class(b) ## [1] &quot;logical&quot; Activity Assign a TRUE to a variable grateful and find the data type of the variable. # Assign a `TRUE` to a variable `grateful` and find its data type ## CODE HERE Complex They represent complex numbers with real and imaginary parts a=2+3i # Complex data types have &#39;i&#39; at the end of each number class(a) ## [1] &quot;complex&quot; 2 is the real part while 3i is the imaginary part. Also, complex numbers can be created by complex() function with real and imaginary as the arguments. z = complex(real = 3, imaginary = 7) print(z) #show the comlex value ## [1] 3+7i class(z) #confirm that it is a complex number ## [1] &quot;complex&quot; Lets try another values to fit to the complex data type 2+5i z = complex(real=2, imaginary = 5) print(z) ## [1] 2+5i class(z) ## [1] &quot;complex&quot; 7 + 6i m=complex(real=7, imaginary = 6) print(m) ## [1] 7+6i class(m) ## [1] &quot;complex&quot; 4i - 1 b = 4i-1 print(b) ## [1] -1+4i class(b) ## [1] &quot;complex&quot; Complex data types can include the imaginary part only without real number, R will assume the real part to be 0(zero). For instance; h = 3i print(h) ## [1] 0+3i class(h) ## [1] &quot;complex&quot; Activity Find the data type of the following values; One of them is a numeric element 3i + 8 5 - 1i 4i 12 #i. 3i + 8 # CODE HERE # ii. 5 - 1i # CODE HERE # iii. 4i # CODE HERE # iv. 12 # CODE HERE Raw They represent a vector of bytes in their natural form. They are used in storing binary data. Example; a=charToRaw(&quot;DNA&quot;) print(a) ## [1] 44 4e 41 class(a) ## [1] &quot;raw&quot; # convert back to character b=rawToChar(a) class(b) ## [1] &quot;character&quot; “Hello world” can be represented as in the results below when converted to raw data type binary_data = charToRaw(&quot;Hello World&quot;) print(binary_data) ## [1] 48 65 6c 6c 6f 20 57 6f 72 6c 64 class(binary_data) ## [1] &quot;raw&quot; Numeric can also be represented as raw vectors; age=as.raw(27) print(age) ## [1] 1b class(age) ## [1] &quot;raw&quot; Activity Convert the following values to raw data types; Hint: use charToRaw() function for character data types and as.raw() to other data types. \"Vipin\" 27 69.0 FALSE 12L # i. &quot;Vipin&quot; # CODE HERE # ii. 27 # CODE HERE # iii. 69.0 # CODE HERE # iv. FALSE # CODE HERE # v. 12L # CODE HERE 2.2 Data Structures This is the organization of data into one or multiple data values in specific structures. Different types of data structures in R include; Vector Matrix Data frame 2.2.1 Vector A vector is a single entity consisting of a collection of things. They are versatile providing a basis of many operations in statistics and data manipulation hence it is important to have knowledge of vectors for effective programming in R. Vectors are created using a c() function, here is an example of a vector. marks = c(23, 67, 98, 34, 98, 21) print(marks) # print to the console ## [1] 23 67 98 34 98 21 Activity Create a vector named ages and insert the following values 21, 32, 22, 24, 27, 54, 20, 13 and print it out on the console # vector ages with elements 21, 32, 22, 24, 27, 54, 20, 13 # CODE HERE The class function is utilized to determine the data types present within vector data values. marks = c(23, 67, 98, 34, 98, 21) class(marks) ## [1] &quot;numeric&quot; The vector “marks” consist of only numeric values is.vector function is used to check if the variable is a vector. It will return a Boolean value, TRUE if the variable in question is truly a vector while FALSE if otherwise. marks = c(23, 67, 98, 34, 98, 21) is.vector(marks) ## [1] TRUE unlike matrix and data frame, vector has no dimension marks = c(23, 67, 98, 34, 98, 21) dim(marks) ## NULL length() function is used to count number of elements in vectors. In our case vector marks, marks = c(23, 67, 98, 34, 98, 21) has six elements, therefore, length() command will return 6. marks = c(23, 67, 98, 34, 98, 21) length(marks) ## [1] 6 Activity Create a vector named height with its elements/values as 120.1, 118, 123.4, 130.8, 115.2 and do the following; print it out to the console using print() function. find the data type of its elements using class() function use is.vector() function to find if its really a vector count the number of elements in the vector using length() function. #Create a vector named `height` with its elements/values as `120.1, 118, 123.4, 130.8, 115.2` and do the following; # CODE HERE # print it out # CODE HERE # find the data type of its elements # CODE HERE # find if its really a vector # CODE HERE # count the number of elements in the vector # CODE HERE Index is the position of an element in a vector, in R it starts at index 1 - lets say we find the third element by index 3 marks = c(23, 67, 98, 34, 98, 21) marks[3] ## [1] 98 value “98” is at index 3, or the third in the vector. The first value/element of a vector is indexed 1, for instance if we find the first value in the vector marks. marks = c(23, 67, 98, 34, 98, 21) marks[1] #returns the first value ## [1] 23 The sequence goes on, the second, third, fourth, fifth … values are indexed as , 2, 3, 4, 5… respectively. i.e the n^th value is indexed as n. Vectors can also be sliced to obtain values over a range of indices. For instance the code below shows how to retrieve the from the second to the fourth values as a vector marks = c(23, 67, 98, 34, 98, 21) print(marks[2:4]) ## [1] 67 98 34 is.vector(marks[2:4]) # confirm if the retrieved values are in a vector ## [1] TRUE An element at a specific index in a vector can be excluded by adding a - sign before the index value. marks = c(23, 67, 98, 34, 98, 21) marks[-2] #exclude the element at index 2 ## [1] 23 98 34 98 21 rev() command is used to reverse the order of elements in a vector marks = c(23, 67, 98, 34, 98, 21) rev(marks) ## [1] 21 98 34 98 67 23 Activity Create a vector named ages and insert the following values; 13, 59, 27, 22, 19, 31, 43. Use it to answer the questions below. Print out the vector ages to the console Store the third element in a variable called my_age and print it out. Extract the values from the second to the fifth element and print them out. Exclude the third element Reverse the order of the elements in the vector. # Create a vector named `ages` and insert the following values; `13, 59, 27, 22, 19, 31, 43` # CODE HERE # i. Print out the vector `ages` to the console # CODE HERE # ii. Store the third element in a variable called `my_age` and print it out. # CODE HERE # iii. Extract the values from the second to the fifth element and print them out. # CODE HERE # iv. Exclude the third element # CODE HERE # v. Reverse the order of the elements in the vector. # CODE HERE 2.2.1.1 Mathematical Operations in a vector The summary/descriptive statistics are calculated by summary() command. marks = c(23, 67, 98, 34, 98, 21) summary(marks) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 21.00 25.75 50.50 56.83 90.25 98.00 sum(), median(), and mean() are used to calculate the total, median, average and the standard deviation of the values in a vector marks = c(23, 67, 98, 34, 98, 21) print(&quot;MARKS&quot;) ## [1] &quot;MARKS&quot; print(paste(&quot;TOTAL: &quot;, sum(marks))) ## [1] &quot;TOTAL: 341&quot; print(paste(&quot;MEDIAN: &quot;, median(marks))) ## [1] &quot;MEDIAN: 50.5&quot; print(paste(&quot;AVERAGE: &quot;, mean(marks))) ## [1] &quot;AVERAGE: 56.8333333333333&quot; Vector multiplication and division - vectors can be multiplied or divided by a scalar value of another vector of the same length and numeric data type. For instance, the vector marks=c(23, 67, 98, 34, 98, 21) is being multiplied by a scalar value 2, that will multiply each element in a vector by two. marks = c(23, 67, 98, 34, 98, 21) # Multiply each element in the vector by 2 double_marks =2 * marks marks ## [1] 23 67 98 34 98 21 double_marks ## [1] 46 134 196 68 196 42 The values in the vector marks can also be scaled down to a half when multiplied by a scalar value 0.5. marks = c(23, 67, 98, 34, 98, 21) # Multiply by 0.5 to scale the marks by a half half_marks =0.5 * marks marks ## [1] 23 67 98 34 98 21 half_marks ## [1] 11.5 33.5 49.0 17.0 49.0 10.5 Alternatively, instead of multiplying the vector by 0.5, it can be divided by 2 a scalar value two. This is what is referred to as vector division. marks = c(23, 67, 98, 34, 98, 21) # Scale down the marks by a half by dividing by 2 instead of multiplying by 0.5 half_marks = marks/2 half_marks ## [1] 11.5 33.5 49.0 17.0 49.0 10.5 Activity Create a vector with the following values; 67, 55, 60, 59, 57.2, 71, 62, 66, 70 and name the vector weights. Use the variable weights to solve the following problems Calculate the; i. median weight ii. mean(average) weight iii. the total weight when summed together Calculate the summary statistics using the summary() function. Add 10 to variable weights and the answer added_weights. Subtract 15 to weights and name it reduced_weights. Scale the weights by multiplying the vector by 1.5. ’ Scale down the weights to a third by dividing the vector by 3. # Create vector weights from 67, 55, 60, 59, 57.2, 71, 62, 66, 70 # CODE HERE ## a. Calculate the; ### i. median weight # CODE HERE ### ii. mean(average) weight # CODE HERE ### iii. the total weight when summed together # CODE HERE ## b. Calculate the summary statistics using the `summary()` function. # CODE HERE ## c. Add 10 to variable `weights` and the answer `added_weights`. # CODE HERE ## d. Subtract 15 to `weights` and name it `reduced_weights`. # CODE HERE ## e. Scale the weights by multiplying the vector by 1.5. &#39; # CODE HERE ## f. Scale down the weights to a third by dividing the vector by 3. # CODE HERE Vector by vector multiplication and division Two or more vectors of numeric values of the equal length can be multiplied or divided by each other. The example below demonstrates vector by vector multiplication of vector a; 3, 5, 1 and vector b: 7, 3, 9. Each value is multiplied by a value of a corresponding index in the next vector such that; 3 is multiplied by 7 to be 21 5 is multiplied by 3 to be 15 1 is multiplied by 9 to be 9. The resultant vector is now 21 15 9. a = c(3, 5, 1) b = c(7, 3, 9) ab = a*b ab ## [1] 21 15 9 ba = b*a # is the same as ab ba ## [1] 21 15 9 The same vectors can also be divided by each other provided they are of the same length and all have numeric values. The order of vector division, for instance in the first case vector a is divided by vector b such that; 3 is divided by 7 to be 0.4285714 5 is divided by 3 to be 1.6666667 1 is divided by 9 to be 0.1111111. The resultant vector is now 0.4285714 1.6666667 0.1111111. # First case a = c(3, 5, 1) b = c(7, 3, 9) # Divide vector a by b abdiv=a/b abdiv ## [1] 0.4285714 1.6666667 0.1111111 , and in the second case the order of vector division is reversed by vector b being divided by a (b/a instead of a/b) such that; 7 is divided by 3 to be 2.333333 3 is divided by 5 to be 0.600000 9 is divided by 1 to be 9.000000. The resultant vector is now 2.333333 0.600000 9.000000. # Second case a = c(3, 5, 1) b = c(7, 3, 9) # Divide vector b by a badiv=b/a badiv ## [1] 2.333333 0.600000 9.000000 However, when multiplying vectors of unequal length the shorter one is replicated to match the longer vector. It will then return a warning. The case below shows how vector e=c(1,2,3,4,5) and f=c(1,2) are multiplied. vector f=c(1,2) will be replicated to match the length of vector e, therefore, vector f will be f=c(1,2,1,2,1). The process of vector by vector multiplication will be followed. e=c(1,2,3,4,5) f=c(1,2) ef = e*f #it shows an error ## Warning in e * f: longer object length is not a multiple of shorter object ## length ef #shows results since f is replicated to match e as f=c(1,2,1,2,1) ## [1] 1 4 3 8 5 Multiple vectors can be concatenated/combined to come up with one giant vector a ## [1] 3 5 1 b ## [1] 7 3 9 z=c(a,b,a) #concatenates the vectors z ## [1] 3 5 1 7 3 9 3 5 1 Activity Create two vectors, vector1;4, 6, 12, 7 and vector2:7, 3, 5, 10. Use the two vectors to solve the following questions. Create vector3 by multiplying vector1 and vector2. Print it out. Create vector4a by diving vector1 by vector2. Print it out. Create vector4b by dividing vector2 by vector1. Print it out. Is there a difference between vector4a and vector4b? If there is, what brought the difference? Write the answer as a comment. Create another vector5; 4, 6 and multiply it with vector1 to come up with vector6. Print it out. Concatenate vector1, vector2 and vector5 to come up with a giant_vector. Print it out. # Create vector1 and vector2 as instructed # CODE HERE ## i. Create `vector3` by multiplying `vector1` and `vector2`. Print it out. # CODE HERE ## ii. Create `vector4a` by diving `vector1` by `vector2`. Print it out. # CODE HERE ## iii. Create `vector4b` by dividing `vector2` by `vector1`. Print it out. # CODE HERE ## iv. Is there a difference between `vector4a` and `vector4b`? If there is, ## what brought the difference? Write the answer as a comment. # COMMENT HERE ## v. Create another `vector5`; `4, 6` and multiply it with `vector1` to come up ## with `vector6`. Print it out. # CODE HERE ## vi. Concatenate `vector1`, `vector2` and `vector5` to come up with a ## `giant_vector`. Print it out. # CODE HERE 2.2.1.2 Character Vectors Vectors can also contain character data types for instance my_name = c(&quot;My&quot;, &quot;name&quot;, &quot;is&quot;, &quot;Vipin&quot;) my_name[5] = &quot;Singh&quot; #insert at the end my_name ## [1] &quot;My&quot; &quot;name&quot; &quot;is&quot; &quot;Vipin&quot; &quot;Singh&quot; Combining the vectors to a single string. For instance the vector my_name = c(\"My\", \"name\", \"is\", \"Vipin\") is combined to \"My name is Vipin\". The collapse argument is used as below; print(paste(my_name, collapse=&quot; &quot;)) ## [1] &quot;My name is Vipin Singh&quot; Calculate the summary/descriptive statistics of the vector by function summary(). It finds; Count/length Class (data type) Mode summary(my_name) ## Length Class Mode ## 5 character character 2.2.1.3 Vectors with mixed data types A vector can also consist of characters values and numeric values for instance numbers=c(1,&quot;two&quot;, 3, &quot;three&quot;) numbers ## [1] &quot;1&quot; &quot;two&quot; &quot;3&quot; &quot;three&quot; however the numeric elements in the vector are recognized by R as character data type. They can be converted to numeric by; as.numeric(numbers[1]) + 2 ## [1] 3 the integers can be converted by; as.integer(numbers[1]) ## [1] 1 2.2.1.4 Named Vectors Variable names can be assigned to vectors like; named_vector=c(EcoR1=&quot;GAATTC&quot;, HindIII=&quot;AAGCTT&quot;, Pst1=&quot;CTGCAG&quot;) named_vector ## EcoR1 HindIII Pst1 ## &quot;GAATTC&quot; &quot;AAGCTT&quot; &quot;CTGCAG&quot; to access the names of the values is; names(named_vector) ## [1] &quot;EcoR1&quot; &quot;HindIII&quot; &quot;Pst1&quot; A vector element can be accessed using its name named_vector[&quot;EcoR1&quot;] # find the value of a vector by its name ## EcoR1 ## &quot;GAATTC&quot; 2.2.1.5 Generating number series as vectors The seq function in R is used to generate sequences of numbers. It takes several arguments, including from, to, by, and length.out, among others, to specify the range and increment of the sequence. Here’s a brief overview of its usage: from: The starting value of the sequence. to: The end value of the sequence. # Generate a sequence from 1 to 10 series = seq(from=1, to=20) series ## [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 # It can also be written as series = seq(1,20) series ## [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 class(series) ## [1] &quot;integer&quot; by: The increment between consecutive values in the sequence. # generate numbers 0 to 10 incremented by 0.5 series3=seq(0, 10, by=0.5) series3 ## [1] 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0 5.5 6.0 6.5 7.0 ## [16] 7.5 8.0 8.5 9.0 9.5 10.0 length: The desired length of the sequence. # generate 10 numbers from 0 to 6 series4=seq(0, 6, length=10) series4 ## [1] 0.0000000 0.6666667 1.3333333 2.0000000 2.6666667 3.3333333 4.0000000 ## [8] 4.6666667 5.3333333 6.0000000 seq(0, 6) ## [1] 0 1 2 3 4 5 6 along.with: An optional vector argument specifying the length and names of the output sequence. # Generate a sequence along with a vector seq(along.with = c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;)) ## [1] 1 2 3 2.2.1.6 Null data points in vectors NA data (Not available or blank) for instance marks=c(78,65, 98, 87, 89, NA) sum(is.na(marks)) #Count the null values in a vector ## [1] 1 Other inbuilt functions for mathematical operations cannot be done if Null values exists in a vector unless they are removed/ignored #sum(marks) #returns an error sum(marks, na.rm = TRUE) #remove null values before calculating the sum ## [1] 417 median(marks, na.rm = TRUE) ## [1] 87 summary(marks, na.rm = TRUE) ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## 65.0 78.0 87.0 83.4 89.0 98.0 1 2.2.2 Matrix A matrix is a two dimensional data type that contain a single class of data. The code below shows one can produce a matrix from a vector vector1 = seq(1, 9) # Convert to matrix ## create by column data1=matrix(vector1, ncol=3) data1 ## [,1] [,2] [,3] ## [1,] 1 4 7 ## [2,] 2 5 8 ## [3,] 3 6 9 A vector of values 1 to 9 is being converted to a matrix where the values are being arranged column wise by default. A matrix has a multiple dimensions, the most common type of matrix is two dimesnional. vector1 = seq(1, 9) # Convert to matrix ## create by column data1=matrix(vector1, ncol=3) # find the dimension of the vector dim(data1) ## [1] 3 3 is.matrix() function is used to confirm if a given variable is a matrix and it return a boolean value. vector1 = seq(1, 9) # Convert to matrix ## create by column data1=matrix(vector1, ncol=3) # confirm if `data1` is really a matrix is.matrix(data1) ## [1] TRUE A matrix can also be created row-wise from a vector. vector1 = seq(1, 9) ## create a matrix by row data2=matrix(vector1, ncol=3, byrow=TRUE) data2 # is a transpose of data1 ## [,1] [,2] [,3] ## [1,] 1 2 3 ## [2,] 4 5 6 ## [3,] 7 8 9 Matrix is recognized either as a matrix or array by R vector1 = seq(1, 9) # Convert to matrix ## create by column data1=matrix(vector1, ncol=3) # find the data type of `data1` class(data1) ## [1] &quot;matrix&quot; &quot;array&quot; To access a specific data point in a matrix, the matrix is indexed by row then column for instance matrix_data[row_index, column_index] vector1 = seq(1, 9) # Convert to matrix ## create by column data1=matrix(vector1, ncol=3) # retrieve the value in the third row second column in `data1` data1[3, 2] ## [1] 6 To access a single row, in this case we find the second row which will be returned as a vector vector1 = seq(1, 9) # Convert to matrix ## create by column data1=matrix(vector1, ncol=3) row2 = data1[2,] # access row 2 is.vector(row2) #can be accessed by row 2 ## [1] TRUE To access a single column vector1 = seq(1, 9) # Convert to matrix ## create by column data1=matrix(vector1, ncol=3) col3=data1[,3] # access column 3 is.vector(col3) #can be accessed by column 3 ## [1] TRUE Count the number of rows in a matrix vector1 = seq(1, 9) # Convert to matrix ## create by column data1=matrix(vector1, ncol=3) nrow(data1) ## [1] 3 data1 has 3 rows Count the number of columns in a matrix vector1 = seq(1, 9) # Convert to matrix ## create by column data1=matrix(vector1, ncol=3) ncol(data1) ## [1] 3 2.2.2.1 Mathematical Operations in a matrix Matrix Addition Matrix addition can be done by adding a number to the matrix or another matrix of the equal number of rows and columns. vector1 = seq(1, 9) # Convert to matrix ## create by column data1=matrix(vector1, ncol=3) data2 = data1 + 3 data2 ## [,1] [,2] [,3] ## [1,] 4 7 10 ## [2,] 5 8 11 ## [3,] 6 9 12 For instance, the code snippet above demonstrates matrix addition by a numeric value. Adding value 3 to a matrix adds each value in the matrix by 3. To demonstrate a matrix to a matrix addition, we will create two matrices of the equal dimensions then add to each other. data1 = matrix(seq(1, 9), ncol=3, byrow=TRUE) print(data1) ## [,1] [,2] [,3] ## [1,] 1 2 3 ## [2,] 4 5 6 ## [3,] 7 8 9 data2 = matrix(seq(1, 18, 2), ncol=3, byrow=TRUE) print(data2) ## [,1] [,2] [,3] ## [1,] 1 3 5 ## [2,] 7 9 11 ## [3,] 13 15 17 # Add data1 to data2 resultant_matrix = data1 + data2 resultant_matrix ## [,1] [,2] [,3] ## [1,] 2 5 8 ## [2,] 11 14 17 ## [3,] 20 23 26 Matrix Subtraction The same concept of matrix addition applies to matrix subtraction as well. data1 = matrix(seq(1, 9), ncol=3, byrow=TRUE) data3 = data1-1 #reduce each value by 1 data3 ## [,1] [,2] [,3] ## [1,] 0 1 2 ## [2,] 3 4 5 ## [3,] 6 7 8 Subtracting 1 to data1 subtract each value in the matrix by 1. Lets now subtract data1 from data2. data1 = matrix(seq(1, 9), ncol=3, byrow=TRUE) data1 ## [,1] [,2] [,3] ## [1,] 1 2 3 ## [2,] 4 5 6 ## [3,] 7 8 9 data2 = matrix(seq(1, 18, 2), ncol=3, byrow=TRUE) data2 ## [,1] [,2] [,3] ## [1,] 1 3 5 ## [2,] 7 9 11 ## [3,] 13 15 17 resultant_matrix = data2-data1 resultant_matrix ## [,1] [,2] [,3] ## [1,] 0 1 2 ## [2,] 3 4 5 ## [3,] 6 7 8 Matrix Multiplication(scalar) A matrix can be multiplied by a scalar whereby the scalar value multiplies all the cells in the matrix. data1 = matrix(seq(1, 9), ncol=3, byrow=TRUE) data1 ## [,1] [,2] [,3] ## [1,] 1 2 3 ## [2,] 4 5 6 ## [3,] 7 8 9 data4 = data1*5 data4 ## [,1] [,2] [,3] ## [1,] 5 10 15 ## [2,] 20 25 30 ## [3,] 35 40 45 Matrix multiplication applies a concept of row by column. The row of the first matrix is multiplied with a row of the second matrix. It also known as the dot product. data1 = matrix(seq(1, 9), ncol=3, byrow=TRUE) data1 ## [,1] [,2] [,3] ## [1,] 1 2 3 ## [2,] 4 5 6 ## [3,] 7 8 9 data2 = matrix(seq(1, 18, 2), ncol=3, byrow=TRUE) data2 ## [,1] [,2] [,3] ## [1,] 1 3 5 ## [2,] 7 9 11 ## [3,] 13 15 17 # Find the product of the two matrices product_matrix = data1 * data2 product_matrix ## [,1] [,2] [,3] ## [1,] 1 6 15 ## [2,] 28 45 66 ## [3,] 91 120 153 Matrix division data1 = matrix(seq(1, 9), ncol=3, byrow=TRUE) data1 ## [,1] [,2] [,3] ## [1,] 1 2 3 ## [2,] 4 5 6 ## [3,] 7 8 9 # Divide `data1` matrix by 2 data5 = data1/2 data5 ## [,1] [,2] [,3] ## [1,] 0.5 1.0 1.5 ## [2,] 2.0 2.5 3.0 ## [3,] 3.5 4.0 4.5 2.2.3 Data frame is a two dimensional data structure, like a 2d array/matrix with rows and columns. Lets convert a matrix into a data frame vector1 = c(1:12) matrix1 = matrix(vector1, ncol=4) #create a matrix from the vector # Adding a column student Students=c(&quot;Pragya&quot;, &quot;Deepika&quot;, &quot;Chandran&quot;) data = data.frame(Students, matrix1) data ## Students X1 X2 X3 X4 ## 1 Pragya 1 4 7 10 ## 2 Deepika 2 5 8 11 ## 3 Chandran 3 6 9 12 The above data shows scores of different students in different subjects. The column names are automatically generated by R, however, the column names can be added as below. vector1 = c(1:12) matrix1 = matrix(vector1, ncol=4) #create a matrix from the vector # Adding a column student Students=c(&quot;Pragya&quot;, &quot;Deepika&quot;, &quot;Chandran&quot;) data = data.frame(Students, matrix1) data ## Students X1 X2 X3 X4 ## 1 Pragya 1 4 7 10 ## 2 Deepika 2 5 8 11 ## 3 Chandran 3 6 9 12 # Create column names headers=c(&quot;Students&quot;, &quot;Geonomics&quot;, &quot;Proteomics&quot;, &quot;Microbiology&quot;, &quot;Biostatistics&quot;) colnames(data)=headers #add column names data ## Students Geonomics Proteomics Microbiology Biostatistics ## 1 Pragya 1 4 7 10 ## 2 Deepika 2 5 8 11 ## 3 Chandran 3 6 9 12 A row wise addition can be performed on a data frame to find the total scores for each student in the four units vector1 = c(1:12) matrix1 = matrix(vector1, ncol=4) #create a matrix from the vector # Adding a column student Students=c(&quot;Pragya&quot;, &quot;Deepika&quot;, &quot;Chandran&quot;) data = data.frame(Students, matrix1) data ## Students X1 X2 X3 X4 ## 1 Pragya 1 4 7 10 ## 2 Deepika 2 5 8 11 ## 3 Chandran 3 6 9 12 ## Add a new column with total marks obtained data$total_marks=rowSums(data[, c(2, 3, 4, 5)]) #add from second to fifth column data ## Students X1 X2 X3 X4 total_marks ## 1 Pragya 1 4 7 10 22 ## 2 Deepika 2 5 8 11 26 ## 3 Chandran 3 6 9 12 30 Find the average score for each student.rowMeans() is used the average of each row/record. vector1 = c(1:12) matrix1 = matrix(vector1, ncol=4) #create a matrix from the vector # Adding a column student Students=c(&quot;Pragya&quot;, &quot;Deepika&quot;, &quot;Chandran&quot;) data = data.frame(Students, matrix1) data ## Students X1 X2 X3 X4 ## 1 Pragya 1 4 7 10 ## 2 Deepika 2 5 8 11 ## 3 Chandran 3 6 9 12 data$average_marks=rowMeans(data[, c(2, 3, 4, 5)]) data # confirm if the new column is added ## Students X1 X2 X3 X4 average_marks ## 1 Pragya 1 4 7 10 5.5 ## 2 Deepika 2 5 8 11 6.5 ## 3 Chandran 3 6 9 12 7.5 "],["data-importing-and-exporting.html", "Chapter 3 Data Importing and Exporting 3.1 Introduction to Data Importing 3.2 Demonstration of Data Importing 3.3 Introduction to Data Exporting", " Chapter 3 Data Importing and Exporting 3.1 Introduction to Data Importing Loading data in R can be very stressful since every file format has a function to import the data. However, it can be very simple when well explained. Here are some of the most commonly used data file formats R and their importation;- Comma Separated Value(CSV - .csv) files imported by read.csv(\"filepath\") function Excel(.xlsx) files loaded by read_excel(\"filepath\") function` XML(.xml) imported by read.xml(\"filepath\") function` Javascript Object Notation (JSON) by read.json(\"filepath\") There are more data files with different formats that can be used in R, their importation will be explained later in the course. 3.2 Demonstration of Data Importing Importing a CSV file in R There are different formats to import a csv file in R. We will use the read.csv() and read.delim() which are functions from the baseR. Reading a csv file using read.csv() function. # read the csv file to the data frame df &lt;- read.csv(&#39;data/hotel_bookings_clean.csv&#39;) head(df) # show first few rows Now, lets read the same csv file with read.delim() function. Remember to add “,” as the separator. sep=\",\". # Read the csv file df &lt;- read.delim(&quot;data/hotel_bookings_clean.csv&quot;, sep = &quot;,&quot;) head(df) # show the first few rows Lets also read the csv file using the read.table() function(note the sep=\",\" argument); df &lt;- read.table(&quot;data/hotel_bookings_clean.csv&quot;, sep = &quot;,&quot;) head(df) The read_csv() function is one more way to read a csv file. This function comes from the readr package that is installed by; install.packages(&quot;readr&quot;) and the library can be imported by; library(readr) Lets read a csv file using the read_csv()(note the ’_’ instead of a ‘.’) from the readr library. df &lt;- read_csv(&quot;data/hotel_bookings_clean.csv&quot;, show_col_types = FALSE) head(df) ## # A tibble: 6 × 53 ## is_canceled lead_time arrival_date_week_number arrival_date_day_of_month ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 342 27 1 ## 2 0 737 27 1 ## 3 0 7 27 1 ## 4 0 13 27 1 ## 5 0 14 27 1 ## 6 0 14 27 1 ## # ℹ 49 more variables: arrival_date_month &lt;dbl&gt;, stays_in_weekend_nights &lt;dbl&gt;, ## # stays_in_week_nights &lt;dbl&gt;, adults &lt;dbl&gt;, children &lt;dbl&gt;, babies &lt;dbl&gt;, ## # is_repeated_guest &lt;dbl&gt;, previous_cancellations &lt;dbl&gt;, ## # previous_bookings_not_canceled &lt;dbl&gt;, required_car_parking_spaces &lt;dbl&gt;, ## # total_of_special_requests &lt;dbl&gt;, avg_daily_rate &lt;dbl&gt;, ## # booked_by_company &lt;dbl&gt;, booked_by_agent &lt;dbl&gt;, hotel_City &lt;dbl&gt;, ## # hotel_Resort &lt;dbl&gt;, meal_BB &lt;dbl&gt;, meal_FB &lt;dbl&gt;, meal_HB &lt;dbl&gt;, … read.csv(), read.delim(), read.table() and read_csv() are the functions used to read CSV files in R. Importing Excel file in R In this section we will read the HR employee data for descriptive and inferential analytics. We will use the read_excel from the readxl package. It can be imported by; library(readxl) Let’s load the excel data into R data frame. df &lt;- read_excel(&quot;data/HR_Employee_Data.xlsx&quot;) head(df) ## # A tibble: 6 × 11 ## Emp_Id satisfaction_level last_evaluation number_project average_montly_hours ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 IND024… 0.38 0.53 2 157 ## 2 IND281… 0.8 0.86 5 262 ## 3 IND071… 0.11 0.88 7 272 ## 4 IND304… 0.72 0.87 5 223 ## 5 IND240… 0.37 0.52 2 159 ## 6 IND086… 0.41 0.5 2 153 ## # ℹ 6 more variables: time_spend_company &lt;dbl&gt;, Work_accident &lt;dbl&gt;, ## # left &lt;dbl&gt;, promotion_last_5years &lt;dbl&gt;, Department &lt;chr&gt;, salary &lt;chr&gt; If they are multiple worksheets in the excel, the read_excel has an argument sheet for sheet number(By default, sheet 1 is loaded. Lets load the first sheet. df_sheet1 &lt;- read_excel(&quot;data/HR_Employee_Data.xlsx&quot;, sheet=1) head(df_sheet1) ## # A tibble: 6 × 11 ## Emp_Id satisfaction_level last_evaluation number_project average_montly_hours ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 IND024… 0.38 0.53 2 157 ## 2 IND281… 0.8 0.86 5 262 ## 3 IND071… 0.11 0.88 7 272 ## 4 IND304… 0.72 0.87 5 223 ## 5 IND240… 0.37 0.52 2 159 ## 6 IND086… 0.41 0.5 2 153 ## # ℹ 6 more variables: time_spend_company &lt;dbl&gt;, Work_accident &lt;dbl&gt;, ## # left &lt;dbl&gt;, promotion_last_5years &lt;dbl&gt;, Department &lt;chr&gt;, salary &lt;chr&gt; A detail documentation of the read_excel() function can be found here 3.3 Introduction to Data Exporting After data wrangling, manipulation and processing, the end product(processed data) can be saved for further use. The data can also be share to others. R has several functions to write the processed data locally into either CSV, Excel, text or other file formats. Lets go through a brief overview of how data can be exported in R; CSV files are exported by write.csv() function. The excel files are written locally by write.xlsx() function. Remember to install the xlsx package before exporting data to excel. The package is installed by install.packages(&quot;xlsx&quot;) writeLines() is used to write vectors to a text file. The write.table() function is more of a general function that different types of files for example TSV, CSV and text files. Lets create a small data frame and demonstrate how these functions work. # Sample data frame df &lt;- data.frame( Name = c(&quot;Alice&quot;, &quot;Bob&quot;, &quot;Charlie&quot;), Age = c(25, 30, 35), Salary = c(50000, 60000, 70000) ) df ## Name Age Salary ## 1 Alice 25 50000 ## 2 Bob 30 60000 ## 3 Charlie 35 70000 The data frame will be exported to csv by write.csv(data_frame, file=filepath). Lets do it; write.csv(df, file=&quot;data/sample_data.csv&quot;) Then export the same data frame df to an excel file using the the command write.xlsx(data_frame, file=filepath) library(xlsx) write.xlsx(df, file=&quot;data/sample_data.xlsx&quot;) Also, the openxlsx package can be used to export data to excel. The package is installed by; install.packages(&quot;openxlsx&quot;) and the file is written by the same write.xlsx() function. Lets repeat the process of exporting using the write.xlsx() function from the openxlsx library. library(openxlsx) write.xlsx(df, file=&quot;data/sample_data_2.xlsx&quot;) Finally, lets export the data frame using the write.table() function. # Export to a text file with space as a delimiter write.table(df, file = &quot;data/sample_data.txt&quot;, sep = &quot; &quot;, row.names = FALSE, quote = FALSE) The same data frame can be converted to a data vector and loaded to a text file using the writeLines(data_vector, filepath). Lets convert the data frame to a data vector and show the output. # Convert data frame to a character vector df_vector &lt;- apply(df, 1, function(row) paste(row, collapse = &quot; &quot;)) df_vector ## [1] &quot;Alice 25 50000&quot; &quot;Bob 30 60000&quot; &quot;Charlie 35 70000&quot; Export the file # Write the character vector to a text file writeLines(data_vector, &quot;data/sample_data_with_writeLines.txt&quot;) Practical Exercise Down the house prices regression data set from here and do the following; Read the CSV file into a data.frame and give it a variable name housePricesDF Write the data.frame into an Excel file locally. Solution Down the house prices regression data set from here and do the following; Read the CSV file into a data.frame and give it a variable name housePricesDF Write the data.frame into an Excel file locally. # i. Read the CSV file into a `data.frame` and give it a variable name `housePricesDF` housePricesDF &lt;- read.csv(&quot;data/house_price_regression_dataset.csv&quot;) # ii. Write the `data.frame` into an Excel file locally write.csv(housePricesDF, &quot;data/saved_data/house_price_regression_dataset.csv&quot;) ________________________________________________________________________________ "],["data-manipulation.html", "Chapter 4 Data Manipulation 4.1 Basic Data Manipulation 4.2 Data Manipulation with Dplyr 4.3 Chaining", " Chapter 4 Data Manipulation 4.1 Basic Data Manipulation 4.1.1 Introduction to Data Manipulation Data Manipulation is the adjusting, organizing and transforming of the raw data is not a more useful and suitable format for data analysis. These are some of the reasons that make data manipulation mandatory in the data analysis process; Improves the data quality Raw data may be incomplete, messy, containing irrelevant information, errors ,or duplicates that need to be cleaned and rectified. This will ensure the data is reliable thereby preventing incorrect conclusions or decisions. Making Data Usable Sometimes data is collected from different sources that is not ready for analysis. Data Manipulation will transform the data into a structured and consistent format for easy analysis. Enhancing Data Exploration By cleaning the data, analysts explore the data thereby understanding different concepts of the data. Enabling Complex Analysis Some types of analysis require data to be in specific format or structure, for instance the time series analysis require data to be sorted out by date. Supporting Decision Making Data Manipulation ensures that the data that is fed into the system is timely, accurate and reliable for informed decision-making models and relevant reports These are the key tasks in the data manipulation; Cleaning: by removing inaccurate and incomplete data entries. Filtering the data by selecting certain rows or columns based on a certain criteria. Reshaping: Changing the structure of the data for instance pivoting. Merging: Combine multiple data sets into one. Transforming: Modify existing data by mathematical or logical operations. Aggregation: Summarizing the data by performing operations like sum,average and count. 4.1.2 Subsetting and Filtering Data: Subsetting is a data management strategy that involves creating a coherent slice data from different data set for specific use cases. This topic will better be explained practically, therefore we will use the titanic data set. The data set contains information about the passengers on the Titanic, including their age, gender, passenger class, whether they survived and other details. Since the titanic dataset is absent in baseR, the titanic library will be installed by; install.packages(&quot;titanic&quot;) load the library library(&quot;titanic&quot;) The data set will indexed using different indexing techniques such as indexing of a single element, row and column indexing. First we load the data set and view the first few records before indexing data(&quot;titanic_train&quot;) titanic &lt;- titanic_train head(titanic) # view the first few rows of the titanic data set ## PassengerId Survived Pclass ## 1 1 0 3 ## 2 2 1 1 ## 3 3 1 3 ## 4 4 1 1 ## 5 5 0 3 ## 6 6 0 3 ## Name Sex Age SibSp Parch ## 1 Braund, Mr. Owen Harris male 22 1 0 ## 2 Cumings, Mrs. John Bradley (Florence Briggs Thayer) female 38 1 0 ## 3 Heikkinen, Miss. Laina female 26 0 0 ## 4 Futrelle, Mrs. Jacques Heath (Lily May Peel) female 35 1 0 ## 5 Allen, Mr. William Henry male 35 0 0 ## 6 Moran, Mr. James male NA 0 0 ## Ticket Fare Cabin Embarked ## 1 A/5 21171 7.2500 S ## 2 PC 17599 71.2833 C85 C ## 3 STON/O2. 3101282 7.9250 S ## 4 113803 53.1000 C123 S ## 5 373450 8.0500 S ## 6 330877 8.4583 Q Subsetting by indexing can be done in three different ways that include Extracting a Row Extracting a column Extracting a Single Element Extract a row When subsetting to extract data for a single row the square brackets [ ] will be used with the position of the index you want to extract. Lets extract all the information of the 10th passenger. titanic[10, ] # note the comma after the index 10 ## PassengerId Survived Pclass Name Sex Age ## 10 10 1 2 Nasser, Mrs. Nicholas (Adele Achem) female 14 ## SibSp Parch Ticket Fare Cabin Embarked ## 10 1 0 237736 30.0708 C Also, more indices can be subsetted in the format [i:j, ] where the i is the starting index while j is the ending index respectively. Lets extract the information by subsetting the titanic data from index 7 to 10. titanic[7:10, ] ## PassengerId Survived Pclass ## 7 7 0 1 ## 8 8 0 3 ## 9 9 1 3 ## 10 10 1 2 ## Name Sex Age SibSp Parch ## 7 McCarthy, Mr. Timothy J male 54 0 0 ## 8 Palsson, Master. Gosta Leonard male 2 3 1 ## 9 Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg) female 27 0 2 ## 10 Nasser, Mrs. Nicholas (Adele Achem) female 14 1 0 ## Ticket Fare Cabin Embarked ## 7 17463 51.8625 E46 S ## 8 349909 21.0750 S ## 9 347742 11.1333 S ## 10 237736 30.0708 C Extract a column When subsetting to extract data for a single column the square brackets [ ] will be used as before, with the position of the index or column name you want to extract. Lets extract all the information of the column Name. titanic[, &quot;Name&quot;] # note the comma before &quot;Name&quot; An index of the column can be used in place of the column name. For instance, the column, “PassengerId” is the first column therefore its index will be 1. Lets subset the column by calling the index. titanic[, 1] # note the comma before the column index Extracting a single element A single element that has a defined position in a data frame, both the row index and the column name/index are called. dataframe[row_index, column index/name] Lets extract the age of the Name of the fifth passenger. titanic[5, &quot;Name&quot;] ## [1] &quot;Allen, Mr. William Henry&quot; Instead of using the column name. Lets use the column index. In the above context, the column “Name” appears at index(is the fourth column). titanic[5, 4] ## [1] &quot;Allen, Mr. William Henry&quot; Subsetting a data set can be done by filtering data based on logical conditions to extract rows that meet certain criteria. They involve comparisons operators such as &gt;, &lt;, ==, != or logical operators like &amp;(and), |(or), ! (not). In this titanic data set we:- Filter based on a single condition Lets find the passengers who survived on the titanic. survivors &lt;- titanic[titanic$Survived == 1, ] head(survivors) # view the first few rows of survivors ## PassengerId Survived Pclass ## 2 2 1 1 ## 3 3 1 3 ## 4 4 1 1 ## 9 9 1 3 ## 10 10 1 2 ## 11 11 1 3 ## Name Sex Age SibSp Parch ## 2 Cumings, Mrs. John Bradley (Florence Briggs Thayer) female 38 1 0 ## 3 Heikkinen, Miss. Laina female 26 0 0 ## 4 Futrelle, Mrs. Jacques Heath (Lily May Peel) female 35 1 0 ## 9 Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg) female 27 0 2 ## 10 Nasser, Mrs. Nicholas (Adele Achem) female 14 1 0 ## 11 Sandstrom, Miss. Marguerite Rut female 4 1 1 ## Ticket Fare Cabin Embarked ## 2 PC 17599 71.2833 C85 C ## 3 STON/O2. 3101282 7.9250 S ## 4 113803 53.1000 C123 S ## 9 347742 11.1333 S ## 10 237736 30.0708 C ## 11 PP 9549 16.7000 G6 S The above data set consists of titanic passengers who survived. Who were the passengers who boarded the first class on the Titan? first_class_passengers &lt;- titanic[titanic$Pclass == 1, ] head(first_class_passengers) ## PassengerId Survived Pclass ## 2 2 1 1 ## 4 4 1 1 ## 7 7 0 1 ## 12 12 1 1 ## 24 24 1 1 ## 28 28 0 1 ## Name Sex Age SibSp Parch ## 2 Cumings, Mrs. John Bradley (Florence Briggs Thayer) female 38 1 0 ## 4 Futrelle, Mrs. Jacques Heath (Lily May Peel) female 35 1 0 ## 7 McCarthy, Mr. Timothy J male 54 0 0 ## 12 Bonnell, Miss. Elizabeth female 58 0 0 ## 24 Sloper, Mr. William Thompson male 28 0 0 ## 28 Fortune, Mr. Charles Alexander male 19 3 2 ## Ticket Fare Cabin Embarked ## 2 PC 17599 71.2833 C85 C ## 4 113803 53.1000 C123 S ## 7 17463 51.8625 E46 S ## 12 113783 26.5500 C103 S ## 24 113788 35.5000 A6 S ## 28 19950 263.0000 C23 C25 C27 S The above examples, the extracted data set met a single condition. Filtering based on Multiple Conditions Data can be subsetted by filtering based on more than one condition. To demonstrate this, lets find the female passengers who survived. Here there are two conditions;- the passenger must be a female, the passenger must have survived. The resultant data set must meet the above conditions female_survivors &lt;- titanic[titanic$Sex == &quot;female&quot; &amp; titanic$Survived == 1, ] head(female_survivors) #view the first few rows ## PassengerId Survived Pclass ## 2 2 1 1 ## 3 3 1 3 ## 4 4 1 1 ## 9 9 1 3 ## 10 10 1 2 ## 11 11 1 3 ## Name Sex Age SibSp Parch ## 2 Cumings, Mrs. John Bradley (Florence Briggs Thayer) female 38 1 0 ## 3 Heikkinen, Miss. Laina female 26 0 0 ## 4 Futrelle, Mrs. Jacques Heath (Lily May Peel) female 35 1 0 ## 9 Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg) female 27 0 2 ## 10 Nasser, Mrs. Nicholas (Adele Achem) female 14 1 0 ## 11 Sandstrom, Miss. Marguerite Rut female 4 1 1 ## Ticket Fare Cabin Embarked ## 2 PC 17599 71.2833 C85 C ## 3 STON/O2. 3101282 7.9250 S ## 4 113803 53.1000 C123 S ## 9 347742 11.1333 S ## 10 237736 30.0708 C ## 11 PP 9549 16.7000 G6 S Lets also add one more condition, the female survivor must be under 18. minor_female_survivors &lt;- titanic[titanic$Sex == &quot;female&quot; &amp; titanic$Survived == 1 &amp; titanic$Age &lt; 18, ] # comma should be after the conditons head(minor_female_survivors) ## PassengerId Survived Pclass Name Sex Age ## 10 10 1 2 Nasser, Mrs. Nicholas (Adele Achem) female 14 ## 11 11 1 3 Sandstrom, Miss. Marguerite Rut female 4 ## NA NA NA NA &lt;NA&gt; &lt;NA&gt; NA ## 23 23 1 3 McGowan, Miss. Anna &quot;Annie&quot; female 15 ## NA.1 NA NA NA &lt;NA&gt; &lt;NA&gt; NA ## NA.2 NA NA NA &lt;NA&gt; &lt;NA&gt; NA ## SibSp Parch Ticket Fare Cabin Embarked ## 10 1 0 237736 30.0708 C ## 11 1 1 PP 9549 16.7000 G6 S ## NA NA NA &lt;NA&gt; NA &lt;NA&gt; &lt;NA&gt; ## 23 0 0 330923 8.0292 Q ## NA.1 NA NA &lt;NA&gt; NA &lt;NA&gt; &lt;NA&gt; ## NA.2 NA NA &lt;NA&gt; NA &lt;NA&gt; &lt;NA&gt; Filtering using Negation The != sign a logical operator that is used to negate a condition. Lets use it to find the passengers who did not survive. non_survivors &lt;- titanic[titanic$Survived != 1, ] tail(non_survivors) # view the last few records ## PassengerId Survived Pclass Name Sex ## 884 884 0 2 Banfield, Mr. Frederick James male ## 885 885 0 3 Sutehall, Mr. Henry Jr male ## 886 886 0 3 Rice, Mrs. William (Margaret Norton) female ## 887 887 0 2 Montvila, Rev. Juozas male ## 889 889 0 3 Johnston, Miss. Catherine Helen &quot;Carrie&quot; female ## 891 891 0 3 Dooley, Mr. Patrick male ## Age SibSp Parch Ticket Fare Cabin Embarked ## 884 28 0 0 C.A./SOTON 34068 10.500 S ## 885 25 0 0 SOTON/OQ 392076 7.050 S ## 886 39 0 5 382652 29.125 Q ## 887 27 0 0 211536 13.000 S ## 889 NA 1 2 W./C. 6607 23.450 S ## 891 32 0 0 370376 7.750 Q Alternatively you can use non_survivors &lt;- titanic[!titanic$Survived == 1, ] Also, lets find the passengers who were not in the third class not_third_class &lt;- titanic[titanic$Pclass != 3, ] head(not_third_class) ## PassengerId Survived Pclass ## 2 2 1 1 ## 4 4 1 1 ## 7 7 0 1 ## 10 10 1 2 ## 12 12 1 1 ## 16 16 1 2 ## Name Sex Age SibSp Parch ## 2 Cumings, Mrs. John Bradley (Florence Briggs Thayer) female 38 1 0 ## 4 Futrelle, Mrs. Jacques Heath (Lily May Peel) female 35 1 0 ## 7 McCarthy, Mr. Timothy J male 54 0 0 ## 10 Nasser, Mrs. Nicholas (Adele Achem) female 14 1 0 ## 12 Bonnell, Miss. Elizabeth female 58 0 0 ## 16 Hewlett, Mrs. (Mary D Kingcome) female 55 0 0 ## Ticket Fare Cabin Embarked ## 2 PC 17599 71.2833 C85 C ## 4 113803 53.1000 C123 S ## 7 17463 51.8625 E46 S ## 10 237736 30.0708 C ## 12 113783 26.5500 C103 S ## 16 248706 16.0000 S Alternatively not_third_class &lt;- titanic[!titanic$Pclass == 3, ] 4.1.3 Sorting Data Sorting is the ordering of elements in a data set (vectors, lists, matrix and data frames) based on a particular criteria. This is a fundamental operation data analysis, as it enables data organization in a meaningful way for easier visualization and interpretation. These are the several functions in Base R that are used in sorting;- sort() Lets create a vector v with five elements Sort the elements in a descending order v = c(43, 82, 11, 73, 34) # Create a vector v1 = sort(v, decreasing = TRUE) #sort the elements in a descending order v1 ## [1] 82 73 43 34 11 to order the same vector in an ascending order the decreasing argument is set to FALSE. v = c(43, 82, 11, 73, 34) # Create a vector v2 = sort(v, decreasing = FALSE) #sort the elements in an ascending order v2 ## [1] 11 34 43 73 82 Also character vectors can be sorted in alphabetical order for instance lets sort the the names, \"Alice\", \"Charlie\", \"Bob\" in the alphabetical order. names &lt;- c(&quot;Alice&quot;, &quot;Charlie&quot;, &quot;Bob&quot;) sorted_names &lt;- sort(names) sorted_names ## [1] &quot;Alice&quot; &quot;Bob&quot; &quot;Charlie&quot; Alternatively, the names can be ordered in the reverse alphabetical order when the decreasing argument is set to TRUE. names &lt;- c(&quot;Alice&quot;, &quot;Charlie&quot;, &quot;Bob&quot;, &quot;Zach&quot;) names_1 &lt;- sort(names, decreasing = TRUE) # order in reverse alphabetical order names_1 ## [1] &quot;Zach&quot; &quot;Charlie&quot; &quot;Bob&quot; &quot;Alice&quot; order() This function returns the indices that would sort the vectors. For instance lets sort the vector v = c(43, 82, 11, 73, 34) in an ascending order(from smallest to the largest). The smallest number in this case is 11, therefore, the order() function will return 1 while 82 is the largest(5th smallest) number in this case, it will be returned as 5. v = c(43, 82, 11, 73, 34) order(v, decreasing = FALSE) ## [1] 3 5 1 4 2 Lets repeat the process but this time we order the indices of the vector v in a descending order v = c(43, 82, 11, 73, 34) order(v, decreasing = TRUE) ## [1] 2 4 1 5 3 You can see that index 2 which has a value 82 on the original vector comes first while index 3 which has a value of 11 comes last The default value for the decreasing argument is FALSE. v = c(43, 82, 11, 73, 34) order(v) # no `decreasing` argument ## [1] 3 5 1 4 2 rank() Returns of the rank of the element in a vector, list. The smallest element is ranked as 1(in this case its 11) while largest element is ranked last(82 is ranked 5 here) v = c(43, 82, 11, 73, 34) rank(v, ties.method = &quot;average&quot;, na.last = TRUE) ## [1] 3 5 1 4 2 rev() This function simply reverse the order of elements. The first element in a vector will be last while the last one will be first. v = c(43, 82, 11, 73, 34) rev(v) ## [1] 34 73 11 82 43 Sorting Data Frames A data frame can be sorted in descending/ascending order of a certain column. For instance, we will sort the titanic data set in the order of age in ascending order. titanic_by_age &lt;- titanic[order(titanic$Age), ] head(titanic_by_age) ## PassengerId Survived Pclass Name Sex Age ## 804 804 1 3 Thomas, Master. Assad Alexander male 0.42 ## 756 756 1 2 Hamalainen, Master. Viljo male 0.67 ## 470 470 1 3 Baclini, Miss. Helene Barbara female 0.75 ## 645 645 1 3 Baclini, Miss. Eugenie female 0.75 ## 79 79 1 2 Caldwell, Master. Alden Gates male 0.83 ## 832 832 1 2 Richards, Master. George Sibley male 0.83 ## SibSp Parch Ticket Fare Cabin Embarked ## 804 0 1 2625 8.5167 C ## 756 1 1 250649 14.5000 S ## 470 2 1 2666 19.2583 C ## 645 2 1 2666 19.2583 C ## 79 0 2 248738 29.0000 S ## 832 1 1 29106 18.7500 S Sorting the titanic data by age in descending order, - will be added infront of argument titanic$Age to be -titanic$Age titanic_by_age &lt;- titanic[order(-titanic$Age), ] # note the - sign head(titanic_by_age) ## PassengerId Survived Pclass Name Sex Age ## 631 631 1 1 Barkworth, Mr. Algernon Henry Wilson male 80.0 ## 852 852 0 3 Svensson, Mr. Johan male 74.0 ## 97 97 0 1 Goldschmidt, Mr. George B male 71.0 ## 494 494 0 1 Artagaveytia, Mr. Ramon male 71.0 ## 117 117 0 3 Connors, Mr. Patrick male 70.5 ## 673 673 0 2 Mitchell, Mr. Henry Michael male 70.0 ## SibSp Parch Ticket Fare Cabin Embarked ## 631 0 0 27042 30.0000 A23 S ## 852 0 0 347060 7.7750 S ## 97 0 0 PC 17754 34.6542 A5 C ## 494 0 0 PC 17609 49.5042 C ## 117 0 0 370369 7.7500 Q ## 673 0 0 C.A. 24580 10.5000 S Also, dataframes can be sorted based by multiple columns. Lets sort the titanic data set by Passenger class (Pclass) in ascending order and by age in descending order at once. # Pclass in ascending order, Age in descending order titanic_sorted_by_class_and_age &lt;- titanic[order(titanic$Pclass, -titanic$Age), ] head(titanic_sorted_by_class_and_age) ## PassengerId Survived Pclass Name Sex Age ## 631 631 1 1 Barkworth, Mr. Algernon Henry Wilson male 80 ## 97 97 0 1 Goldschmidt, Mr. George B male 71 ## 494 494 0 1 Artagaveytia, Mr. Ramon male 71 ## 746 746 0 1 Crosby, Capt. Edward Gifford male 70 ## 55 55 0 1 Ostby, Mr. Engelhart Cornelius male 65 ## 457 457 0 1 Millet, Mr. Francis Davis male 65 ## SibSp Parch Ticket Fare Cabin Embarked ## 631 0 0 27042 30.0000 A23 S ## 97 0 0 PC 17754 34.6542 A5 C ## 494 0 0 PC 17609 49.5042 C ## 746 1 1 WE/P 5735 71.0000 B22 S ## 55 0 1 113509 61.9792 B30 C ## 457 0 0 13509 26.5500 E38 S 4.1.4 Basic Data Cleaning Data Cleaning is the process of fixing, removing incorrect, incomplete or otherwise problematic data from a data set. This is a crucial step in data analysis as it leads to more reliable analyses and insights. Here some data cleaning techniques used; Handling Missing Data The null values are identified by is.na() function. If there exists null values, they are removed by na.omit() function. The null values can also be replaced by appropriate substitutes such as the mean, median, zero value or a placeholder, this is referred to as imputation. Lets create a vector myvector will null values, identify the null values, remove/impute them. # Create a vector that has missing values my_vector &lt;- c(12, 43, NA, 32, 65, 11, NA, NA, 34, 98, 57) # NA is the missing value # Identify existence of the null values is.na(my_vector) ## [1] FALSE FALSE TRUE FALSE FALSE FALSE TRUE TRUE FALSE FALSE FALSE # Count the null values sum(is.na(my_vector)) ## [1] 3 # remove null values clean_vector &lt;- na.omit(my_vector) clean_vector ## [1] 12 43 32 65 11 34 98 57 ## attr(,&quot;na.action&quot;) ## [1] 3 7 8 ## attr(,&quot;class&quot;) ## [1] &quot;omit&quot; # impute missing values my_vector[is.na(my_vector)] &lt;- mean(my_vector, na.rm = TRUE) my_vector ## [1] 12 43 44 32 65 11 44 44 34 98 57 Removing Duplicates In a raw data set there may exist some duplicated entries/records/rows that will give false results in data analysis thereby leading to poor insights and decision-making. The duplicates are identified by duplicated() function. If there exists any duplicates, they are removed by subsetting or calling unique() function. Lets create a data frame with duplicate values and remove them # Creating a simple data frame data &lt;- data.frame( Position = c(1, 2, 3, 4, 5, 3), # Notice the duplicate position Name = c(&quot;Alice&quot;, &quot;Bob&quot;, &quot;Charlie&quot;, &quot;David&quot;, &quot;Eva&quot;, &quot;Charlie&quot;), Age = c(25, 30, NA, 40, 29, NA), # NA represents null values Score = c(85, 90, 88, NA, 92, 88) # NA represents null values ) # Position 3 is duplicated # Display the data frame print(data) ## Position Name Age Score ## 1 1 Alice 25 85 ## 2 2 Bob 30 90 ## 3 3 Charlie NA 88 ## 4 4 David 40 NA ## 5 5 Eva 29 92 ## 6 3 Charlie NA 88 print(&quot;CLEAN DATA&quot;) ## [1] &quot;CLEAN DATA&quot; # Remove the duplicate row clean_data &lt;- unique(data) clean_data ## Position Name Age Score ## 1 1 Alice 25 85 ## 2 2 Bob 30 90 ## 3 3 Charlie NA 88 ## 4 4 David 40 NA ## 5 5 Eva 29 92 Also the duplicated rows can be removed by clean_data2 &lt;- data[!duplicated(data), ] clean_data2 ## Position Name Age Score ## 1 1 Alice 25 85 ## 2 2 Bob 30 90 ## 3 3 Charlie NA 88 ## 4 4 David 40 NA ## 5 5 Eva 29 92 Handling Outliers Outliers are extreme values that differ from most other data points in a data set. hey can be detected by identifying the upper bound and lower bound using boxplots. They are removed by; clean_data &lt;- raw_data[raw_data$column &lt; upper_bound &amp; raw_data$column &gt; lower_bound, ] Standardizing Data There are numerous ways that data can be standardized such ensuring a consistent date format across the data set and correcting case. Lets create a vector with different date formats and make it consistent # Creating a vector with different date formats date_vector &lt;- c(&quot;2023-08-01&quot;, &quot;01/08/2023&quot;, &quot;August 1, 2023&quot;, &quot;20230801&quot;, &quot;08-01-2023&quot;) # Converting the date_vector to a consistent format clean_date_vector &lt;- as.Date(date_vector, format=&quot;%Y-%m-%d&quot;) # Display the clean date vector print(clean_date_vector) ## [1] &quot;2023-08-01&quot; NA NA NA &quot;8-01-20&quot; The lubridate package can do it better # Load necessary library library(lubridate) ## ## Attaching package: &#39;lubridate&#39; ## The following objects are masked from &#39;package:base&#39;: ## ## date, intersect, setdiff, union # Correcting each date format using lubridate functions clean_date_vector &lt;- c( as.Date(date_vector[1], format=&quot;%Y-%m-%d&quot;), # &quot;2023-08-01&quot; as.Date(date_vector[2], format=&quot;%d/%m/%Y&quot;), # &quot;01/08/2023&quot; as.Date(date_vector[3], format=&quot;%B %d, %Y&quot;), # &quot;August 1, 2023&quot; as.Date(date_vector[4], format=&quot;%Y%m%d&quot;), # &quot;20230801&quot; as.Date(date_vector[5], format=&quot;%m-%d-%Y&quot;) # &quot;08-01-2023&quot; ) # Display the clean date vector print(clean_date_vector) ## [1] &quot;2023-08-01&quot; &quot;2023-08-01&quot; NA &quot;2023-08-01&quot; &quot;2023-08-01&quot; Practical exercise You will use the car sales data set found here to; Delete all the records that have any null value Remove duplicated observations Solution You will use the car sales data set found here to; Delete all the records that have any null value # Read the data sets car_sales &lt;- read.csv(&quot;data/car_sales.csv&quot;) # file path can be the whole link # i. Delete all the records that have any null value car_sales_1 &lt;- na.omit(car_sales) Remove duplicated observations # ii. Remove duplicated observations car_sales_2 &lt;- car_sales_1[!duplicated(car_sales_1), ] ________________________________________________________________________________ 4.1.5 Hands-on Exercises Download the Bengaluru Restaurants Dataset from here and use it to for the activities below. Subsetting and Filtering Subset the data set to include only restaurants with a rating of 4.0 and above. Filter out restaurants that have zero reviews or null ratings. Sorting Sort the data set by restaurant ratings in descending order. Sort the data set by the number of reviews a restaurant received, with the highest reviews appearing last Data Cleaning Identify and remove rows with missing or null values in any key columns (e.g., ratings, cost, cuisine type). Solution Download the Bengaluru Restaurants Dataset from here and use it to for the activities below. Subsetting and Filtering Subset the data set to include only restaurants with a rating of 4.0 and above. df &lt;- read.csv(&quot;data/Bengaluru_Restaurants.csv&quot;) # Subsetting and Filtering ## Subset the data set to include only restaurants with a rating of 4.0 and above. high_rated_restaurants &lt;- df[df$rating &gt;= 4.0, ] head(high_rated_restaurants[, c(&quot;name&quot;, &quot;rating&quot;) ]) # show the restaurants ## name rating ## 1 Absolute Barbecues - Marathahalli, Bengaluru 4.5 ## 2 Cuppa Redefined - HSR 5.0 ## 3 Bharjari Oota 4.0 ## 4 The Big Barbeque 4.5 ## 5 Savoury Restaurant 4.0 ## 6 Rajdhani Thali Restaurant, Bannerghatta Road, Bangalore 4.0 Filter out restaurants that have zero reviews or null ratings. ## Filter out restaurants that have no reviews or ratings. # Filter out restaurants with no reviews or ratings filtered_restaurants &lt;- df[df$numberOfReviews&gt;0 &amp; !is.na(df$rating), ] head(filtered_restaurants[, c(&quot;name&quot;, &quot;rating&quot;, &quot;numberOfReviews&quot;) ]) ## name rating ## 1 Absolute Barbecues - Marathahalli, Bengaluru 4.5 ## 2 Cuppa Redefined - HSR 5.0 ## 3 Bharjari Oota 4.0 ## 4 The Big Barbeque 4.5 ## 5 Savoury Restaurant 4.0 ## 6 Rajdhani Thali Restaurant, Bannerghatta Road, Bangalore 4.0 ## numberOfReviews ## 1 816 ## 2 25 ## 3 21 ## 4 45 ## 5 122 ## 6 78 Sorting Sort the data set by restaurant ratings in descending order. # Sort the restaurants by ratings in descending order sorted_restaurants &lt;- df[order(-df$rating), ] # Display the sorted data head(sorted_restaurants[, c(&quot;name&quot;, &quot;rating&quot;)]) ## name rating ## 2 Cuppa Redefined - HSR 5 ## 14 The Merak Brewhouse 5 ## 18 Vanamo 5 ## 22 Wabi Sabi 5 ## 23 7rivers Brewing Co 5 ## 50 Dum Pukht 5 Sort the data set by the number of reviews a restaurant received, with the highest reviews appearing last. # Sort the restaurants by no of reviews in ascending order sorted_restaurants &lt;- df[order(df$numberOfReviews), ] # Display the sorted data tail(sorted_restaurants[, c(&quot;name&quot;, &quot;numberOfReviews&quot;)]) ## name numberOfReviews ## 475 Toit Bangalore 2161 ## 133 Time Traveller 2427 ## 295 Karavalli @ Taj 2510 ## 121 Byg Brewski Brewing Company 2689 ## 117 MTR - Mavalli Tiffin Rooms 2713 ## 107 Barbeque Nation - Indiranagar, Bangalore 3336 Data Cleaning Identify and remove rows with missing or null values in any key columns (e.g., ratings, cost, cuisine type). sum(is.na(df)) # count the null values ## [1] 15856 # Count the null values complete_data &lt;- na.omit(df) # Confirm the operation sum(is.na(complete_data)) ## [1] 0 ________________________________________________________________________________ 4.2 Data Manipulation with Dplyr 4.2.1 Introduction to Dplyr package Dplyr is a package designed for data manipulation equipped with a set of intuitive functions to perform tasks like filtering rows, selecting columns, rearranging data and summarizing information. The package is part of a larger library, tidyverse. The tidyverse package is a package designed for data science that share an underlying design philosophy, grammar and data structures. The packages within the tidyverse are widely used for data manipulation, exploration, and visualization in R. Here are some of the core packages in tidyverse; ggplot2 dplyr tidyr readr purrr tibble The tidyverse package is installed by install.packages(&quot;tidyverse&quot;) To invoke the package into the system, the below command is invoked library(tidyverse) ## ── Attaching core tidyverse packages ────────────────── ## ✔ dplyr 1.1.4 ✔ stringr 1.5.1 ## ✔ forcats 1.0.0 ✔ tibble 3.2.1 ## ✔ ggplot2 3.5.1 ✔ tidyr 1.3.1 ## ✔ purrr 1.0.2 ## ── Conflicts ───────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() ## ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors In this course, we will discuss on dplyr since it is an essential tool in data analysis. If you want to use dplyr alone then it can be installed by; install.packages(&quot;dplyr&quot;) To load the library into the system; library(dplyr) 4.2.2 Key Functions in dplyr There are some functions that are use by data scientist when working with dplyr, they are referred to as dplyr verbs. To explain these verbs better, we will use an example data set to explain. A good example is the famous iris data set, it is always used by beginners in data science. The data set contains measurements of various characteristics of iris flowers. These characteristics include sepal length, sepal width, petal length, and petal width. There are three species of iris flowers in the data set: setosa, versicolor, and virginica. The data will be invoked to R before assessment and wrangling; Lets load the iris data and explore the first few records. data(&quot;iris&quot;) head(iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa 4.2.3 select This dplyr verb is used when selecting or dropping specific columns. In this lesson we will find the iris column names and select two of them using select. data(iris) # Find the column names colnames(iris) ## [1] &quot;Sepal.Length&quot; &quot;Sepal.Width&quot; &quot;Petal.Length&quot; &quot;Petal.Width&quot; &quot;Species&quot; Remember the data frame to work on need to be specified in the arguments such that selected_data = select(data_frame, col1, col2, col3) Therefore, we will select the columns; Species, Petal length and petal width. # Load the required libraries library(dplyr) # Load the iris data data(iris) selected_iris_data = select(iris, Petal.Length, Petal.Width, Species) # view the first few rows of the selected data head(selected_iris_data) ## Petal.Length Petal.Width Species ## 1 1.4 0.2 setosa ## 2 1.4 0.2 setosa ## 3 1.3 0.2 setosa ## 4 1.5 0.2 setosa ## 5 1.4 0.2 setosa ## 6 1.7 0.4 setosa The three selected columns are displayed in the data frame above. Specific columns can be dropped by putting - before the column name as # Drop specified columns remaining_data = select(data_frame, -col1_to_drop, -col2_to_drop) In this lesson, we will drop petal length, petal width and Species columns; # Load the required libraries library(dplyr) # Load the iris data data(iris) # Drop some columns remaining_iris_data = select(iris, -Petal.Length, -Petal.Width, -Species) # view the first few rows of the selected data head(remaining_iris_data) ## Sepal.Length Sepal.Width ## 1 5.1 3.5 ## 2 4.9 3.0 ## 3 4.7 3.2 ## 4 4.6 3.1 ## 5 5.0 3.6 ## 6 5.4 3.9 Practical exercise You will be required to use the car_sales data set from https://raw.githubusercontent.com/insaid2018/Term-1/master/Data/Projects/car_sales.csv. Read the data using read.csv and select the car, price, body, mileage, engV, engType, year, model. Save the data frame from the selected columns as selected_cars_df. Show the first few rows of the selected_cars_df. Solution library(dplyr) # Load the data filepath = &quot;https://raw.githubusercontent.com/insaid2018/Term-1/master/Data/Projects/car_sales.csv&quot; car_sales &lt;- read.csv(filepath) # select the required columns selected_cars_df &lt;- select(car_sales, car, price, body, mileage, engV, engType, year, model # mention the columns ) # Show the first few rows of the data set head(selected_cars_df) ## car price body mileage engV engType year model ## 1 Ford 15500 crossover 68 2.5 Gas 2010 Kuga ## 2 Mercedes-Benz 20500 sedan 173 1.8 Gas 2011 E-Class ## 3 Mercedes-Benz 35000 other 135 5.5 Petrol 2008 CL 550 ## 4 Mercedes-Benz 17800 van 162 1.8 Diesel 2012 B 180 ## 5 Mercedes-Benz 33000 vagon 91 NA Other 2013 E-Class ## 6 Nissan 16600 crossover 83 2.0 Petrol 2013 X-Trail ________________________________________________________________________________ 4.2.4 filter Is a verb/function from dplyr used to filter records in a data frame based on a specific condition. It allows the analyst to retrieve the records he/she is interested in and work easier with the subset. With filter(), the data frame and the condition are passed as a arguments; # Filtering rows where a certain column meets a condition filtered_data = filter(data_frame, column_name &gt; 5 # This is the condition) Lets select the species ‘setosa’ from the iris data set # Load the required libraries library(dplyr) # Load the iris data data(iris) # Filter to select Setosa setosa_iris = filter(iris, # the data frame Species == &quot;setosa&quot; # the condition ) # First few records of setosa data head(setosa_iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa Records with sepal width of more than 3.0 can be filtered. Here is how we achieve such a subset # Load the required libraries library(dplyr) # Load the iris data data(iris) # Filtered to select records with more than 3.0 sepal width wide_sepal_iris = filter(iris, #the data frame Sepal.Width&gt;3.0 # the condition ) head(wide_sepal_iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.7 3.2 1.3 0.2 setosa ## 3 4.6 3.1 1.5 0.2 setosa ## 4 5.0 3.6 1.4 0.2 setosa ## 5 5.4 3.9 1.7 0.4 setosa ## 6 4.6 3.4 1.4 0.3 setosa Practical exercise With the car_sales data set that you used above, use filter() function to get the cars that were sold from the year 2008 to present and name them latest_car_sales. Count the number of observations made and show the first few rows. Solution library(dplyr) # Load the data filepath = &quot;https://raw.githubusercontent.com/insaid2018/Term-1/master/Data/Projects/car_sales.csv&quot; car_sales &lt;- read.csv(filepath) # Filter to find cars sold from 2008 latest_car_sales = filter(car_sales, year&gt;=2008) # Count the observations made. Use nrow function nrow(latest_car_sales) ## [1] 5089 # Show the first few rows head(latest_car_sales) ## car price body mileage engV engType registration year model ## 1 Ford 15500 crossover 68 2.5 Gas yes 2010 Kuga ## 2 Mercedes-Benz 20500 sedan 173 1.8 Gas yes 2011 E-Class ## 3 Mercedes-Benz 35000 other 135 5.5 Petrol yes 2008 CL 550 ## 4 Mercedes-Benz 17800 van 162 1.8 Diesel yes 2012 B 180 ## 5 Mercedes-Benz 33000 vagon 91 NA Other yes 2013 E-Class ## 6 Nissan 16600 crossover 83 2.0 Petrol yes 2013 X-Trail ## drive ## 1 full ## 2 rear ## 3 rear ## 4 front ## 5 ## 6 full ________________________________________________________________________________ 4.2.5 arrange This is dplyr verb/function used for sorting rows by rearranging in a specific order. here is how to use arrange() function; arranged_data = arrange(data_frame, column_name) This allows the analyst to arrange the data in a default ascending order. To arrange in a descending order a desc() function is added as; # Note the additional desc function arranged_data = arrange(data_frame, desc(column_name)) Now lets order the iris data in an ascending order based on Petal length and view the first 6 records with the shortest petal. # Load the required libraries library(dplyr) # Load the iris data data(iris) # Sort the data by_petal_length = arrange(iris, # data frame Petal.Length # order by column ) # View the data head(by_petal_length) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 4.6 3.6 1.0 0.2 setosa ## 2 4.3 3.0 1.1 0.1 setosa ## 3 5.8 4.0 1.2 0.2 setosa ## 4 5.0 3.2 1.2 0.2 setosa ## 5 4.7 3.2 1.3 0.2 setosa ## 6 5.4 3.9 1.3 0.4 setosa Lets repeat the same process but now we order the data in a descending order. # Sort the data by_petal_length = arrange(iris, # data frame desc(Petal.Length) # order by column ) # View the data head(by_petal_length) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 7.7 2.6 6.9 2.3 virginica ## 2 7.7 3.8 6.7 2.2 virginica ## 3 7.7 2.8 6.7 2.0 virginica ## 4 7.6 3.0 6.6 2.1 virginica ## 5 7.9 3.8 6.4 2.0 virginica ## 6 7.3 2.9 6.3 1.8 virginica Practical exercise Arrange the columns in the car_sales data set according to mileage in descending order. Show the last few rows Solution library(dplyr) # Load the data filepath = &quot;https://raw.githubusercontent.com/insaid2018/Term-1/master/Data/Projects/car_sales.csv&quot; car_sales &lt;- read.csv(filepath) # Order according to mileage in descending order mileage_order = arrange(car_sales, desc(mileage)) # Show the last few rows of the data set tail(mileage_order) ## car price body mileage engV engType registration year ## 9571 Land Rover 67999.00 crossover 0 3.0 Diesel yes 2016 ## 9572 Hyundai 12800.77 hatch 0 1.4 Petrol yes 2016 ## 9573 Subaru 37500.00 crossover 0 2.0 Diesel yes 2016 ## 9574 Suzuki 15486.90 hatch 0 1.2 Petrol yes 2016 ## 9575 Opel 20120.00 sedan 0 1.6 Diesel yes 2016 ## 9576 Nissan 29077.95 crossover 0 1.6 Diesel yes 2016 ## model drive ## 9571 Discovery full ## 9572 Solaris front ## 9573 Forester full ## 9574 Swift front ## 9575 Astra J front ## 9576 X-Trail front ________________________________________________________________________________ 4.2.6 mutate mutate() is a dplyr verb used to modifying the existing variables or creating new variables in a data set. In this case we can calculate the log off Sepal length in the iris data # Load the required libraries library(dplyr) # Load the iris data data(iris) # modify Sepal.Length new_iris = mutate(iris, Sepal.Length=log(Sepal.Length)) head(new_iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 1.629241 3.5 1.4 0.2 setosa ## 2 1.589235 3.0 1.4 0.2 setosa ## 3 1.547563 3.2 1.3 0.2 setosa ## 4 1.526056 3.1 1.5 0.2 setosa ## 5 1.609438 3.6 1.4 0.2 setosa ## 6 1.686399 3.9 1.7 0.4 setosa Additionally, we can create an entirely new variable by mutate(). In this case we will find the ratio between petal length and petal width. The new variable will be called “Petal.Length.Width.Ratio” # Load the required libraries library(dplyr) # Load the iris data data(iris) # Create a new column in the data set new_iris = mutate(iris, Petal.Length.Width.Ratio = Petal.Length/Petal.Width) head(new_iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa ## Petal.Length.Width.Ratio ## 1 7.00 ## 2 7.00 ## 3 6.50 ## 4 7.50 ## 5 7.00 ## 6 4.25 The “Petal.Length.Width.Ratio” is found by dividing the Petal.Length and the Petal.Width variables. Practical exercise Using the car_sales data set, create a new column, \"distance_covered_km\", calculated from the mileage. Just multiply mileage with 1.609. Show the first few rows of the mutated data frame. Solution library(dplyr) # Load the data filepath = &quot;https://raw.githubusercontent.com/insaid2018/Term-1/master/Data/Projects/car_sales.csv&quot; car_sales &lt;- read.csv(filepath) # Create new column &quot;distance_covered_km mutated_car_sales = mutate(car_sales, distance_covered_km = mileage * 1.609) # Show first few rows head(mutated_car_sales) ## car price body mileage engV engType registration year model ## 1 Ford 15500 crossover 68 2.5 Gas yes 2010 Kuga ## 2 Mercedes-Benz 20500 sedan 173 1.8 Gas yes 2011 E-Class ## 3 Mercedes-Benz 35000 other 135 5.5 Petrol yes 2008 CL 550 ## 4 Mercedes-Benz 17800 van 162 1.8 Diesel yes 2012 B 180 ## 5 Mercedes-Benz 33000 vagon 91 NA Other yes 2013 E-Class ## 6 Nissan 16600 crossover 83 2.0 Petrol yes 2013 X-Trail ## drive distance_covered_km ## 1 full 109.412 ## 2 rear 278.357 ## 3 rear 217.215 ## 4 front 260.658 ## 5 146.419 ## 6 full 133.547 ________________________________________________________________________________ 4.2.7 Summarise To calculate summary statistics such as average, median and maximum the summarise() is used. This function collapses multiple rows into a summary row. For instance calculating the mean Petal width; # Load the required libraries library(dplyr) # Load the iris data data(iris) # Calculate the mean petal width summarise(iris, mean_petal_width=mean(Petal.Width)) ## mean_petal_width ## 1 1.199333 To find the mean petal width for each iris species; the iris data will be grouped by species a mean value for each group will be calculated # Load the required libraries library(dplyr) # Load the iris data data(iris) # To find the mean petal width for each iris species; # - the iris data will be grouped by species # - a mean value for each group will be calculated grouped_iris = group_by(iris, Species) mean_petal_widths = summarise(grouped_iris, mean_value=mean(Petal.Width)) mean_petal_widths ## # A tibble: 3 × 2 ## Species mean_value ## &lt;fct&gt; &lt;dbl&gt; ## 1 setosa 0.246 ## 2 versicolor 1.33 ## 3 virginica 2.03 Practical exercise You will be required to use the car_sales data set once again. Calculate the descriptive statistics using summarise() command. Solution library(dplyr) # Load the data filepath = &quot;https://raw.githubusercontent.com/insaid2018/Term-1/master/Data/Projects/car_sales.csv&quot; car_sales &lt;- read.csv(filepath) # Calculate the summary statistics summarise(car_sales) ## data frame with 0 columns and 1 row 4.2.8 group_by The group_by() is a function used to group records in a data frame by one or more variables. It allows the analyst to create a group based on a certain criteria. It is always chained together with summarise(). Lets group the iris data based on the Species variable and find the mean of each variable; # Load the required libraries library(dplyr) # Load the iris data data(iris) # Group the iris based on their Species iris_groups = group_by(iris, Species) %&gt;% summarise(across(everything(), mean, na.rm = TRUE)) ## Warning: There was 1 warning in `summarise()`. ## ℹ In argument: `across(everything(), mean, na.rm = ## TRUE)`. ## ℹ In group 1: `Species = setosa`. ## Caused by warning: ## ! The `...` argument of `across()` is deprecated as of ## dplyr 1.1.0. ## Supply arguments directly to `.fns` through an ## anonymous function instead. ## ## # Previously ## across(a:b, mean, na.rm = TRUE) ## ## # Now ## across(a:b, \\(x) mean(x, na.rm = TRUE)) head(iris_groups) ## # A tibble: 3 × 5 ## Species Sepal.Length Sepal.Width Petal.Length Petal.Width ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 setosa 5.01 3.43 1.46 0.246 ## 2 versicolor 5.94 2.77 4.26 1.33 ## 3 virginica 6.59 2.97 5.55 2.03 This groupings allow the analyst to retrieve insights at more base level and uncover more insights that could not have been possible when analyzing the entire data set Practical exercise Use the car_sales data set provided before to work on this activity. Load the data and group the sales by model to get the sum of every quantitative feature/variable. Name the resultant data frame, car_sales_by_model. Display the resultant data frame. Hint: Use across(where(is.numeric), sum) as an argument to summarise instead of across(everything(), mean, na.rm = TRUE) to find the sum of quantitative variables. Solution library(dplyr) # Load the data filepath = &quot;https://raw.githubusercontent.com/insaid2018/Term-1/master/Data/Projects/car_sales.csv&quot; car_sales &lt;- read.csv(filepath) # Group the sales by model car_sales_by_model = group_by(car_sales, model)%&gt;% summarise(across(where(is.numeric), sum)) # Print out the dataframe car_sales_by_model ## # A tibble: 888 × 5 ## model price mileage engV year ## &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 &quot;1 Series&quot; 4100 160 1.6 2004 ## 2 &quot;1.3&quot; 1000 90 1.3 1989 ## 3 &quot;10&quot; 3600 43 1.4 1992 ## 4 &quot;100&quot; 64639 5341 43.1 35827 ## 5 &quot;106&quot; 1900 154 1.4 1993 ## 6 &quot;107&quot; 89127. 1003 11 22137 ## 7 &quot;11&quot; 1800 1 1.4 1988 ## 8 &quot;1102 \\xd2\\xe0\\xe2\\xf0\\xe8\\xff&quot; 24447. 2530 NA 44009 ## 9 &quot;1103 \\xd1\\xeb\\xe0\\xe2\\xf3\\xf2\\xe0&quot; 54198. 4147 NA 58158 ## 10 &quot;110557&quot; 19360 1130 NA 18050 ## # ℹ 878 more rows ________________________________________________________________________________ 4.3 Chaining Chaining is the process of combining several operations together using the %&gt;% or forward pipe operator. The chained workflow succeeds each other until the whole process is done. To understand chaining, the mtcars(Motor Trend cars) data set will be used. Mtcars is also a well-known data set containing several attributes of 32 different cars from 1974. Here’s a brief explanation of the variables in the mtcars data set: mpg: Miles per gallon (fuel efficiency). cyl: Number of cylinders. disp: Displacement (cubic inches). hp: Horsepower. drat: Rear axle ratio. wt: Weight (in 1000 lbs). qsec: Quarter mile time (in seconds). vs: Engine type (0 = V-shaped, 1 = Straight). am: Transmission type (0 = Automatic, 1 = Manual). gear: Number of forward gears. carb: Number of carburetors. Lets start by loading the data into the program and view its first few records; data(mtcars) head(mtcars) ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 ## Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 ## Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 ## Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 Lets select 6 most important columns in this analysis # Load the library library(dplyr) # Load the data data(mtcars) # Lets `select` 6 most important columns in this analysis cars1 = mtcars %&gt;% select(mpg, cyl, disp, hp, qsec, am) head(cars1) ## mpg cyl disp hp qsec am ## Mazda RX4 21.0 6 160 110 16.46 1 ## Mazda RX4 Wag 21.0 6 160 110 17.02 1 ## Datsun 710 22.8 4 108 93 18.61 1 ## Hornet 4 Drive 21.4 6 258 110 19.44 0 ## Hornet Sportabout 18.7 8 360 175 17.02 0 ## Valiant 18.1 6 225 105 20.22 0 Lets now filter to find vehicles with an automatic transmission type. The filter verb will be chained to select verb with %&gt;%. # Load the library library(dplyr) # Load the data data(mtcars) # Selct and filter chained together cars2 = mtcars %&gt;%select(mpg, cyl, disp, hp, qsec, am) %&gt;% filter(am==0) head(cars2) ## mpg cyl disp hp qsec am ## Hornet 4 Drive 21.4 6 258.0 110 19.44 0 ## Hornet Sportabout 18.7 8 360.0 175 17.02 0 ## Valiant 18.1 6 225.0 105 20.22 0 ## Duster 360 14.3 8 360.0 245 15.84 0 ## Merc 240D 24.4 4 146.7 62 20.00 0 ## Merc 230 22.8 4 140.8 95 22.90 0 All these vehicles are of automatic transmission type, lets rank them according to the horsepower in descending order. # Load the library library(dplyr) # Load the data data(mtcars) # Select, filter and arrange chained together cars3= mtcars %&gt;%select(mpg, cyl, disp, hp, qsec, am, wt) %&gt;% filter(am==0) %&gt;% arrange(desc(hp)) head(cars3) ## mpg cyl disp hp qsec am wt ## Duster 360 14.3 8 360.0 245 15.84 0 3.570 ## Camaro Z28 13.3 8 350.0 245 15.41 0 3.840 ## Chrysler Imperial 14.7 8 440.0 230 17.42 0 5.345 ## Lincoln Continental 10.4 8 460.0 215 17.82 0 5.424 ## Cadillac Fleetwood 10.4 8 472.0 205 17.98 0 5.250 ## Merc 450SE 16.4 8 275.8 180 17.40 0 4.070 A new column of weight in 1000kgs (wt_1000kgs) can be created by diving weight in 1000lbs by 2.20462. mutate verb will be chained also. # Load the library library(dplyr) # Load the data data(mtcars) # Multiple chains cars4= mtcars %&gt;%select(mpg, cyl, disp, hp, qsec, am, wt) %&gt;% filter(am==0) %&gt;% arrange(desc(hp)) %&gt;% mutate(wt_1000kgs=wt/2.20462) head(cars4) ## mpg cyl disp hp qsec am wt wt_1000kgs ## Duster 360 14.3 8 360.0 245 15.84 0 3.570 1.619327 ## Camaro Z28 13.3 8 350.0 245 15.41 0 3.840 1.741797 ## Chrysler Imperial 14.7 8 440.0 230 17.42 0 5.345 2.424454 ## Lincoln Continental 10.4 8 460.0 215 17.82 0 5.424 2.460288 ## Cadillac Fleetwood 10.4 8 472.0 205 17.98 0 5.250 2.381363 ## Merc 450SE 16.4 8 275.8 180 17.40 0 4.070 1.846123 The above process has explained how chained works in dplyr. Many functions/processed can be chained together to manipulate data to the desired output. The next section will apply chaining to biology and be used to answer a few questions that will cement your understanding in R as a biologist. 4.3.1 Hands-on Exercises You will be required to download the Furniture Sales Dataset from here. Use the data set to answer the questions below; 1. Perform the following tasks using a combination of select(), filter(), arrange(), mutate(), and group_by(). Select the columns \"category\", \"sales\", and \"profit_margin\". Filter the data set to include only orders with a \"sales\" amount greater than 25, and arrange the data by \"profit_margin\" in descending order. Group the data by \"category\" and calculate the total sales and profit_margin for each category. Arrange the results by total sales in ascending order. Answer the following questions by chaining two or three dplyr verbs: What store type had products made of fabric material sold? What is the average \"profit margin\" for each \"category\" of products? Solution Load the data and libraries library(dplyr) # Load the data furniture_sales &lt;- read.csv(&quot;data/Furniture.csv&quot;) Perform the following tasks using a combination of select(), filter(), arrange(), mutate(), and group_by(). Select the columns \"category\", \"sales\", and \"profit_margin\". Filter the data set to include only orders with a \"sales\" amount greater than 25, and arrange the data by \"profit_margin\" in descending order. high_sales &lt;- furniture_sales %&gt;% select(category, sales, profit_margin)%&gt;% filter(sales &gt; 25)%&gt;% arrange(desc(profit_margin)) Group the data by \"category\" and calculate the total sales and profit_margin for each category. Arrange the results by total sales in ascending order. sales_by_category &lt;- furniture_sales %&gt;% group_by(category) %&gt;% summarise(total_sales = sum(sales), total_profit_margin = sum(profit_margin)) %&gt;% arrange(desc(total_sales)) # View the result print(sales_by_category) ## # A tibble: 5 × 3 ## category total_sales total_profit_margin ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Table 13320 15919. ## 2 Sofa 12585 14670. ## 3 Chair 12286 14988. ## 4 Desk 12151 15114. ## 5 Bed 11968 14830. Answer the following questions by chaining two or three dplyr verbs: What store type had products made of fabric material sold? fabric_stores &lt;- furniture_sales %&gt;% select(store_type, material) %&gt;% # select relevant columns filter(material==&quot;Fabric&quot;) unique(fabric_stores$store_type) ## [1] &quot;Retail&quot; &quot;Online&quot; Both the Online and Retail stores had products made of Fabric materials What is the average \"profit margin\" for each \"category\" of products? profit_margin_by_category &lt;- furniture_sales %&gt;% select(category, profit_margin) %&gt;% # Select relevant columns group_by(category) %&gt;% summarise(total_profit_margin = sum(profit_margin)) profit_margin_by_category ## # A tibble: 5 × 2 ## category total_profit_margin ## &lt;chr&gt; &lt;dbl&gt; ## 1 Bed 14830. ## 2 Chair 14988. ## 3 Desk 15114. ## 4 Sofa 14670. ## 5 Table 15919. ________________________________________________________________________________ "],["data-visualization.html", "Chapter 5 Data Visualization 5.1 Basic Data Visualization 5.2 Advanced Data Visualization", " Chapter 5 Data Visualization 5.1 Basic Data Visualization 5.1.1 Introduction to ggplot2 This is package designed for data visualization. It provides a powerful and flexible framework for creating complex and aesthetically pleasing visualizations, allowing users to layer various components such as axes, scales, colors, and geoms to create detailed and customizable plots. The package is said that it implements the principle of “Grammar of Graphics”. The “Grammar of Graphics” is a conceptual framework for creating graphs by Leland Wilkinson. The process of creating graphs is broken into fundamental components. This allows structure and flexible approach in data visualization. The plots are constructed by a layered approach by implementing these concepts step by step; data, aesthetics(aes), geometrics(geoms), statistical transformations (stats), scales, cordinates and facets. The package is also under the tidyverse package but can be installed by install.packages(&quot;ggplot2&quot;) Once the package is installed, it can be loaded by library(ggplot2) There are 5 key steps in plotting in ggplot; The Setup - Read the data set, define x and y axis The Labels - Title, X and Y axis labels The Theme - Default, Black and White, colored etc. The Facets - Individual Graphs for each group in data with exactly same range The Layers or geoms - The actual plot type - e.g Bar plot, Box plot, Violin plot etc. Here is an example of a layered approach in creating charts by applying the “Grammar of Graphics”. library(ggplot2) # Sample data df &lt;- data.frame( x = rnorm(100), y = rnorm(100), category = sample(c(&quot;A&quot;, &quot;B&quot;), 100, replace = TRUE) ) # Creating a scatter plot ggplot(df, # data aes(x = x, y = y, color = category)) + #aesthetics geom_point() + #geometrics labs(title = &quot;Scatter Plot Example&quot;, x = &quot;X-Axis&quot;, y = &quot;Y-Axis&quot;) + theme_minimal() 5.1.1.1 Creating Basic Plots Scatter Plots Scatter plot is used to show a numerical relationship between two or more variables for instance height versus weight of footballers. It is also used to detect correlation, the increase in one variable can lead to increase/decrease of another variable. Furthermore scatter plots are used to detect outliers, values that appear out of the general patterns of other data points is said to be an outlier in this case. However, scatter plots is not suitable when working with categorical data especially when only two variables are to be analyzed. Lets create a basic scatter plot to compare height and weight;- #Prepare a Sample data df &lt;- data.frame( height = c(150, 160, 170, 180, 190), weight = c(50, 60, 70, 80, 90) ) # Creating a basic scatter plot ggplot(df, # data aes(x = height, y = weight)) + # aesthetics geom_point() To make the chart more informative, titles and labels are added. # Creating a basic scatter plot ggplot(df, # data aes(x = height, y = weight)) + # aesthetics geom_point() + labs(title = &quot;Height vs. Weight&quot;, x = &quot;Height (cm)&quot;, y = &quot;Weight (kg)&quot;) + theme_minimal() Lets explain the chart step by step;- ggplot(df, aes(x = height, y = weight)): Initializes the ggplot object, specifying the data frame (df) and mapping the height variable to the x-axis and the weight variable to the y-axis using the aes() function. geom_point(): Creates the scatter plots by adding dots to the chart labs(): Adds the title and the axis labels to the chart. theme_minimal(): Applies a minimal theme to the plot for a minimalist and clean appearance. Practical exercise Solve the questions below; Using the \"mtcars\" inbuilt data set, plot a scatter chart to visualize the relationship between mpg (miles per gallon) and hp (horsepower). Label the axes appropriately. Hint: the data set can be loaded by data(\"mtcars\") command. Using the airquality data set, plot a scatter plot to show the relationship between Wind and Temp. Hint: the data set can be loaded by data(\"airquality\") command. Solution Using the \"mtcars\" inbuilt data set, plot a scatter chart to visualize the relationship between mpg (miles per gallon) and hp (horsepower). Label the axes appropriately. Hint: the data set can be loaded by data(\"mtcars\") command. # Import the libraries library(ggplot2) # Load the data and show the first few rows data(mtcars) head(mtcars) ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 ## Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 ## Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 ## Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 # Creating a scatter plot ggplot(mtcars, # data aes(x = hp, y = mpg)) + # aesthetics geom_point() Using the airquality data set, plot a scatter plot to show the relationship between Wind and Temp. Hint: the data set can be loaded by data(\"airquality\") command. # Load the data data(&quot;airquality&quot;) head(airquality) ## Ozone Solar.R Wind Temp Month Day ## 1 41 190 7.4 67 5 1 ## 2 36 118 8.0 72 5 2 ## 3 12 149 12.6 74 5 3 ## 4 18 313 11.5 62 5 4 ## 5 NA NA 14.3 56 5 5 ## 6 28 NA 14.9 66 5 6 # Creating a scatter plot ggplot(airquality, # data aes(x = Wind, y = Temp)) + # aesthetics geom_point() ________________________________________________________________________________ Bar Charts Bar charts are used to represent both categorical and numeric data in form of rectangular bars. The length/height of each category represents its numeric value. It may corresponds to either length, count, age or any other numerical value. Bar charts are used when;- Comparing categorical data Visualizing summarized data for instance aggregated sum, average or any other summary statistics. Showing frequency or count for instance representing the number of products sold per each category. Ranking data. Bar charts can effectively represents ranks especially in descending/ascending order for instance ranking the life expectancy of different countries. Other type of complex bar charts like stacked bar charts can be used to compare part-to-whole relationships. There are many more uses of bar charts however there are some use cases where bar charts are not preferred like when working with continuous data, scatter and line charts are more befitting. Also, bar charts are not appropriate where data has too many categories, heatmaps will do better. To create a simple bar chart using ggplot2, we use geom_bar to define thats its a bar chart. # Sample data df &lt;- data.frame( category = c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;), value = c(23, 17, 35, 10) ) ## The data set above will be used to create a bar chart # Creating a bar chart ggplot(df, aes(x = category, y = value)) + geom_bar(stat = &quot;identity&quot;) + labs(title = &quot;Value by Category&quot;, x = &quot;Category&quot;, y = &quot;Value&quot;) + theme_minimal() Practical exercise Using the mtcars data set, create a bar plot to visualize the number of cars for each unique number of cylinders (cyl) in the mtcars data set. Label the bars with the appropriate cylinder categories and provide axis labels for clarity. Solution library(ggplot2) library(dplyr) # Load the data data(&quot;mtcars&quot;) # Apply data manipulation techniques mtcars_by_cyl &lt;- mtcars %&gt;% group_by(cyl) %&gt;% summarise(number_of_cars = n()) # get the number of cars per each cylinder # Creating a bar chart ggplot(mtcars_by_cyl, aes(x = as.character(cyl), y = number_of_cars)) + geom_bar(stat = &quot;identity&quot;) + labs(title = &quot;Cars by Cyclinders&quot;, x = &quot;Number of Cylinders&quot;, y = &quot;Number of Cars&quot;) + theme_minimal() ________________________________________________________________________________ Histograms Histogram is visually similar to the bar chart however it is used show frequency and distribution across a list-like data set(vectors, lists, sets, arrays, etc) that stores continuous numeric values. The count of observation within a certain range of values are displayed. Lets create a vector of random 100 ages and plot the data to a histogram. # Generate random 1000 ages between 0 and 100 set.seed(42) ages &lt;- sample(0:100, 1000, replace = TRUE) # Create a data frame to use with ggplot age_data &lt;- data.frame(Age = ages) # PLOTTING # Create the histogram using ggplot ggplot(age_data, aes(x = Age)) + geom_histogram(binwidth = 5, fill = &quot;blue&quot;, color = &quot;black&quot;) + labs(title = &quot;Histogram of Randomly Generated Ages&quot;, x = &quot;Age&quot;, y = &quot;Frequency&quot;) + theme_minimal() The bins are groups of ages ranging 5 years. Practical exercise Using the rivers in built R data set, plot a histogram to visualize the distribution of rivers’ length. Hint: The data set is loaded by the command data(\"rivers\"). Solution library(ggplot2) # Load the data data(&quot;rivers&quot;) # Create the histogram using ggplot ggplot(as.data.frame(rivers), aes(x = rivers)) + geom_histogram(bins = 8, fill = &quot;blue&quot;, color = &quot;black&quot;) + labs(title = &quot;Distribution of Rivers&#39; Length&quot;, x = &quot;rivers&quot;, y = &quot;frequency&quot;) + theme_minimal() ________________________________________________________________________________ 5.1.1.2 Customizing plots Themes Themes in ggplot2 control the overall appearance of the plot for instance the grid line, font style, background color etc. To clearly explain the theme concept we will create a sample data set and plot it. The theme will be altered to show the changes in the appearance. Create a data set and plot a basic bar plot from ggplot2 # Sample data df &lt;- data.frame( category = c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;), value = c(23, 17, 35, 10) ) # Plot a basic bar chart # Create a basic bar plot ggplot(df, aes(x = category, y = value)) + geom_bar(stat = &quot;identity&quot;) Lets apply some built-in themes from ggplot. In this case, we will use the theme_minimal(). # Bar plot with theme_minimal ggplot(df, aes(x = category, y = value)) + geom_bar(stat = &quot;identity&quot;) + theme_minimal() The theme_minimal() removes background colors, extra gridlines and axes’ tickers to give the a chart a clean look. Now, lets try with the theme_classic() # Bar plot with theme_classic ggplot(df, aes(x = category, y = value)) + geom_bar(stat = &quot;identity&quot;) + theme_classic() The theme_classic() gives the bar plot a clear white background, black borderline with raised 0 point on the axes and bars. The third theme in this course is the theme_light(). Lets try it with the same data set. # Bar plot with theme_light ggplot(df, aes(x = category, y = value)) + geom_bar(stat = &quot;identity&quot;) + theme_light() The theme_light() is gives grey gridlines, grey borderline and raised bars(not touching the axes). The last theme in this list is the theme_dark(). Lets try it below here; # Bar plot with theme_dark ggplot(df, aes(x = category, y = value)) + geom_bar(stat = &quot;identity&quot;) + theme_dark() The theme_dark() is a clear opposite of theme_light as it produces a plot with a dark grid lines. There are more themes provided in ggplot2 like theme_bw() Practical exercise Using the mtcars data set, create a bar plot to visualize the number of cars for each unique number of cylinders (cyl) in the mtcars data set. Apply different themes to the bar plot. Solution Preparing the data library(ggplot2) library(dplyr) # Load the data data(&quot;mtcars&quot;) mtcars_by_cyl &lt;- mtcars %&gt;% group_by(cyl) %&gt;% summarise(number_of_cars = n()) # get the number of cars per each cylinder Minimal theme # Creating a bar chart ggplot(mtcars_by_cyl, aes(x = as.character(cyl), y = number_of_cars)) + geom_bar(stat = &quot;identity&quot;) + labs(title = &quot;Cars by Cyclinders&quot;, x = &quot;Number of Cylinders&quot;, y = &quot;Number of Cars&quot;) + theme_minimal() Classic theme # Creating a bar chart ggplot(mtcars_by_cyl, aes(x = as.character(cyl), y = number_of_cars)) + geom_bar(stat = &quot;identity&quot;) + labs(title = &quot;Cars by Cyclinders&quot;, x = &quot;Number of Cylinders&quot;, y = &quot;Number of Cars&quot;) + theme_classic() Light theme # Creating a bar chart ggplot(mtcars_by_cyl, aes(x = as.character(cyl), y = number_of_cars)) + geom_bar(stat = &quot;identity&quot;) + labs(title = &quot;Cars by Cyclinders&quot;, x = &quot;Number of Cylinders&quot;, y = &quot;Number of Cars&quot;) + theme_light() Dark theme # Creating a bar chart ggplot(mtcars_by_cyl, aes(x = as.character(cyl), y = number_of_cars)) + geom_bar(stat = &quot;identity&quot;) + labs(title = &quot;Cars by Cyclinders&quot;, x = &quot;Number of Cylinders&quot;, y = &quot;Number of Cars&quot;) + theme_dark() ________________________________________________________________________________ Labels and Titles The appearance of the charts can be made more appealing and informative by adding labels and titles. This can be achieved adding the function labs to a ready plot. Lets plot the same bar chart as above and add more information with labels; # Create a basic bar plot p &lt;- ggplot(df, aes(x = category, y = value)) + geom_bar(stat = &quot;identity&quot;) p # Show the plot The above chart is stored in a variable p. Lets add/modify the labels and titles using the labs() function. # Bar plot with title, axis labels, and caption p + labs( title = &quot;Bar Plot of Categories&quot;, subtitle = &quot;Values by Category&quot;, x = &quot;Category&quot;, y = &quot;Value&quot;, caption = &quot;Source: Sample Data&quot; ) # Remember `p` is the bar plot The bar plot is more informative now. Lets make the chart more appealing by modifying the appearance of the labels using the theme() function along with the element_text() function. # Bar plot with customized title and labels p + labs( title = &quot;Bar Plot of Categories&quot;, subtitle = &quot;Values by Category&quot;, x = &quot;Category&quot;, y = &quot;Value&quot;, caption = &quot;Source: Sample Data&quot; ) + theme( plot.title = element_text(hjust = 0.5, face = &quot;bold&quot;, size = 16), plot.subtitle = element_text(hjust = 0.5, size = 14), axis.title.x = element_text(face = &quot;italic&quot;, size = 12), axis.title.y = element_text(face = &quot;italic&quot;, size = 12), plot.caption = element_text(hjust = 0, face = &quot;italic&quot;, size = 10) ) Practical exercise Using the mtcars data set, create a bar plot to visualize the number of cars for each unique number of cylinders (cyl) in the mtcars data set. Add x, y axes labels and titles to the plot. Solution # Creating a bar chart ggplot(mtcars_by_cyl, aes(x = as.character(cyl), y = number_of_cars)) + geom_bar(stat = &quot;identity&quot;) + labs(title = &quot;Cars by Cyclinders&quot;, x = &quot;Number of Cylinders&quot;, y = &quot;Number of Cars&quot;) ________________________________________________________________________________ Color and aesthetics The general aesthetics are essential when presenting the analysis results. They enhance the visual appearance of the charts. It also allows the users/analyst to distinguish different properties/categories in a variable. These are; color, size and shape. Lets plot a simple bar chart and add a flavor of aesthetics. # Sample data with an additional variable. Modified the original one df &lt;- data.frame( category = c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;), value = c(23, 17, 35, 10), group = c(&quot;G1&quot;, &quot;G2&quot;, &quot;G1&quot;, &quot;G2&quot;) ) # Bar plot with mapped color, size, and shape ggplot(df, aes(x = category, y = value, color = group, size = value, shape = group)) + geom_point() Lets break down the scatter plot on how aesthetics are applied. color = group: Maps the group variable to the color of the points. Each group will have a different color. size = value: Maps the value variable to the size of the points. Points with higher values will be larger. shape = group: Maps the group variable to the shape of the points. Different groups will have different shapes. Above there, the color was selected automatically by R. It can also be customized to fit our taste. Lets do a bit of color customization. # Customizing colors manually ggplot(df, aes(x = category, y = value, color = group)) + geom_point(size = 5) + scale_color_manual(values = c(&quot;G1&quot; = &quot;blue&quot;, &quot;G2&quot; = &quot;red&quot;)) The scale_color_manual() function allows you to manually set the colors for each level of the group variable. In this case, G1 is set to blue and G2 to red. Practical exercise Using the mtcars data set, create a bar plot to visualize the number of cars for each unique number of cylinders (cyl) in the mtcars data set. Play with different foreground and background colors to come up with the best looking plot. Solution Let the student show his best chart # Load necessary libraries library(ggplot2) # Create the bar plot ggplot(mtcars, aes(x = factor(cyl))) + geom_bar(fill = &quot;#4C72B0&quot;, color = &quot;#DD8452&quot;, width = 0.7) + # Custom colors for bars and borders labs(title = &quot;Number of Cars by Cylinders&quot;, x = &quot;Number of Cylinders&quot;, y = &quot;Count of Cars&quot;) + theme_minimal(base_size = 15) + # Minimal theme theme( panel.background = element_rect(fill = &quot;#F7F7F7&quot;), # Light background plot.title = element_text(hjust = 0.5, color = &quot;#4C72B0&quot;), # Centered and colored title axis.text.x = element_text(color = &quot;#DD8452&quot;), # Custom axis text color axis.text.y = element_text(color = &quot;#DD8452&quot;) ) ________________________________________________________________________________ 5.2 Advanced Data Visualization 5.2.1 Creating Complex Visualizations Boxplots Box plot is a type of chart that uses boxes and lines to show distributions of one or more groups of a continuous numeric data. They provide a high-level information at a glance for instance the data symmetry, skew and variance. They can also be used to show outliers within a data set. This is the only chart that can effectively give the measure of spread and central tendency(median, quartiles and range). Lets plot a simple boxplot for the age of employees from the IBM Human Resource Analytics data; The data can be downloaded from here df &lt;- read.csv(&quot;data/HR-Employee-Attrition.csv&quot;) # Plot a boxplot ggplot(df, aes(y=Age))+ geom_boxplot(fill=&quot;black&quot;) The upper quartile for the employees’ age is 43(Q3) while the lower quartile(Q1) is 30.The median age is approximately 36 (Median is represented by the line that appears in the middle of the box). There are no outliers in the dataset. Lets compare the distribution of ages between male and female employees; (The x argument, in this case Gender will be introduced in aes() function to achieve comparison between genders) # Plot a boxplot ggplot(df, aes(y=Age, x=Gender))+ geom_boxplot(fill=c(&quot;pink&quot;, &quot;lightblue&quot;)) The female employee have an older working population than the male counterparts with a median age of approximately 36 and 35 respectively. The distance between Q3 and Q1 (in this case the height of the box) is know as the Interquartile Range(IQR). The above boxplots are vertical, they can be shifted to horizontal by interchanging the y and x argument in the function aes(). Lets do it; # Plot a horizontal boxplot ggplot(df, aes(x=Age, y=Gender))+ geom_boxplot(fill=c(&quot;pink&quot;, &quot;lightblue&quot;)) Lets add a few customization to make the graph more informative and visually appealing # BOXPLOT ggplot(df, aes(x=Age, y=Gender))+ geom_boxplot(fill=c(&quot;pink&quot;, &quot;lightblue&quot;))+ theme_dark() + labs( title = &quot;IBM HR Analytics&quot;, subtitle = &quot;Boxplots of Employees&#39; age by Gender&quot;, x = &quot;Age&quot;, y = &quot;Gender&quot;, caption = &quot;Source: Kaggle&quot; ) + theme( plot.title = element_text(hjust = 0.5, face = &quot;bold&quot;, size = 16), plot.subtitle = element_text(hjust = 0.5, size = 14), axis.title.x = element_text(face = &quot;italic&quot;, size = 12), axis.title.y = element_text(face = &quot;italic&quot;, size = 12), plot.caption = element_text(hjust = 0, face = &quot;italic&quot;, size = 10) ) Practical exercise Load the inbuilt ToothGrowth data set and create a boxplot to compare the distribution of tooth length (len) across the different dose levels (dose) of Vitamin C in the ToothGrowth data set. Make sure to label the axes and title the plot appropriately. Solution library(ggplot2) library(dplyr) # Load the data data(&quot;ToothGrowth&quot;) # Filter to get Vitamin C vit_C_data &lt;- ToothGrowth %&gt;% filter(supp==&quot;VC&quot;) # CREATE BOXPLOT ggplot(vit_C_data, aes(x=len, y=as.character(dose)))+ geom_boxplot()+ theme_dark() + labs( title = &quot;Tooth Length distribution comparison&quot;, x = &quot;Tooth Length&quot;, y = &quot;Dose&quot; ) + theme( plot.title = element_text(hjust = 0.5, face = &quot;bold&quot;, size = 16), axis.title.x = element_text(face = &quot;italic&quot;, size = 12), axis.title.y = element_text(face = &quot;italic&quot;, size = 12) ) ________________________________________________________________________________ Violin Plots An alternative to the boxplot is the violin plot. The violin plot addresses the issue of distribution. It shows the distribution curve unlike the boxplot and histogram. To demonstrate this, lets visualize the distribution of employees age using a violin plot. # VIOLINE PLOT ggplot(df, aes(x=Age, y=&quot;&quot;))+ geom_violin(fill = &quot;skyblue&quot;, color = &quot;darkblue&quot;) Practical exercise Using the same ToothGrowth data set and create a violin plot to compare the distribution of tooth length (len) across the different dose levels (dose) of Vitamin C in the ToothGrowth data set. Make sure to label the axes and title the plot appropriately. Solution library(ggplot2) library(dplyr) # Load the data data(&quot;ToothGrowth&quot;) # Filter to get Vitamin C vit_C_data &lt;- ToothGrowth %&gt;% filter(supp==&quot;VC&quot;) # CREATE BOXPLOT ggplot(vit_C_data, aes(x=len, y=as.character(dose)))+ geom_violin()+ theme_dark() + labs( title = &quot;Tooth Length distribution comparison&quot;, x = &quot;Tooth Length&quot;, y = &quot;Dose&quot; ) + theme( plot.title = element_text(hjust = 0.5, face = &quot;bold&quot;, size = 16), axis.title.x = element_text(face = &quot;italic&quot;, size = 12), axis.title.y = element_text(face = &quot;italic&quot;, size = 12) ) ________________________________________________________________________________ Facet Grids ggplot2 has a functionality that enables creating multiple plots at once on the same chart. This concept is referred to as faceting. This use full especially when comparing two populations characteristics like distributions. There are two main functions that are used by statisticians; facet_wrap() and facet_grid(). facet_wrap(): Is used to design multiple plots under a single dimension(facet by one variable). Wraps plots in a grid facet_grid(): It creates grid of plots based on two variables(rows and columns). It’s particularly useful when comparing data across multiple dimensions. Lets create a box plot to demonstrate facet_wrap() and facet_grid(). Create a sample data for demonstrations # Create sample data df &lt;- data.frame( category = rep(c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;), each = 100), sub_category = rep(c(&quot;X&quot;, &quot;Y&quot;), each = 50, times = 3), value = c(rnorm(100, mean = 10, sd = 2), rnorm(100, mean = 15, sd = 2.5), rnorm(100, mean = 20, sd = 3)) ) head(df) ## category sub_category value ## 1 A X 8.853046 ## 2 A X 6.143750 ## 3 A X 11.328782 ## 4 A X 6.794920 ## 5 A X 7.290799 ## 6 A X 3.964135 A basic box plot to show the distribution of \"value\". # Basic boxplot p &lt;- ggplot(df, aes(x = sub_category, y = value)) + geom_boxplot() + labs( title = &quot;Boxplot of Values by Sub-Category&quot;, x = &quot;Sub-Category&quot;, y = &quot;Value&quot; ) p Facet with facet_wrap(). This will divide \"value\" into categories based on the category column and create a separate boxplot for each category. # Facet the plot by &#39;category&#39; using facet_wrap p + facet_wrap(~ category) Facet with the facet_grid() to create a grid of plots based on each category and sub_category # Facet the plot by &#39;category&#39; and &#39;sub_category&#39; using facet_grid p + facet_grid(category ~ sub_category) As seen above, there is a boxplot to represent the distribution for each sub_category in a category. Practical exercise Using the same ToothGrowth data set, create a violin plot to visualize the distribution of tooth length (len) across the different dose levels (dose). Use facet_wrap() or facet_grid() to split the plot by the supplement type (supp). Make sure to label the axes and add an appropriate title Solution Load the data and libraries library(ggplot2) # Load the data data(&quot;ToothGrowth&quot;) Create a basic violin plot # Basic violin plot p &lt;- ggplot(ToothGrowth, aes(x = len, y = as.character(dose))) + geom_violin() + labs( title = &quot;Distribution of length across different dose levels&quot;, x = &quot;Tooth Length&quot;, y = &quot;Dose levels&quot; ) p # variable to store out plot/chart Use facet_wrap() to divide values into categories based on the supp column p + facet_wrap(~ supp) # facet plot the supp column Lets make the facet more interesting by adding facet_grids p + facet_grid(supp ~ as.character(dose)) ________________________________________________________________________________ 5.2.2 Combining Multiple Plots Different plots can be combined into a single visualization. This provides a comprehensive view and different aspects of the data. The interrelationships between the variables can also be analyzed. There are multiple ways to combine plots in R(ggplot); Using patchwork Using gridExtra patchwork The patchwork library provides a straightforward way to combine plots. Install the library install.packages(&quot;patchwork&quot;) Lets then make some plots from the mtcars data set and combine them # Load the library library(patchwork) # Load the data data(&quot;mtcars&quot;) # Make three plots p1 &lt;- ggplot(mtcars, aes(x = mpg, y = disp)) + geom_point() + ggtitle(&quot;Scatter Plot&quot;) p2 &lt;- ggplot(mtcars, aes(x = factor(cyl), y = mpg)) + geom_boxplot() + ggtitle(&quot;Boxplot&quot;) p3 &lt;- ggplot(mtcars, aes(x = factor(cyl))) + geom_bar() + ggtitle(&quot;Bar Chart&quot;) Combine the plots side by side # Combine plots side by side p1 + p2 + p3 The plots can also be combined by stacking them together # Stack plots vertically p1 / p2 / p3 + is used to combine the plots side by side while / is used to stack the plots vertically. Brackets () are used to group two or more plots together. Lets create a complex layout where two plots are grouped to the right and one plot is one the left side. (p1+(p2/p3)) Alternatively the plots can be plotted by using | instead of +. (p1|(p2/p3)) # Note the &#39;|&#39; instead of &#39;+&#39; gridExtra The gridExtra package allows the plots to be combined with a pre-determined number of rows and columns. Install the gridExtra package install.packages(&quot;gridExtra&quot;) Load the package and combine the three plots library(gridExtra) ## ## Attaching package: &#39;gridExtra&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## combine # Combine plots using gridExtra grid.arrange(p1, p2, p3, nrow = 2, ncol = 2) Lets stack the plots # Stack the plots grid.arrange(p1, p2, p3, nrow = 3, ncol = 1) Now combine the three plots side by side grid.arrange(p1, p2, p3, nrow = 1, ncol = 3) Practical exercise Download the E-commerce Customer Behavior Data set from here. Create multiple plots that will combine; A histogram to show annual income distribution A box plot that will show Time on Site distribution A bar chart that compare different genders count for the customers A scatter plot to compare age vs annual income Solution library(ggplot2) library(patchwork) # Load the data ecommerce &lt;- read.csv(&quot;data/E-commerce.csv&quot;) # PLOTTING ## A histogram to show annual income distribution p1 &lt;- ggplot(ecommerce, aes(x = Annual.Income)) + geom_histogram(bins = 8, fill = &quot;blue&quot;, color = &quot;black&quot;) + labs(title = &quot;Distribution of Annual Income&quot;, x = &quot;Annual Income&quot;, y = &quot;frequency&quot;) + theme_minimal() # A box plot that will show Time on Site distribution p2 &lt;- ggplot(ecommerce, aes(y=Time.on.Site))+ geom_boxplot(fill=&quot;blue&quot;, color=&quot;black&quot;) + labs(title = &quot;Time on Site distribution&quot;) + theme_minimal() # A bar chart that compare different genders count for the customers p3 &lt;- ggplot(ecommerce, aes(x = factor(Gender))) + geom_bar(fill=&quot;blue&quot;, color=&quot;black&quot;) + ggtitle(&quot;Customers Gender&quot;) # A scatter plot to compare age vs annual income p4 &lt;- ggplot(ecommerce, aes(x = Age, y = Annual.Income)) + geom_point(color=&quot;blue&quot;) + ggtitle(&quot;Age vs Annual Income&quot;) # Combine the plots (p1+p2)/(p3+p4) ________________________________________________________________________________ "],["explanatory-data-analysis.html", "Chapter 6 Explanatory Data Analysis 6.1 Introduction to EDA 6.2 Choosing a data set 6.3 Conducting EDA 6.4 Present Findings 6.5 EDA - Example", " Chapter 6 Explanatory Data Analysis 6.1 Introduction to EDA Exploratory Data Analysis (EDA) is a critical process in the data analysis workflow. It involves examining and visualizing a dataset to uncover patterns, spot anomalies, test hypotheses, and check assumptions using summary statistics and graphical representations. EDA provides a solid understanding of the data and lays the foundation for more advanced statistical analyses or machine learning models. The key objectives of EDA are;- Understand and familiarize the data structure. Detect outliers and anomalies. Identify patterns and trends. Check and verify assumptions, etc. By performing EDA thoroughly, the groundwork is laid for more accurate results, ultimately provides insights that lead to better decision-making. EDA also guides to better model-selection. Here are the common techniques involved in EDA;- Summary Statistics: The analysts compute the summary statistics like mean, median, mode and standard deviation to find the spread and central tendency in the data. Data Visualization: Charts and graphs are drawn to provide a visual interpretation of the data set. Data Cleaning and Preparation: Data manipulation and pre-processing is done to reduce the risk of misinterpretation. There are more EDA techniques like univariate, bivariate and multivariate analysis, and inferential statistics. 6.2 Choosing a data set You are required to select a data set from the list below and perform EDA; MBA Admission dataset, Class 2025 - download here Global Black Money Transactions Dataset - download here Crop Yield Prediction Dataset - download here ChickWeight inbuilt R dataset - load from the command data(\"ChickWeight\") Seatbelts data set - load from the command data(\"Seatbelts\") The “Groceries” from the R package comes arules- Load the data by first importing arules(library(arules)) then the data by data(\"Groceries\") \"CreditCard\" data from package AER- Load the data by first importing AER (library(AER)) then the data by data(\"CreditCard\") Outline the questions to explore during EDA based on the Data set you have chosen 6.3 Conducting EDA You will use the skills acquired in this camp(especially dplyr and ggplot2) to; Explore the data set structure Visualize the relationship between variables Summarize key findings 6.4 Present Findings Finally, you will prepare a brief presentation of your findings ,and discuss on the insights obtained and the techniques used. 6.5 EDA - Example This is just an example on how to explore EDA. Download the Ecommerce data set from here 6.5.1 Introduction This eCommerce dataset contains transaction-level information, including customer demographics (e.g., age, gender), purchase details (e.g., product category, payment method), and discount information. It captures both gross and net amounts, helping to analyze the impact of discounts and customer behavior across different locations. This data set provides insights into customer spending patterns and promotional effectiveness 6.5.2 Data Assessment and Cleaning Load the necessary data sets and libraries # Load the libraries library(dplyr) library(ggplot2) # Load the data ecommerce &lt;- read.csv(&quot;data/ecommerce.csv&quot;) Show the first five rows head(ecommerce) ## CID TID Gender Age.Group Purchase.Date Product.Category ## 1 943146 5876328741 Female 25-45 30/08/2023 20:27:08 Electronics ## 2 180079 1018503182 Male 25-45 23/02/2024 09:33:46 Electronics ## 3 337580 3814082218 Other 60 and above 06/03/2022 09:09:50 Clothing ## 4 180333 1395204173 Other 60 and above 04/11/2020 04:41:57 Sports &amp; Fitness ## 5 447553 8009390577 Male 18-25 31/05/2022 17:00:32 Sports &amp; Fitness ## 6 200614 3994452858 Male 18-25 12/07/2021 15:10:27 Clothing ## Discount.Availed Discount.Name Discount.Amount..INR. Gross.Amount ## 1 Yes FESTIVE50 64.30 725.304 ## 2 Yes SEASONALOFFER21 175.19 4638.992 ## 3 Yes SEASONALOFFER21 211.54 1986.373 ## 4 No 0.00 5695.613 ## 5 Yes WELCOME5 439.92 2292.651 ## 6 Yes FESTIVE50 127.01 3649.397 ## Net.Amount Purchase.Method Location ## 1 661.004 Credit Card Ahmedabad ## 2 4463.802 Credit Card Bangalore ## 3 1774.833 Credit Card Delhi ## 4 5695.613 Debit Card Delhi ## 5 1852.731 Credit Card Delhi ## 6 3522.387 Credit Card Delhi The data set features/columns and the shape of the data set colnames(ecommerce) #the data set features ## [1] &quot;CID&quot; &quot;TID&quot; &quot;Gender&quot; ## [4] &quot;Age.Group&quot; &quot;Purchase.Date&quot; &quot;Product.Category&quot; ## [7] &quot;Discount.Availed&quot; &quot;Discount.Name&quot; &quot;Discount.Amount..INR.&quot; ## [10] &quot;Gross.Amount&quot; &quot;Net.Amount&quot; &quot;Purchase.Method&quot; ## [13] &quot;Location&quot; dim(ecommerce) # the shape of the data ## [1] 55000 13 There are 55000 observations and 13 features/columns in the data set. Lets now find the data types of the columns’ str(df) ## &#39;data.frame&#39;: 300 obs. of 3 variables: ## $ category : chr &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; ... ## $ sub_category: chr &quot;X&quot; &quot;X&quot; &quot;X&quot; &quot;X&quot; ... ## $ value : num 8.85 6.14 11.33 6.79 7.29 ... There are 5 numeric columns and 8 non numeric columns. Find and count the null values in the data set sum(is.na(ecommerce)) ## [1] 0 The data set is complete with no missing values. Lets now find if there are any duplicated records sum(duplicated(ecommerce)) ## [1] 0 There are no duplicated records Calculate the summary statistics summary(ecommerce) ## CID TID Gender Age.Group ## Min. :100009 Min. :1.000e+09 Length:55000 Length:55000 ## 1st Qu.:323717 1st Qu.:3.253e+09 Class :character Class :character ## Median :550088 Median :5.498e+09 Mode :character Mode :character ## Mean :551246 Mean :5.505e+09 ## 3rd Qu.:776956 3rd Qu.:7.748e+09 ## Max. :999996 Max. :9.999e+09 ## Purchase.Date Product.Category Discount.Availed Discount.Name ## Length:55000 Length:55000 Length:55000 Length:55000 ## Class :character Class :character Class :character Class :character ## Mode :character Mode :character Mode :character Mode :character ## ## ## ## Discount.Amount..INR. Gross.Amount Net.Amount Purchase.Method ## Min. : 0.0 Min. : 136.5 Min. :-351.1 Length:55000 ## 1st Qu.: 0.0 1st Qu.:1562.1 1st Qu.:1429.6 Class :character ## Median : 0.0 Median :2954.3 Median :2814.9 Mode :character ## Mean :137.0 Mean :3012.9 Mean :2875.9 ## 3rd Qu.:274.1 3rd Qu.:4342.2 3rd Qu.:4211.4 ## Max. :500.0 Max. :8394.8 Max. :8394.8 ## Location ## Length:55000 ## Class :character ## Mode :character ## ## ## 6.5.3 Data Visualization The relationship between the discount amount and the gross amount using a scatter plot # Create a scatter plot ggplot(ecommerce, # data aes(x = Gross.Amount, y = Discount.Amount..INR.)) + # aesthetics geom_point(color=&quot;blue&quot;) + labs( title=&quot;The relationship between Gross Amount and Discount&quot;, y=&quot;Discount Amount&quot;, x=&quot;Gross Amount&quot; ) There is no clear relationship between the discount amount and the gross amount The average discount per product category discount_per_category &lt;- ecommerce %&gt;% select(Product.Category, Discount.Amount..INR.) %&gt;% group_by(Product.Category) %&gt;% summarise(Average.Discount=mean(Discount.Amount..INR.)) discount_per_category ## # A tibble: 9 × 2 ## Product.Category Average.Discount ## &lt;chr&gt; &lt;dbl&gt; ## 1 Beauty and Health 136. ## 2 Books 137. ## 3 Clothing 138. ## 4 Electronics 136. ## 5 Home &amp; Kitchen 139. ## 6 Other 139. ## 7 Pet Care 134. ## 8 Sports &amp; Fitness 136. ## 9 Toys &amp; Games 139. Plot the data on a bar chart # Create a bar chart ggplot(discount_per_category, aes(x = Product.Category, y = Average.Discount)) + geom_bar(stat = &quot;identity&quot;, fill=&quot;blue&quot;) + labs(title = &quot;Average discount per the product category&quot;, x = &quot;Product Category&quot;, y = &quot;Average Discount&quot;) + theme_classic() The average discount was almost equal for all the categories Count the purchases by purchase method purchase_method_count &lt;- ecommerce %&gt;% group_by(Purchase.Method)%&gt;% summarize(Count=n()) purchase_method_count ## # A tibble: 8 × 2 ## Purchase.Method Count ## &lt;chr&gt; &lt;int&gt; ## 1 Cash on Delivery 2768 ## 2 Credit Card 22096 ## 3 Debit Card 13809 ## 4 Google Pay UPI 2670 ## 5 International Card 2815 ## 6 Net Banking 5485 ## 7 Paytm UPI 2674 ## 8 PhonePe UPI 2683 Display on a bar chart # Create a bar chart ggplot(purchase_method_count, aes(x = Purchase.Method, y = Count)) + geom_bar(stat = &quot;identity&quot;, fill=&quot;blue&quot;) + labs(title = &quot;Count of purchases by purchase method&quot;, x = &quot;Purchase Method&quot;, y = &quot;Count&quot;) + theme_classic() Most customers purchased goods by credit and debit cards. Show the net amount distribution by histogram # Create a histogram ggplot(ecommerce, aes(x = Net.Amount)) + geom_histogram(bins = 8, fill = &quot;blue&quot;, color = &quot;black&quot;) + labs(title = &quot;Distribution of Net Amount&quot;, x = &quot;Net Amount&quot;, y = &quot;frequency&quot;) + theme_minimal() Most products range from 1000 to 4000 Indian Rupees Find out the different age groups unique(ecommerce$Age.Group) ## [1] &quot;25-45&quot; &quot;60 and above&quot; &quot;18-25&quot; &quot;45-60&quot; &quot;under 18&quot; There are five different age groups, lets inspect their spending characteristics on the eCommerce site using box plots # Plot a box plot ggplot(ecommerce, aes(x=Age.Group, y=Gross.Amount, fill = Age.Group))+ geom_boxplot() + labs(title=&quot;Spending based on different age groups&quot;, x = &quot;Age groups&quot;, y = &quot;Gross Amount&quot;) + theme_classic() There was no clear difference between the spending patterns based on the age groups 6.5.4 Key Findings These were the key findings that were discovered; The amount spend on purchase and the category of product purchased had no effect on the discount given. Most of the customers find it easy using their credit and debit cards. This may be due to low fees imposed , transaction speeds, not adopted alternatives methods, or the popularity of these cards when purchasing. There was no clear difference between the spending patterns based on the age groups. ________________________________________________________________________________ "],["statistical-analysis.html", "Chapter 7 Statistical Analysis 7.1 Basic Statistical Concepts 7.2 Correlation and Regression Analysis 7.3 Advanced Statistical Methods 7.4 Project: Applying Statistical Analysis", " Chapter 7 Statistical Analysis 7.1 Basic Statistical Concepts 7.1.1 Introduction to Descriptive Statistics Mean, Median and Mode Mean is the sum of all values divided by the number of values in the set. It also referred to as average. Median is the middle value when values in a data set is ordered/lined up in ascending/descending order. Mode is the number that occurs most in the data set. Simply, the most frequent value in the data set. All these, are measures of central tendency. Central tendency identifies the center or typical value of a data set. Measuring central tendency summarizes the data by identifying skewness, distribution and how the data is robust to outliers. Business calculate the central tendencies like average sales, median customer age to make informed decision-making. Below is the formula to calculate mean. where \\(x_i\\) represents each value in the data set, and \\(n\\) is the number of values. Lets calculate mean of the vector ages with the values 12, 58, 27, 33, 31, 27, 37 manually with the steps below; # Add the values together total_age = 12 + 58 + 27 + 33 + 31 + 27 + 37 total_age # is 225 number_of_values = 7 # there are 7 values mean age = total_age/number_of_values mean_age # is 32.14 The average age is 32.14. To calculate the median of the vector ages, the ages are arranged in descending/ascending order and the middle one is selected. In this case we will line them up in ascending order # Line the ages up in ascending order 12, 27, 27, 31, 33, 37, 58 # There are 7 ages, the fourth one from either side is the median value # The median age is 31 To find mode, you just find the value that appears the most, for the values 12, 27, 27 , 31, 33, 37, 58, age 27 appears twice while these other ages appear once. The value 27 is therefore the mode. Mean, median and mode can also be calculated using Base R using the functions mean(), median() and mode() where the vector ages is the argument. # Create vector ages ages = c(12, 58, 27, 33, 31, 27, 37) # Average/mean mean(ages) ## [1] 32.14286 # Median median(ages) ## [1] 31 # Mode mode(ages) ## [1] &quot;numeric&quot; Practical exercise Load the inbuilt iris R data set, retrieve the sepal width(Sepal.Width) and sepal length(Sepal.length) of the each Iris species and store in separate vectors. For instance, the sepal width for setosa should be in a variable setosa.sepal.width. Finally calculate and interpret the mean, median and mode for each vector Solution Get the data library(dplyr) data(iris) # Retrieve the sepal width(Sepal.Width) and sepal length(Sepal.length) of the each Iris species and store in separate vectors. ## Create a function to do this get_iris_data &lt;- function(x, y){ selected_iris &lt;- iris %&gt;% filter(Species==x) as.vector(selected_iris[, y]) } ## Now retrieve the sepal length and width for each species setosa.sepal.width &lt;- get_iris_data(&quot;setosa&quot;, &quot;Sepal.Width&quot;) setosa.sepal.length &lt;- get_iris_data(&quot;setosa&quot;, &quot;Sepal.Length&quot;) versicolor.sepal.width &lt;- get_iris_data(&quot;versicolor&quot;, &quot;Sepal.Width&quot;) versicolor.sepal.length &lt;- get_iris_data(&quot;versicolor&quot;, &quot;Sepal.Length&quot;) virginica.sepal.width &lt;- get_iris_data(&quot;virginica&quot;, &quot;Sepal.Width&quot;) virginica.sepal.length &lt;- get_iris_data(&quot;virginica&quot;, &quot;Sepal.Length&quot;) Calculate the mean print(&quot;MEAN&quot;) ## [1] &quot;MEAN&quot; print(paste(&quot;Setosa Sepal Width:&quot;, mean(setosa.sepal.width))) ## [1] &quot;Setosa Sepal Width: 3.428&quot; print(paste(&quot;Setosa Sepal Length:&quot;, mean(setosa.sepal.length))) ## [1] &quot;Setosa Sepal Length: 5.006&quot; print(paste(&quot;Versicolor Sepal Width:&quot;, mean(versicolor.sepal.width))) ## [1] &quot;Versicolor Sepal Width: 2.77&quot; print(paste(&quot;Versicolor Sepal Length:&quot;, mean(versicolor.sepal.length))) ## [1] &quot;Versicolor Sepal Length: 5.936&quot; print(paste(&quot;Virginica Sepal Width:&quot;, mean(virginica.sepal.width))) ## [1] &quot;Virginica Sepal Width: 2.974&quot; print(paste(&quot;virginica Sepal Length:&quot;, mean(virginica.sepal.length))) ## [1] &quot;virginica Sepal Length: 6.588&quot; Calculate median print(&quot;MEDIAN&quot;) ## [1] &quot;MEDIAN&quot; print(paste(&quot;Setosa Sepal Width:&quot;, median(setosa.sepal.width))) ## [1] &quot;Setosa Sepal Width: 3.4&quot; print(paste(&quot;Setosa Sepal Length:&quot;, median(setosa.sepal.length))) ## [1] &quot;Setosa Sepal Length: 5&quot; print(paste(&quot;Versicolor Sepal Width:&quot;, median(versicolor.sepal.width))) ## [1] &quot;Versicolor Sepal Width: 2.8&quot; print(paste(&quot;Versicolor Sepal Length:&quot;, median(versicolor.sepal.length))) ## [1] &quot;Versicolor Sepal Length: 5.9&quot; print(paste(&quot;Virginica Sepal Width:&quot;, median(virginica.sepal.width))) ## [1] &quot;Virginica Sepal Width: 3&quot; print(paste(&quot;virginica Sepal Length:&quot;, median(virginica.sepal.length))) ## [1] &quot;virginica Sepal Length: 6.5&quot; Calculate the mode print(&quot;MODE&quot;) ## [1] &quot;MODE&quot; print(paste(&quot;Setosa Sepal Width:&quot;, mode(setosa.sepal.width))) ## [1] &quot;Setosa Sepal Width: numeric&quot; print(paste(&quot;Setosa Sepal Length:&quot;, mode(setosa.sepal.length))) ## [1] &quot;Setosa Sepal Length: numeric&quot; print(paste(&quot;Versicolor Sepal Width:&quot;, mode(versicolor.sepal.width))) ## [1] &quot;Versicolor Sepal Width: numeric&quot; print(paste(&quot;Versicolor Sepal Length:&quot;, mode(versicolor.sepal.length))) ## [1] &quot;Versicolor Sepal Length: numeric&quot; print(paste(&quot;Virginica Sepal Width:&quot;, mode(virginica.sepal.width))) ## [1] &quot;Virginica Sepal Width: numeric&quot; print(paste(&quot;virginica Sepal Length:&quot;, mode(virginica.sepal.length))) ## [1] &quot;virginica Sepal Length: numeric&quot; ________________________________________________________________________________ Variance and Standard deviation Variance is statistical measure of dispersion that defines how spread the data points are in a data set in relation to the mean of the data set. Standard deviation is the measure of how data is clustered around the mean. It is simply defined to as the square root of variance. Variance and standard deviation can be calculated in R environment using var() and sd() functions respectively. Lets create a vector of weights of the athletes in kilograms and calculate the variance and standard deviation. # Sample vector athlete_weights = c(55, 76, 52, 68, 71, 63, 58, 52, 85, 96) # Calculate variance var(athlete_weights) ## [1] 216.7111 # Calculate standard deviation sd(athlete_weights) ## [1] 14.72111 Practical exercise Using the same iris data set, calculate the variance and standard deviation for the versicolor species petal width(Petal.Width). Solution library(dplyr) # Load the data data(&quot;iris&quot;) versicolor &lt;- iris%&gt;% filter(Species==&quot;versicolor&quot;) # Convert to a vector versicolor.petal.width = as.vector(versicolor$Petal.Width) # Calculate variance and standard deviation sd(versicolor.petal.width) # standard deviation ## [1] 0.1977527 var(versicolor.petal.width) # variance ## [1] 0.03910612 ________________________________________________________________________________ Range and Interquartile Range Range is the difference between the maximum and minimum values in the data set. This defines the spread and dispersion of a data set. Below is the formula for Range: Range = Maximum Value - Minimum Value Lets use the weights vector above to calculate the range; weights = 55, 76, 52, 68, 71, 63, 58, 52, 85, 96 maximum_weight = 96 minimum_weight = 52 Range = maximum_weight - minimum_weight = 44 Range is essential in data analysis as it gives a quick sense of variability in the data set, however it vulnerable to outliers since it gives importance to the maximum and minimum values even if they are extreme. Interquartile Range(IQR) is the difference between the first quartile (Q1) and third quartile(Q3) value. First Quartile is the median of the lower half of the data while Thrid quartile is the median of the upper of the data set. Therefore IQR = Q3 - Q1 Lets calculate the IQR of the vector weights step by step;- Define the data weights and arrange the data in ascending order. weights = 52, 52, 55, 58, 63, 68, 71, 76, 85, 96. Determine the first quartile (Q1). Select the lower half(first five values) of the data and find their median which is the first quartile (Q1) lower_half = 52, 52, 55, 58, 63 Q1 = 55 Determine the third quartile (Q3). Select the upper half(last five values) and find their median which is the third quartile. upper_half = 68, 71, 76, 85, 96 Q3 = 76 Calculate the Interquartile Range (IQR) by finding the difference between Q1 and Q3. IQR = Q3 - Q1 = 76 - 55 = 21 The IQR for the weights data set is 21. Range and IQR can be calculated in R environment using the range() and IQR() function respectively. However function range returns the maximum and the minmum values in the dataset. # Sample vector athlete_weights = c(55, 76, 52, 68, 71, 63, 58, 52, 85, 96) # RANGE range(athlete_weights) # returns maximum and minimum values ## [1] 52 96 diff(range(athlete_weights)) # calculate the range value ## [1] 44 # INTER QUARTILE RANGE (IQR)http://127.0.0.1:38237/rmd_output/1/basic-data-types-and-structures.html IQR(athlete_weights) ## [1] 19 Practical exercise Load the iris data set and calculate the range and the Interquartile Range for the Petal Length and petal width for each iris species Solution library(dplyr) data(iris) # Retrieve the Petal width(Sepal.Width) and Petal length(Petal.length) of the each Iris species and store in separate vectors. ## Create a function to do this get_iris_data &lt;- function(x, y){ selected_iris &lt;- iris %&gt;% filter(Species==x) as.vector(selected_iris[, y]) } ## Now retrieve the petal length and width for each species setosa.petal.width &lt;- get_iris_data(&quot;setosa&quot;, &quot;Petal.Width&quot;) setosa.petal.length &lt;- get_iris_data(&quot;setosa&quot;, &quot;Petal.Length&quot;) versicolor.petal.width &lt;- get_iris_data(&quot;versicolor&quot;, &quot;Petal.Width&quot;) versicolor.petal.length &lt;- get_iris_data(&quot;versicolor&quot;, &quot;Petal.Length&quot;) virginica.petal.width &lt;- get_iris_data(&quot;virginica&quot;, &quot;Petal.Width&quot;) virginica.petal.length &lt;- get_iris_data(&quot;virginica&quot;, &quot;Petal.Length&quot;) Calculate the interquartile range print(&quot;INTERQUARTILE RANGE&quot;) ## [1] &quot;INTERQUARTILE RANGE&quot; print(paste(&quot;Setosa Petal Width:&quot;, IQR(setosa.petal.width))) ## [1] &quot;Setosa Petal Width: 0.1&quot; print(paste(&quot;Setosa Petal Length:&quot;, IQR(setosa.petal.length))) ## [1] &quot;Setosa Petal Length: 0.175&quot; print(paste(&quot;Versicolor Petal Width:&quot;, IQR(versicolor.petal.width))) ## [1] &quot;Versicolor Petal Width: 0.3&quot; print(paste(&quot;Versicolor Petal Length:&quot;, IQR(versicolor.petal.length))) ## [1] &quot;Versicolor Petal Length: 0.6&quot; print(paste(&quot;Virginica Petal Width:&quot;, IQR(virginica.petal.width))) ## [1] &quot;Virginica Petal Width: 0.5&quot; print(paste(&quot;virginica Petal Length:&quot;, IQR(virginica.petal.length))) ## [1] &quot;virginica Petal Length: 0.775000000000001&quot; Calculate range print(&quot;RANGE&quot;) ## [1] &quot;RANGE&quot; print(paste(&quot;Setosa Petal Width:&quot;, range(setosa.petal.width))) ## [1] &quot;Setosa Petal Width: 0.1&quot; &quot;Setosa Petal Width: 0.6&quot; print(paste(&quot;Setosa Petal Length:&quot;, range(setosa.petal.length))) ## [1] &quot;Setosa Petal Length: 1&quot; &quot;Setosa Petal Length: 1.9&quot; print(paste(&quot;Versicolor Petal Width:&quot;, range(versicolor.petal.width))) ## [1] &quot;Versicolor Petal Width: 1&quot; &quot;Versicolor Petal Width: 1.8&quot; print(paste(&quot;Versicolor Petal Length:&quot;, range(versicolor.petal.length))) ## [1] &quot;Versicolor Petal Length: 3&quot; &quot;Versicolor Petal Length: 5.1&quot; print(paste(&quot;Virginica Petal Width:&quot;, range(virginica.petal.width))) ## [1] &quot;Virginica Petal Width: 1.4&quot; &quot;Virginica Petal Width: 2.5&quot; print(paste(&quot;virginica Petal Length:&quot;, range(virginica.petal.length))) ## [1] &quot;virginica Petal Length: 4.5&quot; &quot;virginica Petal Length: 6.9&quot; ________________________________________________________________________________ 7.1.2 Visualization of Descriptive Statistics Boxplots Boxplots provide a visual summary of data distribution. It helps the analyst understand the spread, skewness and outliers in the data set. The diagram above shows a labelled boxplot. Lets breakdown each part; Minimum(Lower Whisker): is the smallest data point that appear between the first quartile(Q1) and 1.5x Interquartile Range(IQR) towards the lower limit(left side here. Maximum(Upper Whisker: is the largest data point that appear between the third quartile(Q3) and 1.5x IQR towards the upper limit(right side here) First Quartile(Q1): is the 25\\(^{th}\\) percentile depicting that 25% of the data points are below here. Third Quartile(Q3): is the 75\\(^{th}\\) percentile, depicting that 75% of the data points are above here. Median(Q2): is the 50\\(^{th}\\) percentile that divides the data into two equal halves, 50% of the data points are above and below here. Interquartile Range: The difference between the third quartile(Q3) and the first quartile(Q1). Outliers: Data points that fall outside the whiskers are considered as outliers and are plotted as dot alone. Lets engage in a practical session and plot a boxplot from the ggplot. library(ggplot2) # Sample data set.seed(123) # for reproducibility df &lt;- data.frame( group = rep(c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;), each = 50), value = c(rnorm(50, mean = 10, sd = 2), rnorm(50, mean = 15, sd = 2.5), rnorm(50, mean = 20, sd = 3)) ) # Create some outliers outliers &lt;- data.frame( group = c(rep(&quot;A&quot;, 4), rep(&quot;B&quot;, 4), rep(&quot;C&quot;, 4)), value = c(-2, 3, -30, -35, 50, 6, 45, 50, -17, 8, 60, 65) # Low and high outliers ) # Introduce the outliers into the data set df_with_outliers &lt;- rbind(df, outliers) # Plot a simple boxplot ggplot(df_with_outliers, aes(x = value)) + geom_boxplot(outlier.colour = &quot;red&quot;) + labs( title = &quot;Boxplot of Values&quot;, x = &quot;Value&quot; ) + theme_minimal() The red dots are the outliers Practical exercise Use the iris data set to plot boxplots that represent the petal length for each species. Analyze the boxplots and compare them. Solution library(ggplot2) library(dplyr) library(patchwork) # Load the data data(&quot;iris&quot;) # Filter to get petal length for each species ## Setosa Petal Length setosa &lt;- filter(iris, Species==&quot;setosa&quot;) ## Versicolor Petal Length versicolor &lt;- filter(iris, Species==&quot;versicolor&quot;) ## Virginica Petal Length virginica &lt;- filter(iris, Species==&quot;virginica&quot;) # Create boxplots ## Setosa p1 &lt;- ggplot(setosa, aes(x=Petal.Length)) + geom_boxplot(outlier.colour = &quot;red&quot;) + labs( title = &quot;Setosa Petal Length&quot;, x = &quot;Petal Length&quot; ) + theme_minimal() ## Versicolor p2 &lt;- ggplot(versicolor, aes(x=Petal.Length)) + geom_boxplot(outlier.colour = &quot;red&quot;) + labs( title = &quot;Versicolor Petal Length&quot;, x = &quot;Petal.Length&quot; ) + theme_classic() ## Setosa p3 &lt;- ggplot(virginica, aes(x=Petal.Length)) + geom_boxplot(outlier.colour = &quot;red&quot;) + labs( title = &quot;Virginica Petal Length&quot;, x = &quot;Petal Length&quot; ) + theme_dark() # Combine the plots p1/p2/p3 Add a brief comparison ________________________________________________________________________________ Histograms and Density Plots These two types of plots are used to visualize the distribution of data. A histogram is a graphical representation of the distribution of a data set. It divides the data into bins (intervals) and displays the frequency (or count) of data points that fall within each bin. Histograms are particularly useful for: Visualizing the shape of the data distribution: Whether the data is skewed, symmetric, or multimodal. Identifying outliers: Extreme values that fall far outside the range of most data points. Understanding the spread: How data points are distributed across the range of values. Lets plot a simple histogram for this; # Simple Histogram ggplot(df_with_outliers, aes(x = value)) + geom_histogram(binwidth = 5, fill = &quot;skyblue&quot;, color = &quot;black&quot;) + labs( title = &quot;Histogram of Values with Outliers&quot;, x = &quot;Value&quot;, y = &quot;Frequency&quot; ) + theme_minimal() Most values in the data set range between 10 and 20. Let,s also plot the data for a nomal distribution. ggplot(df, aes(x = value)) + geom_histogram(binwidth = 5, fill = &quot;skyblue&quot;, color = &quot;black&quot;) + labs( title = &quot;Histogram of Values&quot;, x = &quot;Value&quot;, y = &quot;Frequency&quot; ) + theme_minimal() Most of the values are at the center and make a bell-curve. On the other hand, a density plot is a smoothed version of a histogram. It estimates the probability density function of a continuous variable, allowing for a smoother visualization of the distribution. Density plots are particularly useful for: Comparing distributions: Since density plots can overlay multiple distributions, they are helpful in comparing different data sets or groups within a data set. Visualizing the shape of the data: The smooth curve makes it easier to identify peaks (modes), skewness, and the overall spread. Understanding the relative likelihood of data: The area under the density curve represents the probability of data falling within a particular range. Lets plot a density plot and find the visual difference; # Create a density plot ggplot(df, aes(x = value)) + geom_density(fill = &quot;lightgreen&quot;, color = &quot;darkgreen&quot;, alpha = 0.6) + labs( title = &quot;Density Plot of Values&quot;, x = &quot;Value&quot;, y = &quot;Density&quot; ) + theme_minimal() From the graphs above, histograms provide a bar-chart-like representation of the data distribution while density plots proved a smoother view of the data distribution. Practical exercise Load the airquality, an inbuilt R data set. Plot a density plot and histogram separately to visualize the distribution of the speed of wind in mph(Wind) Solution library(ggplot2) library(patchwork) # Load the data data(&quot;airquality&quot;) # Plot the data ## Histogram p1 &lt;- ggplot(airquality, aes(x = Wind)) + geom_histogram(binwidth = 10, fill = &quot;skyblue&quot;, color = &quot;black&quot;) + labs( title = &quot;Histogram - Wind speed distribution&quot;, x = &quot;Wind speed(mph)&quot;, y = &quot;Frequency&quot; ) + theme_minimal() ## Density plot p2 &lt;- ggplot(airquality, aes(x = Wind)) + geom_density(fill = &quot;lightgreen&quot;, color = &quot;darkgreen&quot;, alpha = 0.6) + labs( title = &quot;Density plot - Wind Speed distribution&quot;, x = &quot;Wind Speed(mph)&quot;, y = &quot;Density&quot; ) + theme_minimal() p1/p2 ________________________________________________________________________________ 7.1.3 Hands On Exercise In this exercise you will use the \"Pima.te\" data set from the MASS package. Follow the steps below to get the data ready and started; Install the MASS package by; install.packages(&quot;MASS&quot;) Import the library library(MASS) Load the pima dataset # Load the data set data(&quot;Pima.te&quot;) # Show the first few rows of the data head(Pima.te) After getting the data ready, apply the skills learnt to; Calculate the destcriptive statistics (mean, mode and median etc) Find the relationship between the plasma glucose concentration in an oral glucose tolerance test(glu) and the diastolic blood pressure(bp) using a scatter plot Compare the average age of the patients that had diabetes(’type`) using a bar chart Find the distribution of age of the patients using a histogram. Explain the distribution. Solution Prepare the workspace install.packages(&quot;MASS&quot;) library(MASS) ## ## Attaching package: &#39;MASS&#39; ## The following object is masked from &#39;package:patchwork&#39;: ## ## area ## The following object is masked from &#39;package:dplyr&#39;: ## ## select # Load the data data(&quot;Pima.te&quot;) Calculate the destcriptive statistics (mean, mode and median etc) summary(Pima.te) ## npreg glu bp skin ## Min. : 0.000 Min. : 65.0 Min. : 24.00 Min. : 7.00 ## 1st Qu.: 1.000 1st Qu.: 96.0 1st Qu.: 64.00 1st Qu.:22.00 ## Median : 2.000 Median :112.0 Median : 72.00 Median :29.00 ## Mean : 3.485 Mean :119.3 Mean : 71.65 Mean :29.16 ## 3rd Qu.: 5.000 3rd Qu.:136.2 3rd Qu.: 80.00 3rd Qu.:36.00 ## Max. :17.000 Max. :197.0 Max. :110.00 Max. :63.00 ## bmi ped age type ## Min. :19.40 Min. :0.0850 Min. :21.00 No :223 ## 1st Qu.:28.18 1st Qu.:0.2660 1st Qu.:23.00 Yes:109 ## Median :32.90 Median :0.4400 Median :27.00 ## Mean :33.24 Mean :0.5284 Mean :31.32 ## 3rd Qu.:37.20 3rd Qu.:0.6793 3rd Qu.:37.00 ## Max. :67.10 Max. :2.4200 Max. :81.00 Find the relationship between the plasma glucose concentration in an oral glucose tolerance test(glu) and the diastolic blood pressure(bp) using a scatter plot. library(ggplot2) # Create a scatter plot ggplot(Pima.te, # data aes(x = glu, y = bp)) + # aesthetics geom_point() Compare the average age of the patients that had diabetes(’type`) using a bar chart library(dplyr) # Group the age by diabetes type age_by_type &lt;- Pima.te %&gt;% group_by(type) %&gt;% summarise(average_age = mean(age)) # Create a scatter plot ggplot(age_by_type, aes(x = type, y = average_age)) + geom_bar(stat = &quot;identity&quot;, fill = &quot;steelblue&quot;) + labs(x = &quot;Diabetes Type&quot;, y = &quot;Average Age&quot;) + theme_minimal() Find the distribution of age of the patients using a histogram. Explain the distribution. ggplot(Pima.te, aes(x=age)) + geom_histogram(binwidth = 10, fill = &quot;skyblue&quot;, color = &quot;black&quot;) + labs( title = &quot;Histogram - Patients&#39; age distribution&quot;, x = &quot;Age&quot;, y = &quot;Frequency&quot; ) + theme_minimal() ________________________________________________________________________________ 7.2 Correlation and Regression Analysis 7.2.1 Introduction to Correlation Pearson and Spearman Correlation Correlation is the relationship between two variables. It can also defined to as the statistic measure to the degree to which two variables move in relation to each other. There are two types of correlation in statistics;- pearson and spearman correlations. They differ in their calculation methods, assumptions and type of relationships they are suited for. Pearson correlation measures the linear relationship between two variables with the assumptions that there is a linear relationship between the variables and the data is normally distributed. Contrarily, Spearman correlation measures the monotonic relationship(whether there is a consistent positive or negative change ) between two variables. The data to be analyzed, don’t need to be normally distributed, it can be ordinal with the variables having a non-linear relationship. Lets take two variables, X and Y, Pearson correlation is calculated by dividing the covariance of X and Y with their product of standard deviation. Below is its formula; r is the pearson correlation and it can range from -1 to +1. Spearman correlation is calculated by ranking data points, then applying Pearson correlation formula. Below is the formula of spearman correlation; where; d is the difference between the ranks of corresponding variables n is the number of observations r is the pearson correlation Spearman correlation is insensitive with outliers because it uses ranks other than outliers. Practical exercise Load the airquality data set. Calculate the correlation of Wind and temperature Wind. Interpret the coefficients for various data sets. Solution # Load the airquality dataset data(&quot;airquality&quot;) # Calculate the correlation between Wind and Temp, excluding NA values correlation &lt;- cor(airquality$Wind, airquality$Temp, use = &quot;complete.obs&quot;) print(correlation) ## [1] -0.4579879 ________________________________________________________________________________ Visualizing Correlation The relationship of exactly two numeric continuous variables can be viewed using scatter plots while the relationship between multiple(two or more) can be analyzed using a correlation plot(heatmap) # Set seed for reproducibility set.seed(123) # Create a linear relationship with some noise x &lt;- rnorm(100, mean = 50, sd = 10) y &lt;- 2 * x + 5 + rnorm(100, mean = 0, sd = 5) # Linear relationship with noise # Combine into a data frame df &lt;- data.frame(x = x, y = y) # Create a scatter plot ggplot(df, aes(x = x, y = y)) + geom_point(color = &quot;blue&quot;) + labs( title = &quot;Scatter Plot of X vs Y&quot;, x = &quot;X Variable&quot;, y = &quot;Y Variable&quot; ) + theme_minimal() In the scatter plot above; The X and y variables have a positive correlation. The increase in X leads to an increase in Y. There are no outliers in the data set. There are no data points that are out of the general pattern. The relationship between X and Y is linear. On the other hand, the heatmap is a visual representation of the correlation matrix where each cell represents the relationship between two variables. The color of the cell indicates the strength of the relationship between the concerned variables. Lets create a data frame with multiple variables, and plot a heatmap. # Set seed for reproducibility set.seed(123) # Create 6 variables var1 &lt;- rnorm(100, mean = 50, sd = 10) var2 &lt;- rnorm(100, mean = 60, sd = 15) var3 &lt;- rnorm(100, mean = 70, sd = 20) var4 &lt;- rnorm(100, mean = 80, sd = 25) var5 &lt;- 3 * var1 + 7 + rnorm(100, mean = 0, sd = 5) # Linear relationship with var1 var6 &lt;- rnorm(100, mean = 90, sd = 30) # Combine into a data frame df &lt;- data.frame(var1 = var1, var2 = var2, var3 = var3, var4 = var4, var5 = var5, var6 = var6) # Display the first few rows of the data set head(df) ## var1 var2 var3 var4 var5 var6 ## 1 44.39524 49.34390 113.97621 62.11895 139.8180 71.94321 ## 2 47.69823 63.85326 96.24826 61.18278 144.2514 60.18904 ## 3 65.58708 56.29962 64.69710 56.53653 200.5875 120.80355 ## 4 50.70508 54.78686 80.86388 53.68717 158.9710 112.53184 ## 5 51.29288 45.72572 61.71320 69.07101 164.2321 44.72500 ## 6 67.15065 59.32458 60.47506 88.27948 200.1992 87.14558 var5 has a linear relationship with var1. Lets plot a heatmap of the data frame df. However, lets install the ggcorrplot and the corrplot packages install.packages(c(&quot;ggcorrplot&quot;, &quot;corrplot&quot;)) Create a correlation heatmap using the ggcorrplot package library(ggcorrplot) # Calculate the correlation matrix cor_matrix &lt;- cor(df) # Create a correlation heatmap ggcorrplot(cor_matrix, method = &quot;circle&quot;, lab = TRUE, lab_size = 3, colors = c(&quot;red&quot;, &quot;white&quot;, &quot;blue&quot;)) + labs(title = &quot;Heatmap 1&quot;) Lets repeat the same using the corrplot package library(corrplot) ## corrplot 0.94 loaded # Create a correlation heatmap using corrplot corrplot(cor_matrix, method = &quot;color&quot;, type = &quot;upper&quot;, tl.col = &quot;black&quot;, tl.srt = 45) The two plots visualizes the correlation between the variables in the data set. Var5 and Var1 have a high correlation(0.98) while Var5 and Var2 have a very low correlation value(-0.07) Practical exercise Load the same airquality inbuilt R data set. Create a scatter plot to show the relationship between Wind and Temp variable. Also plot a correlation heatmap for numeric data set(Ozone, Solar.R, Wind and Temp) Solution # Load necessary libraries library(ggplot2) library(reshape2) # For reshaping data to use in heatmap ## ## Attaching package: &#39;reshape2&#39; ## The following object is masked from &#39;package:tidyr&#39;: ## ## smiths library(ggcorrplot) # For the heatmap # Load the airquality dataset data(&quot;airquality&quot;) # Scatter plot: Wind vs Temp ggplot(airquality, aes(x = Temp, y = Wind)) + geom_point(color = &quot;blue&quot;) + # Scatter plot points labs(x = &quot;Temperature&quot;, y = &quot;Wind Speed&quot;, title = &quot;Scatter Plot: Wind vs Temp&quot;) + theme_minimal() # Correlation heatmap for Ozone, Solar.R, Wind, Temp # Subset the relevant columns and remove rows with NA values airquality_subset &lt;- airquality[, c(&quot;Ozone&quot;, &quot;Solar.R&quot;, &quot;Wind&quot;, &quot;Temp&quot;)] airquality_subset &lt;- na.omit(airquality_subset) # Remove rows with missing values # Calculate correlation matrix cor_matrix &lt;- cor(airquality_subset) # Create a heatmap using ggcorrplot ggcorrplot(cor_matrix, method = &quot;circle&quot;, type = &quot;lower&quot;, lab = TRUE, title = &quot;Correlation Heatmap&quot;) ________________________________________________________________________________ 7.2.2 Introduction to Regression Analysis Regression is a statistical method used to measure the strength and relationship of a target(dependent) with one or more independent variables. Correlation is very vital in determining regression. Linear regression is the most technique of regression, there are some more advanced forms of regression. Simple Linear Regression This type of regression is used to estimate the relation of one independent variable with the target variable. For instance the relationship between age and height of children. relationship between weight and BMI of athletes, relationship between rainfall and soil erosion. When creating a simple linear regression model, a line is fitted a line to the observed data. Simple linear regression is modeled by this equation; y = c + BX + e Where;- y is the target variable X is the independent variable B is the slope.gradient(change in y for one-unit change in X) c is the y-intercept(Value of target variable when independent variable is zero) e is the error/noise. variation in y but not as a resultant explained by X The goal of simple of linear regression it to have the best fitting line that with the equation y = BX + c. Base R has a method of fitting a linear regression and finding the best fit line using lm() function. lm() stands for “linear model”. Lets create a simple linear regression model to a hypothetical data set where height of athletes is predicted based on weight. # Create a sample dataset height &lt;- c(150, 160, 170, 180, 190) # Independent variable (x) weight &lt;- c(65, 70, 75, 80, 85) # Dependent variable (y) # Combine into a data frame data &lt;- data.frame(height, weight) print(data) ## height weight ## 1 150 65 ## 2 160 70 ## 3 170 75 ## 4 180 80 ## 5 190 85 # Fit the linear model lin_reg &lt;- lm(weight ~ height, data = data) # View the summary of the model summary(lin_reg) ## Warning in summary.lm(lin_reg): essentially perfect fit: summary may be ## unreliable ## ## Call: ## lm(formula = weight ~ height, data = data) ## ## Residuals: ## 1 2 3 4 5 ## 1.335e-14 -1.367e-14 -6.097e-15 -1.891e-16 6.607e-15 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.000e+01 6.575e-14 -1.521e+14 &lt;2e-16 *** ## height 5.000e-01 3.854e-16 1.297e+15 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.219e-14 on 3 degrees of freedom ## Multiple R-squared: 1, Adjusted R-squared: 1 ## F-statistic: 1.683e+30 on 1 and 3 DF, p-value: &lt; 2.2e-16 Coefficients, the value B and c represent the intercept and the gradient(slope) in the equation. The R- Squared value indicates how well the independent variable explains the dependent variable. The closer the R-square value to 1 the better the fit. Finally, the p-value is associated with the gradient and tells the statistician whether the relationship between the variables is statistically significant. Practical exercise Using the airquality data set fit a linear regression model to find the relationship between Wind (independent variable) and Temp (dependent/target variable) Solution # Load necessary library data(&quot;airquality&quot;) # Remove missing values airquality_clean &lt;- na.omit(airquality) # Fit the linear regression model model &lt;- lm(Temp ~ Wind, data = airquality_clean) # Display the summary of the model summary(model) ## ## Call: ## lm(formula = Temp ~ Wind, data = airquality_clean) ## ## Residuals: ## Min 1Q Median 3Q Max ## -19.112 -5.646 1.014 6.254 18.888 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 91.0305 2.3489 38.754 &lt; 2e-16 *** ## Wind -1.3318 0.2226 -5.983 2.84e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 8.306 on 109 degrees of freedom ## Multiple R-squared: 0.2472, Adjusted R-squared: 0.2403 ## F-statistic: 35.79 on 1 and 109 DF, p-value: 2.842e-08 ________________________________________________________________________________ Multiple Linear Regression Unlike simple linear regression, multiple linear regression describes the linear relationship between two or more independent variables with one target(dependent) variable. The objective of multiple linear regression is to;- Find the strength of the relationship between two or more independent variables with the target variables. Find the value of the target variable at a certain value of the independent variable. When working on a multiple linear regression it is assumed that; the variance is homogeneous such that the prediction error does not change significantly across the predictor(independent) variables. It is also assumed, observations were independent and there was no hidden relationships among the variables when collecting the data. Additionally, the collected data follows a normal distribution and the independent variables have a linear relationship(linearity) with the dependent variable, therefore, the line of best fit through the data points is straight and not curved. Multiple linear regression is modeled by;- where;- \\(y\\) is the predicted value of the target variable. \\(\\beta_0\\) is the y-intercept. Value of y when all other parameters are zero. \\(\\beta_1X_1\\): \\(\\beta_1\\) is the regression coefficient of the first independent variable while \\(X_1\\) is the independent variable value. \\(\\cdots\\) do the same for however the number of independent variables are present. \\(\\beta_nX_n\\): the regression coefficient of the last independent variable. \\(\\epsilon\\) is the model error(variation not explained by the independent variables) The Multiple linear regression model calculates three things to find the best fit line for each independent variable;- The regression coefficient \\(\\beta_iX_i\\) that will minimize the overall error rate(model error). The associated p-value. If the relationship between the independent variable is statistically significant. The t-statistic of the model. T-statistic is the ratio of the difference in a number’s estimated value from its assumed value to its standard error. Lets create a multiple linear regression from a hypothetical data using base R. # Create a sample dataset height &lt;- c(150, 160, 170, 180, 190) # Independent variable 1 (x1) age &lt;- c(25, 30, 35, 40, 45) # Independent variable 2 (x2) weight &lt;- c(65, 70, 75, 80, 85) # Dependent variable (y) # Combine into a data frame data &lt;- data.frame(height, age, weight) data ## height age weight ## 1 150 25 65 ## 2 160 30 70 ## 3 170 35 75 ## 4 180 40 80 ## 5 190 45 85 # Fit the linear model with multiple predictors model &lt;- lm(weight ~ height + age, data = data) # View the summary of the model summary(model) ## Warning in summary.lm(model): essentially perfect fit: summary may be ## unreliable ## ## Call: ## lm(formula = weight ~ height + age, data = data) ## ## Residuals: ## 1 2 3 4 5 ## 1.335e-14 -1.367e-14 -6.097e-15 -1.891e-16 6.607e-15 ## ## Coefficients: (1 not defined because of singularities) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.000e+01 6.575e-14 -1.521e+14 &lt;2e-16 *** ## height 5.000e-01 3.854e-16 1.297e+15 &lt;2e-16 *** ## age NA NA NA NA ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.219e-14 on 3 degrees of freedom ## Multiple R-squared: 1, Adjusted R-squared: 1 ## F-statistic: 1.683e+30 on 1 and 3 DF, p-value: &lt; 2.2e-16 Practical exercise Using the airquality data set, fit a multiple linear regression model whereby the Solar radiation(Solar.R), Ozone and Wind are the independent variables while the Temperature (Temp) is the dependent variable. Interpret and analyze the results Solution # Load necessary library data(&quot;airquality&quot;) # Remove missing values (important for regression analysis) airquality_clean &lt;- na.omit(airquality) # Fit the multiple linear regression model model &lt;- lm(Temp ~ Solar.R + Ozone + Wind, data = airquality_clean) # Display the summary of the model summary(model) ## ## Call: ## lm(formula = Temp ~ Solar.R + Ozone + Wind, data = airquality_clean) ## ## Residuals: ## Min 1Q Median 3Q Max ## -20.942 -4.996 1.283 4.434 13.168 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 72.418579 3.215525 22.522 &lt; 2e-16 *** ## Solar.R 0.007276 0.007678 0.948 0.345 ## Ozone 0.171966 0.026390 6.516 2.42e-09 *** ## Wind -0.322945 0.233264 -1.384 0.169 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 6.834 on 107 degrees of freedom ## Multiple R-squared: 0.4999, Adjusted R-squared: 0.4858 ## F-statistic: 35.65 on 3 and 107 DF, p-value: 4.729e-16 ________________________________________________________________________________ 7.2.3 Hands-on Exercises In this course, you will be required to download the Boston Housing data set from here. Fit a multiple regression model to the data set. The goal is to predict the price of the houses in Boston, the MEDV variable. Interpret and discuss the multiple linear regression model and its implication Solution # Load the data boston_df &lt;- read.csv(&quot;data/boston_housing.csv&quot;) # Count the null values sum(is.na(boston_df)) ## [1] 0 # Fit multiple linear regression model model &lt;- lm(MEDV ~ . , data = boston_df) summary(model) ## ## Call: ## lm(formula = MEDV ~ ., data = boston_df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -15.595 -2.730 -0.518 1.777 26.199 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.646e+01 5.103e+00 7.144 3.28e-12 *** ## CRIM -1.080e-01 3.286e-02 -3.287 0.001087 ** ## ZN 4.642e-02 1.373e-02 3.382 0.000778 *** ## INDUS 2.056e-02 6.150e-02 0.334 0.738288 ## CHAS 2.687e+00 8.616e-01 3.118 0.001925 ** ## NOX -1.777e+01 3.820e+00 -4.651 4.25e-06 *** ## RM 3.810e+00 4.179e-01 9.116 &lt; 2e-16 *** ## AGE 6.922e-04 1.321e-02 0.052 0.958229 ## DIS -1.476e+00 1.995e-01 -7.398 6.01e-13 *** ## RAD 3.060e-01 6.635e-02 4.613 5.07e-06 *** ## TAX -1.233e-02 3.760e-03 -3.280 0.001112 ** ## PTRATIO -9.527e-01 1.308e-01 -7.283 1.31e-12 *** ## B 9.312e-03 2.686e-03 3.467 0.000573 *** ## LSTAT -5.248e-01 5.072e-02 -10.347 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.745 on 492 degrees of freedom ## Multiple R-squared: 0.7406, Adjusted R-squared: 0.7338 ## F-statistic: 108.1 on 13 and 492 DF, p-value: &lt; 2.2e-16 The multiple linear regression model explains 73.4% of the variance in house prices (Adjusted R-squared = 0.7338). Significant predictors include crime rate (CRIM), room count (RM), and distance to employment (DIS), among others ________________________________________________________________________________ 7.3 Advanced Statistical Methods 7.3.1 Introduction to Hypothesis Testing 7.3.1.1 Concept of Hypothesis Testing Hypothesis testing is a type of statistical analysis that is used to make assumptions of a population based on a sample of data. It is particularly used to find the relationship between two variables(populations). A real life example of hypothesis testing is that a teacher may assume that 60% of the students come from a middle-class family. There are two types of hypothesis; Null hypothesis(\\(H_0\\)) Alternate hypothesis (\\(H_1\\) or \\(H_a\\)) Null hypothesis is states that there is no effect or no difference(\\(\\mu = 0\\)). For instance there is no effect of standards of living to college admissions. Alternate hypothesis is the opposite and contradicts the null hypothesis. It provide evidence for what the statistician is trying to find(\\(\\mu \\neq 0\\)). In this case, the standards of living have an effect on college admissions. The important aspects before conducting hypothesis testing are;- Significance level. It is the probability of rejecting the null hypothesis when it is actually true. P-Value is the probability of obtaining a test statistic at least as extreme as the one observed, given the null hypothesis is true. Most hypothesis testing projects are set at 0.05. Less than 0.05(or the set value) indicates the null the test is statistically significant and the null hypothesis should be rejected. Otherwise, the test is statistically insignificant and the null hypothesis is not rejected. Test statistic also called T-statistic is a standardized value calcluated during a hypothesis test. It cab z-test or a t-test. -Decision rule is based on the calculated p-value and the significant level. In a hypothesis test where the significant and the p-value is 0.03444 the null hypothesis is not rejected. Now that you are familiar with the hypothesis testing aspects, take the following steps to perform hypothesis testing;- Formulating the hypothesis by defining the null and alternate hypothesis. Collect and analyze the data. Choose a significant level(\\(a\\)) and calculate the p-value. Make a decision by comparing the p-value to the significant level. Conclude your analysis results. 7.3.1.2 T-tests One-Sample t-test One sample t-test is a statistical method used to find if the mean of a sample is different from the population(or preassumed) mean. It is based on the t-distribution(most observations fall close to the mean, and the rest of the observations make up the tails on either side) and is commonly used when dealing with small sample sizes. One sample t-test is especially performed where the population standard deviation is unknown. Below is the formula for one sample t-test \\[t={{\\overline{X}-\\mu}\\over s / \\sqrt{n}}\\] where; \\(t\\): the one sample t-test value. t-test statistic \\(n\\): the number of the observations in the sample \\(\\overline{X}\\): is the sample mean \\(s\\): standard deviation of the sample \\(\\mu\\): Hypothesized population mean The result \\(t\\), simply measures how many standard errors the sample mean is away from the hypothesized population mean. Before conducting t-test, there is a need to establish the null(H0) and alternate hypothesis(Ha) where; Null Hypothesis(H0): There is no significant difference between the sample mean and the population(hypothesized) mean. ALternate Hypothesis(Ha): There is a significant difference between the sample mean and the population mean. P-value is the probability value that tells you how likely is that your data could have occurred under null hypothesis. In our case a p-value of below 0.05 is considered to be statistically significant and the null value is rejected. The vice versa is true Lets perform a t-test using R; We will generate sample data # Set seed for reproducibility set.seed(123) # Generate random student marks (out of 100) student_marks &lt;- rnorm(30, mean = 65, sd = 10) # Display the first few marks head(student_marks) ## [1] 59.39524 62.69823 80.58708 65.70508 66.29288 82.15065 Perform the t-test # Conduct one-sample t-test t_test_result &lt;- t.test(student_marks, mu = 70) # Display the t-test result print(t_test_result) ## ## One Sample t-test ## ## data: student_marks ## t = -3.0546, df = 29, p-value = 0.004797 ## alternative hypothesis: true mean is not equal to 70 ## 95 percent confidence interval: ## 60.86573 68.19219 ## sample estimates: ## mean of x ## 64.52896 Practical exercise Conduct one sample t-test on the sepal length of the setosa iris. The pre-assumed mean is 5.84 units. Solution Null Hypothesis(\\(H_0\\)): There is no significant difference between the setosa sepal length mean and the pre-assumed mean Alternate Hypothesis(\\(H_a\\)): There is a significant difference between the setosa sepal length and the pre-assumed mean Note: If the pre-assumed mean is not given in the question therefore the whole population mean is used(as the pre-assumed mean). In this case, the setosa sepal length mean of the whole iris species. library(dplyr) # Load the data data(iris) # Get the setosa sepal length setosa &lt;- iris %&gt;% filter(Species==&quot;setosa&quot;) setosa.sepal.length &lt;- setosa$Sepal.Length # Calculate one-sample ttest ## Set the pre-assumed mean pre_assumed_mean &lt;- 5.84 # as in the question t_test_result &lt;- t.test(setosa.sepal.length, mu = pre_assumed_mean) print(t_test_result) ## ## One Sample t-test ## ## data: setosa.sepal.length ## t = -16.73, df = 49, p-value &lt; 2.2e-16 ## alternative hypothesis: true mean is not equal to 5.84 ## 95 percent confidence interval: ## 4.905824 5.106176 ## sample estimates: ## mean of x ## 5.006 The p-value is much below 0.05, therefore the null hypothesis is rejected. It is concluded that there is a significant difference between the sepal length mean and the pre-assumed mean ________________________________________________________________________________ Two-Sample t-test Unlike one sample t-test where a sample population is tested against a pre-assumed mean, the Tow-sample t-test determines if there is a significant difference between the means of two independent populations. The practical application of two-sample t-test can be when comparing the test scores of two classes. This helps the statistician to understand if one class did better than the other one or it’s just a matter of luck. These are the prerequisites before conducting a two-sample t-test; The groups contain separate data with a similar distribution. The two populations have a normal(typical bell-curve) distribution. The two sample populations have a similar variations The two sample t-test is calculated by; Where; \\(\\overline{x}_1\\) and \\(\\overline{x}_2\\) are the mean of the first sample and the second sample respectively \\(s_{1}\\) and \\(s_{2}\\) are the standard deviation of sample 1 and sample 2 respectively \\(n_1\\) and \\(n_2\\) are the sample sizes of the first and second sample respectively. Let create a random population of student scores for two classes and perform two-sample t-test in R; # Generate the population sample set.seed(123) group_A_scores &lt;- rnorm(25, mean = 75, sd = 8) # Group A group_B_scores &lt;- rnorm(25, mean = 80, sd = 10) # Group B # Display the first few scores of each group head(group_A_scores) ## [1] 70.51619 73.15858 87.46967 75.56407 76.03430 88.72052 head(group_B_scores) ## [1] 63.13307 88.37787 81.53373 68.61863 92.53815 84.26464 Performing the two sample t-test. Lets set the confidence level to 95%(0.95) ttest_result = t.test(group_A_scores, group_B_scores, alternative = &quot;two.sided&quot;, mu = 0, paired = FALSE, var.equal = FALSE, conf.level = 0.95) ttest_result ## ## Welch Two Sample t-test ## ## data: group_A_scores and group_B_scores ## t = -2.6403, df = 46.312, p-value = 0.01125 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -11.080984 -1.495049 ## sample estimates: ## mean of x mean of y ## 74.73336 81.02137 t-value = -2.6403: This indicates the difference between the means of the two groups, in terms of standard errors. A higher absolute value suggests a larger difference. Degrees of freedom (df) = 46.312: This reflects the sample size and variability in the data. p-value = 0.01125: Since the p-value is less than 0.05, we reject the null hypothesis. This suggests that the difference in means between Group A and Group B is statistically significant. 95% confidence interval: (-11.08, -1.50): This indicates that we are 95% confident that the true difference in means lies between -11.08 and -1.50. Mean of x (Group A) = 74.73, Mean of y (Group B) = 81.02: The average score of Group B is higher than Group A. In summary, the test shows a significant difference between the means of the two groups, with Group B having higher scores Practical exercise Using the iris data set, compare the petal length of the versicolor and virginica species using two-sample t-test. Interpret the results Solution Lets formulate the hypothesis; Null Hypothesis(\\(H_0\\)): There is no significant difference between the mean of virginica and the mean of versicolor Alternate Hypothesis(\\(H_a\\)): There is a significant difference between the mean of virginica and the versicolor library(dplyr) # Load the data data(iris) # Retrieve the data for the two species ## Virginica petal length virginica &lt;- iris%&gt;% filter(Species==&quot;virginica&quot;) virginica.petal.length &lt;- virginica$Petal.Length ## Versicolor petal length versicolor &lt;- iris%&gt;% filter(Species==&quot;versicolor&quot;) versicolor.petal.length &lt;- versicolor$Petal.Length # Conduct two-sample t-test ttest_result = t.test(virginica.petal.length, versicolor.petal.length, alternative = &quot;two.sided&quot;, mu = 0, paired = FALSE, var.equal = FALSE, conf.level = 0.95) ttest_result ## ## Welch Two Sample t-test ## ## data: virginica.petal.length and versicolor.petal.length ## t = 12.604, df = 95.57, p-value &lt; 2.2e-16 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 1.08851 1.49549 ## sample estimates: ## mean of x mean of y ## 5.552 4.260 With a p-value of less than 0.05, the null hypothesis is rejected. It is concluded that there is a significant difference between the mean of virginica species and the mean of versicolor species. ________________________________________________________________________________ 7.3.1.3 ANOVA (Analysis of Variance) ANOVA is a statistical test used to analyze the difference between the means of more than two groups. This is different from the ttest that analyzes one or two groups, it uses F test to find the statistical significance. Multiple means are compared at once and if one mean is different the hypothesis is rejected. The F test compares the variance in each group from the overal group variance. An practical example of ANOVA is where a farmer wants to test the effect of three different fertilizers on the crop yield. The difference in the crop yield will be calculated. Before conducting ANOVA, the following assumptions are made; Independence of observations: the data was collected using statistically valid sampling methods and there are no hidden relationships among the observations. Normal distribution: the dependent variable should follow a normal distribution. Homogeinity of variance: All the groups being tested should have similar variations. Lets calculate the ANOVA using the the crop yield data set. The fertilizer are in three categories; 1, 2 and 3 # Load the data set crop_df &lt;- read.csv(&quot;data/cropdata.csv&quot;) head(crop_df) # view the first few rows of the data set ## density block fertilizer yield ## 1 1 1 1 177.2287 ## 2 2 2 1 177.5500 ## 3 1 3 1 176.4085 ## 4 2 4 1 177.7036 ## 5 1 1 1 177.1255 ## 6 2 2 1 176.7783 # Calculate one way ANOVA anova &lt;- aov(yield ~ fertilizer, data = crop_df) anova ## Call: ## aov(formula = yield ~ fertilizer, data = crop_df) ## ## Terms: ## fertilizer Residuals ## Sum of Squares 5.74322 36.21101 ## Deg. of Freedom 1 94 ## ## Residual standard error: 0.6206638 ## Estimated effects may be unbalanced The ANOVA output provides insights into the variation in crop yield explained by the fertilizer type. Here’s a detailed breakdown of the results: Sum of Squares (fertilizer) = 5.74322: This value represents the variation in crop yield that can be attributed to the different types of fertilizers used in the experiment. In this case, 5.74322 units of the total variation are explained by fertilizer differences. Sum of Squares (Residuals) = 36.21101: This is the unexplained variation in crop yield, also known as the error term. This shows how much of the variation is due to factors not accounted for in the model, such as environmental factors or random error. Degrees of Freedom (fertilizer) = 1: There is only 1 degree of freedom for the fertilizer factor, which means there was a comparison between two groups (likely two fertilizer types or one fertilizer versus a control). Degrees of Freedom (Residuals) = 94: There are 94 degrees of freedom associated with the residuals. This is related to the total number of observations minus the number of groups being compared. In this case, the large degrees of freedom indicate a sizable data set. Residual Standard Error = 0.6206638: This value represents the typical deviation of the observed yield values from the predicted values, given the current model. A lower residual standard error suggests a better fit of the model to the data, though this value needs to be interpreted in context. The results show that the fertilizer type explains some of the variation in crop yield (Sum of Squares = 5.74322), while a larger portion remains unexplained (Sum of Squares of Residuals = 36.21101). To fully interpret the significance of this effect, a p-value and F-statistic would typically be calculated, but these are not provided here. Additionally, the residual standard error (0.6206638) gives an indication of the spread of the data around the predicted values, but more information would be needed to assess the strength of the model’s fit. In conclusion, while the fertilizer has some effect on crop yield, the overall variability and potential unbalanced data need further exploration for a complete understanding. Practical exercise Perform ANOVA on the sepal width of the three species in the iris data set and interpret the results. Solution Lets formulate the hypothesis; Null Hypothesis(\\(H_0\\)): There is no siginificant difference among the sepal width of all the iris species. Alternate Hypothesis(\\(H_a\\)): There is a signifincant difference among the sepal width of all the iris species # Load the data data(iris) # Perform the ANOVA anova_result &lt;- aov(Sepal.Width ~ Species, data = iris) # Display the ANOVA table summary(anova_result) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Species 2 11.35 5.672 49.16 &lt;2e-16 *** ## Residuals 147 16.96 0.115 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 it indicates that there are statistically significant differences in the mean Sepal.Width between the species. ________________________________________________________________________________ 7.3.1.4 Chi-Square Test This is a statistical test that determines the difference between the observed and the expected data. It determines if the relationship between two categorical variables is due to chance or a relationship between them. It is calculated by; \\[x_{c}^{2} = \\frac{\\sum(O_{i}-E_{i})}{E_i}\\] Where; \\(c\\) is the degree of freedom. This is a statistical calculation that represents the number of variables that can carry and is calculated to ensure the chi-square tests are statistically valid \\(O\\) is the observed value \\(E\\) is the expected value Lets perform Chi-square on a survey data from the MASS library. The survey data represents data from a survey conducted on students. Null Hypothesis (\\(H_0\\)): The smoking habit is independent of the student’s exercise level ALternate Hypothesis (\\(H_a\\)): The smoking habit is dependent on the exercise level. Load the data # Load the library library(MASS) head(survey) # view the data ## Sex Wr.Hnd NW.Hnd W.Hnd Fold Pulse Clap Exer Smoke Height M.I ## 1 Female 18.5 18.0 Right R on L 92 Left Some Never 173.00 Metric ## 2 Male 19.5 20.5 Left R on L 104 Left None Regul 177.80 Imperial ## 3 Male 18.0 13.3 Right L on R 87 Neither None Occas NA &lt;NA&gt; ## 4 Male 18.8 18.9 Right R on L NA Neither None Never 160.00 Metric ## 5 Male 20.0 20.0 Right Neither 35 Right Some Never 165.00 Metric ## 6 Female 18.0 17.7 Right L on R 64 Right Some Never 172.72 Imperial ## Age ## 1 18.250 ## 2 17.583 ## 3 16.917 ## 4 20.333 ## 5 23.667 ## 6 21.000 Create a contigency table between the Smoke and the Exercise leel. # Create a contingency table with the needed variables. stu_data = table(survey$Smoke,survey$Exer) print(stu_data) ## ## Freq None Some ## Heavy 7 1 3 ## Never 87 18 84 ## Occas 12 3 4 ## Regul 9 1 7 Perform the chi-square test on the stu_data, the contigency table. # applying chisq.test() function print(chisq.test(stu_data)) ## ## Pearson&#39;s Chi-squared test ## ## data: stu_data ## X-squared = 5.4885, df = 6, p-value = 0.4828 From the results, the p-value is 0.4828 which is greater than 0.05 therefore the null hypothesis is not rejected. It’c concluded that the smoking habit is independent of the exercise level since there is weak to now correlation between the Smoke and Exer variables. Finally, lets visualize the results from the contigency table; # Visualize the data with a bar plot barplot(stu_data, beside = TRUE, col = c(&quot;lightblue&quot;, &quot;lightgreen&quot;), main = &quot;Smoking Habits vs Exercise Levels&quot;, xlab = &quot;Exercise Level&quot;, ylab = &quot;Number of Students&quot;) # Add legend separately legend(&quot;center&quot;, legend = rownames(stu_data), fill = c(&quot;lightblue&quot;, &quot;lightgreen&quot;)) You can see from the table, those students who never smoke lead in every exercise level while the heavy smokers are the least in every group. Practical exercise Using the Iris data set, perform a Chi-square test to determine if there is a relationship between two categorical variables: the species (Species) and a new categorical variable that classifies sepal width (Sepal.Length) into categories (e.g., “Short”, “Medium”, “Long”). “Short”: below 3.0 “Medium”: above 3.0 to 3.8 “Long”: above 3.8 Follow the steps below; Create a new variable sepal.Width.Category in the data set by categorizing the Sepal.Width variable into 3 categories: \"Short\", \"Medium\", and \"Long\" (as per the defined ranges). Perform a Chi-square test to see if there’s an association between the new sepal.Width.Category categories and the Species column. Interpret the results of the Chi-square test Solution Load the data set data(iris) Create a new variable sepal.Width.Category in the data set by categorizing the Sepal.Width variable into 3 categories: \"Short\", \"Medium\", and \"Long\" (as per the defined ranges). # Load necessary libraries library(dplyr) # Create a new column &#39;Sepal_Width_Category&#39; based on conditions iris &lt;- iris %&gt;% mutate(sepal.Width.Category = case_when( Sepal.Width &lt; 3.0 ~ &quot;Short&quot;, Sepal.Width &gt;= 3.0 &amp; Sepal.Width &lt;= 3.8 ~ &quot;Medium&quot;, Sepal.Width &gt; 3.8 ~ &quot;Long&quot; )) # Display the first few rows to see the new column head(iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa ## sepal.Width.Category ## 1 Medium ## 2 Medium ## 3 Medium ## 4 Medium ## 5 Medium ## 6 Long Perform a Chi-square test to see if there’s an association between the new sepal.Width.Category categories and the Species column. # Create a contigency table sepal_width_data = table(iris$sepal.Width.Category,iris$Species) print(sepal_width_data) ## ## setosa versicolor virginica ## Long 6 0 0 ## Medium 42 16 29 ## Short 2 34 21 # applying chisq.test() function print(chisq.test(sepal_width_data)) ## Warning in chisq.test(sepal_width_data): Chi-squared approximation may be ## incorrect ## ## Pearson&#39;s Chi-squared test ## ## data: sepal_width_data ## X-squared = 50.918, df = 4, p-value = 2.322e-10 Intepret the results ________________________________________________________________________________ 7.3.2 Hands-on Exercises You are required to download the Groundhog Day Forecasts and Temperatures data set from here. Perform one sample t-test on February Average temperature. The pre-assumed mean is 35 Conduct two sample t-test between the North East average temperature for March and the overall March daily temperature Perform ANOVA between the presence of Punxsutawney Phil and the February average temperature Interpret the results Solution Load the data df &lt;- read.csv(&quot;data/GroundhogDayForecastsandTemperaturesdata.csv&quot;) Perform one sample t-test on February Average temperature. The pre-assumed mean is 35 Formulate the hypothesis - Null hypothesis (\\(H_0\\)): There is no significant difference between the February Average Temeprature mean and the pre-assumed mean - Alternate Hypothesis(\\(H_a\\)): There is a significant difference between the February Average Temperature and the pre-assumed mean. one.sample.ttest &lt;- t.test(df$February.Average.Temperature, mu=35) # Display the results print(one.sample.ttest) ## ## One Sample t-test ## ## data: df$February.Average.Temperature ## t = -4.061, df = 122, p-value = 8.671e-05 ## alternative hypothesis: true mean is not equal to 35 ## 95 percent confidence interval: ## 33.21928 34.38641 ## sample estimates: ## mean of x ## 33.80285 The p-value is below 0.05, therefore the is enough evidence to reject the null hypothesis and conclude that there is a significant difference between the February Average Temperature and the pre-assumed mean(35). Conduct two sample t-test between the North East average temperature for March and the overall March daily temperature. Formulate the hypothesis: Null hypothesis(\\(H_0\\)): There is no significant difference between the North East Average Temperature for March and the March daily temparature. ALternate Hypothesis(\\(H_a\\)): There is a significant difference between the North East Average Temperature and the March daily temparature. # Perform two sample ttest ttest_result = t.test(df$March.Average.Temperature..Northeast., df$March.Average.Temperature, alternative = &quot;two.sided&quot;, mu = 0, paired = FALSE, var.equal = FALSE, conf.level = 0.95) ttest_result ## ## Welch Two Sample t-test ## ## data: df$March.Average.Temperature..Northeast. and df$March.Average.Temperature ## t = -21.288, df = 227.4, p-value &lt; 2.2e-16 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -10.193516 -8.466321 ## sample estimates: ## mean of x mean of y ## 32.36748 41.69740 The p-value is less than 0.05, therefore the null hypothesis is rejected. It is concluded that there is significant difference between the North East Average Temperature for March and overall Average daily Temperature for March Perform ANOVA between the presence of Punxsutawney Phil and the February average temperature Formulate the hypothesis Null Hypothesis(\\(H_0\\)): There is no significant difference between the presence and the absence of Punxsutawney Phil during February based on the Average Temperature Alternate Hypothesis(\\(H_a\\)): There is a significant difference between the presence and the absence of Punxsutawney Phil during February based on the Average Temperature # Perform ANOVA anova_result &lt;- aov(February.Average.Temperature ~ df$Punxsutawney.Phil, data = df) # Display the ANOVA table summary(anova_result) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## df$Punxsutawney.Phil 4 92.7 23.18 2.258 0.0669 . ## Residuals 118 1211.3 10.27 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## 9 observations deleted due to missingness ________________________________________________________________________________ 7.4 Project: Applying Statistical Analysis 7.4.1 Project Overview The primary goal of this project is to give you an opportunity to apply and integrate all the statistical concepts techniques learned so far using R. The project will guide you through the complete statistical modeling workflow. You are therefore required to choose a data set from the list below; MBA Admission dataset, Class 2025 - download here Global Black Money Transactions Dataset - download here Crop Yield Prediction Dataset - download here ChickWeight inbuilt R dataset - load from the command data(\"ChickWeight\") Seatbelts data set - load from the command data(\"Seatbelts\") The “Groceries” from the R package comes arules- Load the data by first importing arules(library(arules)) then the data by data(\"Groceries\") \"CreditCard\" data from package AER- Load the data by first importing AER (library(AER)) then the data by data(\"CreditCard\") You will use the selected data set to; Identify the research questions that will help focus on what you want to investigate. Determine the key variables in your research questions like the independent and dependent variables. Formulate the null and alternate hypothesis 7.4.2 Conducting Statistical Analysis You will use the statistical techniques learned for instance descriptive statistics, correlation, regression and hypothesis testing to analyze this data set Note: Emphasize on interpreting the results and understanding their implications 7.4.3 Presenting Statistical Findings You will finally prepare a brief presentation that will summarize the statistical analysis and research Discuss in groups the insights obtained, challenges faced and any potential improvements 7.4.4 Example: Ecommerce Statistical Analysis Formulation of Hypothesis T-Tests One-Sample T-test Two-sample T-Test ANOVA(Analysis of Variance) Chi-Square Test Key Findings ________________________________________________________________________________ "],["working-with-real-world-data.html", "Chapter 8 Working with Real-World Data 8.1 Handling Large Datasets 8.2 Data Cleaning and Transformation", " Chapter 8 Working with Real-World Data 8.1 Handling Large Datasets 8.1.1 Challenges of Large Data sets When working with data in R, one can encounter large data sets that are challenging to work on. These are some of the challenges; Data I/O takes a long time such that large files takes a long time to transfer data to or from a computers system which will slow down crucial processes like network operations, communication between the devices such as keyboard and microphone, and sharing data R has a file size limit of approximately 2 to 4gb, therefore it is a challenge to work on data sets above the limit like 5gb. There is more difficulty in indexing data sets which extremely large number of rows and columns. The processing speed of several algorithms and pre-trained model will reduce. Large data sets pose a threat on the memory management. R stores data entirely on the memory, therefore it can slow down or even crush the program under extreme cases. As a data analyst/statistician, large data sets are inevitable therefore researchers have worked on the issue of large data sets and come up with the following solutions; Optimize the memory usage through data type conversion and row-wise processing Processing large data sets in batches or in chunks. Using memory efficient objects and programming tricks like nested functions, lazy loading(load data into memory when its needed) and use of garbage collection where objects that are no longer useful are disposed. Partitioning and streaming by loading only small pieces of data into memory at any point in time. Use specialized packages for large scale analysis like data.table. Apply data sampling. Instead of processing the whole data at once. Take a random manageable sample from the data set and process it. 8.1.2 Efficient Data Handling Techniques Lets dive deep on how to work on large data sets in R by applying the popular methods in data science. 8.1.2.1 Using data.table Package The data.table package delivers an improved version of a data.frame structure. The data structure from this package(has the same name, data.table) is high performance, memory efficient thus being more fit for large data sets than the data.frame kind of a data structure. Lets create a simple data.table from a random data set. # Load the library library(data.table) # Create the data.table student_dtb &lt;- data.table(stuid = c(2, 5, 3, 4, 6, 7, 4, 2, 0),# student id age = c(23, 45, 67, 23, 41, 43, 54, 67, 89) ) student_dtb ## stuid age ## &lt;num&gt; &lt;num&gt; ## 1: 2 23 ## 2: 5 45 ## 3: 3 67 ## 4: 4 23 ## 5: 6 41 ## 6: 7 43 ## 7: 4 54 ## 8: 2 67 ## 9: 0 89 A data.frame is created the same as a data.table # Create the data.frame student_df &lt;- data.table(stuid = c(2, 5, 3, 4, 6, 7, 4, 2, 0),# student id age = c(23, 45, 67, 23, 41, 43, 54, 67, 89) ) student_df ## stuid age ## &lt;num&gt; &lt;num&gt; ## 1: 2 23 ## 2: 5 45 ## 3: 3 67 ## 4: 4 23 ## 5: 6 41 ## 6: 7 43 ## 7: 4 54 ## 8: 2 67 ## 9: 0 89 They almost look similar but they are more different in operations. Here are some of the differences; data.table data.frame faster than data.frame 20X slower than data.table Used for more complex data structures and big data Used for smaller tables and matrices Has built-in features like rolling joins and overlapping range Lacks more features but friendly to a beginner code efficient Utilizes more code to get something done setDF(dt) function is used to convert it to a data.frame where argument dt is the data.table setDT(df) function is used to convert it to data.table where argument df is the data.frame Syntax: data.table() Syntax: data.frame() There are varieties of data.table operations that can be applied on large data sets. Let’s explore a few of them Filtering rows Rows in a data.table can be filtered using conditions in data.table. For instance, lets filter the data above to find the ones above the age of 40 # Get the ones above 40 years dtb1 &lt;- student_df[age &gt; 40] print(dtb1) ## stuid age ## &lt;num&gt; &lt;num&gt; ## 1: 5 45 ## 2: 3 67 ## 3: 6 41 ## 4: 7 43 ## 5: 4 54 ## 6: 2 67 ## 7: 0 89 Selecting Specific Columns You can select specific columns in a data.table. Lets select the age column # Select the age column age_dt &lt;- student_df[, .(age)] print(age_dt) ## age ## &lt;num&gt; ## 1: 23 ## 2: 45 ## 3: 67 ## 4: 23 ## 5: 41 ## 6: 43 ## 7: 54 ## 8: 67 ## 9: 89 Aggregating data Aggregation can be done with the help of functions like mean(), sum() … # Create a sample data.table dt &lt;- data.table( ID = 1:10, Name = c(&quot;Alice&quot;, &quot;Bob&quot;, &quot;Charlie&quot;, &quot;David&quot;, &quot;Eve&quot;, &quot;Frank&quot;, &quot;Grace&quot;, &quot;Hannah&quot;, &quot;Isaac&quot;, &quot;Jack&quot;), Age = c(25, 30, 35, 40, 28, 32, 30, 29, 33, 36), Score = c(85, 90, 88, 92, 76, 95, 89, 78, 82, 91) ) # Print the data.table dt ## ID Name Age Score ## &lt;int&gt; &lt;char&gt; &lt;num&gt; &lt;num&gt; ## 1: 1 Alice 25 85 ## 2: 2 Bob 30 90 ## 3: 3 Charlie 35 88 ## 4: 4 David 40 92 ## 5: 5 Eve 28 76 ## 6: 6 Frank 32 95 ## 7: 7 Grace 30 89 ## 8: 8 Hannah 29 78 ## 9: 9 Isaac 33 82 ## 10: 10 Jack 36 91 # Calculate the average score by Age aggregated_dt &lt;- dt[, .(Average_Score = mean(Score)), by = Age] print(aggregated_dt) ## Age Average_Score ## &lt;num&gt; &lt;num&gt; ## 1: 25 85.0 ## 2: 30 89.5 ## 3: 35 88.0 ## 4: 40 92.0 ## 5: 28 76.0 ## 6: 32 95.0 ## 7: 29 78.0 ## 8: 33 82.0 ## 9: 36 91.0 Chaining Multiple opeerations can be chained together for efficiency. In this case, we are; filtering to get the student with score of above 85 aggregate the average score by mean # Chain operations: Filter and aggregate result_dt &lt;- dt[Score &gt; 85, .(Average_Score = mean(Score)), by = Age] print(result_dt) ## Age Average_Score ## &lt;num&gt; &lt;num&gt; ## 1: 30 89.5 ## 2: 35 88.0 ## 3: 40 92.0 ## 4: 32 95.0 ## 5: 36 91.0 Alternatively, we can filter to get the records with a score of above 85 and select the Name and Score column. # Filter and select columns filtered_selected_dt &lt;- dt[Score &gt; 85, .(Name, Score)] print(filtered_selected_dt) ## Name Score ## &lt;char&gt; &lt;num&gt; ## 1: Bob 90 ## 2: Charlie 88 ## 3: David 92 ## 4: Frank 95 ## 5: Grace 89 ## 6: Jack 91 Practical exercise In this course, you will be required to use the US Accidents (2016 - 2023) data set which will be downloaded from here. Download the data and extract it to a specified destination path. Use the data.table package to; read the data set into R using a fread(\"filepath\") function. filter to get the rows with Severity of more than 2 select the Source, Severity and the Distance(mi) columns Aggregate to find the distance of each source Solution read the data set into R using a fread(\"filepath\") function library(data.table) # Read the data us_accidents_dtb &lt;- fread(&quot;big-data/us_accidents/US_Accidents_March23.csv&quot;) # head(us_accidents_dtb) # uncomment to show the data table filter to get the rows with Severity of more than 2 more_severe &lt;- us_accidents_dtb[Severity &gt; 2] # head(more_severe) # uncomment to show the data table select the Source, Severity and the Distance(mi) columns selected_data &lt;- us_accidents_dtb[, .(Source, Severity, `Distance(mi)`)] head(selected_data) ## Source Severity Distance(mi) ## &lt;char&gt; &lt;int&gt; &lt;num&gt; ## 1: Source2 3 0.01 ## 2: Source2 2 0.01 ## 3: Source2 2 0.01 ## 4: Source2 3 0.01 ## 5: Source2 2 0.01 ## 6: Source2 3 0.01 Aggregate to find the distance of each source # Calculate the average distance by Source aggregated_dt &lt;- selected_data[, .(Average_Distance = mean(`Distance(mi)`)), by = Source] print(aggregated_dt) ## Source Average_Distance ## &lt;char&gt; &lt;num&gt; ## 1: Source2 0.21666965 ## 2: Source3 0.05937252 ## 3: Source1 0.83691407 ________________________________________________________________________________ 8.1.2.2 Memory Management in R A clear grasp of memory management in R will help the analyst/statistician predict how much computer memory you will need for a given task and make you have the most out of the memory. Also, a proper memory management is crucial for effective use of the sytem resources hence the program running smoothly. The R objects are stored in the Random Access Memory (RAM). Lets explore some of the key techniques of Memory Management; Removing unused objects Objects like variables and packages that are no longer required need to be removed to free up the memory. The rm() function is used to delete them # Create a variable x x &lt;- rnorm(11000) # Remove the variable x rm(x) # variable x is deleted You can use the function ls() (with no argument) to confirm if x exists got deleted in the variable list Monitoring memory usage R has several functions that help in monitoring the memory usage. This helps the programmer identify if the session is consuming too much memory. object.size() is one of function used to compute the memory usage. # Create a variable x x &lt;- rnorm(100000) # Compute the memory usage object.size(x) ## 800048 bytes The variable x consumes approximately 800048 bytes of memory Besides, one can apply memory profiling, that is, tracking the memory usage while the scripts executes. Rprof() is an inbuilt R profile that gets the job done in real time. Lets demonstrate # Start memory profiling Rprof(memory.profiling = TRUE) # Run some computations x &lt;- rnorm(1e6) y &lt;- rnorm(1e6) # Stop profiling Rprof(NULL) # Summary of memory usage summaryRprof() ## $by.self ## self.time self.pct total.time total.pct ## &quot;rnorm&quot; 0.14 100 0.14 100 ## ## $by.total ## total.time total.pct self.time self.pct ## &quot;rnorm&quot; 0.14 100 0.14 100 ## &quot;block_exec&quot; 0.14 100 0.00 0 ## &quot;call_block&quot; 0.14 100 0.00 0 ## &quot;eng_r&quot; 0.14 100 0.00 0 ## &quot;eval_with_user_handlers&quot; 0.14 100 0.00 0 ## &quot;eval&quot; 0.14 100 0.00 0 ## &quot;evaluate_call&quot; 0.14 100 0.00 0 ## &quot;evaluate::evaluate&quot; 0.14 100 0.00 0 ## &quot;evaluate&quot; 0.14 100 0.00 0 ## &quot;generator$render&quot; 0.14 100 0.00 0 ## &quot;handle&quot; 0.14 100 0.00 0 ## &quot;in_dir&quot; 0.14 100 0.00 0 ## &quot;in_input_dir&quot; 0.14 100 0.00 0 ## &quot;knitr::knit&quot; 0.14 100 0.00 0 ## &quot;process_file&quot; 0.14 100 0.00 0 ## &quot;process_group&quot; 0.14 100 0.00 0 ## &quot;render_book_script&quot; 0.14 100 0.00 0 ## &quot;render_book&quot; 0.14 100 0.00 0 ## &quot;render_cur_session&quot; 0.14 100 0.00 0 ## &quot;rmarkdown::render_site&quot; 0.14 100 0.00 0 ## &quot;rmarkdown::render&quot; 0.14 100 0.00 0 ## &quot;timing_fn&quot; 0.14 100 0.00 0 ## &quot;withCallingHandlers&quot; 0.14 100 0.00 0 ## &quot;withVisible&quot; 0.14 100 0.00 0 ## &quot;xfun:::handle_error&quot; 0.14 100 0.00 0 ## ## $sample.interval ## [1] 0.02 ## ## $sampling.time ## [1] 0.14 Additionally, the pryr package goes further to come up with additional functions to monitor memory usage. It can be installed by; install.packages(&quot;pryr&quot;) Below is a demonstration of how pryr library works # Load the library library(pryr) ## ## Attaching package: &#39;pryr&#39; ## The following object is masked from &#39;package:data.table&#39;: ## ## address ## The following object is masked from &#39;package:dplyr&#39;: ## ## where ## The following objects are masked from &#39;package:purrr&#39;: ## ## compose, partial # Check memory usage of the entire R session mem_used() ## 5.08 GB # Create a variable y &lt;- rnorm(1e6) # Check memory usage of a specific object object_size(y) ## 8.00 MB Freeing up the Memory R has an automatic memory management system, however the programmer might request garbage collection. The gc() function helps you clean unused memory. This is helpful after removing large objects # Collect the garbage to free up unused memory gc() The gc() function will accelerate the process of garbage collection rather than waiting for R to automatically free up the memory. Practical exercise After working on the previous practical exercise, you will manage the memory by; Finding the objects that fill up the memory and list them. Deleting the variables not required Using gc to accelerate the process of garbage collection 8.1.3 Reading and Writing Large Files 8.1.3.1 Optimized File I/O The data.table package offers highly optimized functions like fread() and fwrite() that are much faster for reading and writing large files. They provide significant speedups due to efficient file handling and parallel processing. The I/O processes can be optimized by; Efficient reading Efficient writing Parallel Processing for faster import/export Efficient Reading The fread() function is designed to read large CSV files much faster than the read.csv() by using multiple threads and optimized parsing. Here is how you can read the file # Load the library library(data.table) # Read large csv file data &lt;- fread(&quot;GlobalLandTemperaturesByCity.csv&quot;) Download the csv file from here. Extract the downloaded file and read the first csv file. These are the perks of reading data using fread() function; - Parallel Reading: Uses multiple threads to read files faster. - Automatic Type Detection: Automatically detects column types without needing to pre-specify them. - Handling Big Data: Efficient for reading gigabytes of data. Efficient Writing A large data.table can saved locally using the fwrite() function. It can handle large data volumes efficiently and supports various formats. # Writing a data frame to a CSV file using fwrite() fwrite(data, &quot;output_file.csv&quot;) The \"output_file\" is the destination path where the data.table is written. These are the features of fwrite(); Fast Export: Writes data using optimized I/O to speed up the process. Compression: Supports compression for efficient storage. Parallel Writing: Uses multiple threads to write files faster. Parallel Processing for faster Import/Export The fread() and fwrite() use parallel processing by default however you can configure the number of threads used for reading and writing. The setDTthreads() is used to achieve this. Lets now read and write the data using 4 threads # Set the number of threads to use for parallel processing setDTthreads(4) # Now fread() and fwrite() will use 4 threads data &lt;- fread(&quot;large_dataset.csv&quot;) fwrite(data, &quot;output_file.csv&quot;) Practical exercise In this exercise, you are required to use the US Accidents (2016 - 2023) data set and solve the problems below; Read the data set using fread() Time the fread() when reading the file. Hint: replace the \"large_dataset.csv\" by the file path of the downloaded file # Timing fread() for a large file system.time(data &lt;- fread(&quot;large_dataset.csv&quot;)) Add a new column into the data set. Just the convert the temperature from \\(^oC\\)(degrees Celcius) to \\(^oF\\)(degrees Farenheit) Write the file locally and check the time taken to write the file. Hint: Here is how you can achieve step iii abd iv # Do the Calculations # Timing fwrite() for a large file data[, new_column := rnorm(nrow(data))] # Add a new column system.time(fwrite(data, &quot;large_output.csv&quot;)) 8.1.4 Hands-On Exercises Download the Amazon Fine Food Reviews data set from here and work on the following problems; Remove any unwanted objects in the file system. Use gc to view any garbage collection statistics Read the data set using read.csv and fread and time each method. Compare the time taken for each function to read the file. Remove the data set from the memory using rm() and call gc() again Reread the data set however control the threads. Set different thread value (e.g 1, 2, 3, 4 and 5) the compare the time taken for processing. Write the file locally(use a different file name) and use gc() for garbage collection Finally, discuss on the importance of scalability when dealing with real-world data. 8.2 Data Cleaning and Transformation 8.2.1 Introduction to Data Cleaning 8.2.1.1 Common Data Issues Raw data from the field can be inaccurate and inconsistent and pose threats to data operations leading to false analysis and decision making. This type of data is know as dirty, unclean or rogue data. Here are some of the common data issues that render data unclean; Incomplete data: Some of the data points may be missing or left blank. Duplicate data: Some records in the data sets may occur multiple times in different sources. Outdated data: Some data values that may have relevant information times ago may now be obsolete and irrelevant. Inconsistent data formats: Data may be in different ways across multiple data sources like records be presented in JSON formats, SQL tables, No SQL table or even in image formats(.png, .jpeg). Outlier Values: Outliers are extremely high or low values in the data set like a data set containing age of students having one student that is 304 years old or body temperature data set containing values of \\(-51^0C\\). These values are always impossible and can indicate errors, anomalies and exceptions in the data sets. Data integrity is crucial in data analysis and can hinder the organizations’ operations. The foundation of strategic planning, efficient operations and sound decision-making is based upon accurate, complete and consistent data. Here are some of the threats that unclean data pose to data analytics and businesses in general; Inconsistent Results: For instance if a business finds conflicting or duplicate reports on customer behavior will lead to lose of trust or results’ interpretation. Increased Costs: Companies spend a lot of time and money on data cleaning, processing and verification Inaccurate Insights and Decision making: Poor data quality like incorrect values may lead to poor insights thereby leading to poor decision-making. For instance inaccurate sales data may lead to flawed forecasting, poor inventory management and poor marketing strategies. Missed Opportunities: Poor data quality may prevent the company from identifying key trends and opportunity gaps in the market like customer segments or emerging markets 8.2.1.2 Data Cleaning techniques 8.2.1.2.1 Handling Missing Data The presence of missing data values is inevitable especially when integrating data from different sources. While there is no specific set method(one-time solution) to handle the missing data points in R, researchers have come up with different methodologies to tackle this issue. Here are some of them; Deletion; this is a simple method that involves deleting all the records/rows that have null values. Otherwise, all rows that have null values in important columns can be deleted. na.omit() is one of the method. Imputation: There are packages in R that can fill the null values by imputation. They use the remaining values in the data to create a model that will find a value for the missing one. These packages include imputeR, Amelia and MissForest that use the automated variable selection, bootstrap multiple imputation and single imputation to handle null values. Use of algorithms that support null values for instance K-Nearest Neighbors(kNN) and Naive Bayes. Mean Imputation: This is simply filling the null values with the average of the remaining values. Lets create different random data sets and handle the missing values. However, before the issue of null value is fixed, the null values need to be identified in the data set. Create a random athlete data set set.seed(123) # Set seed for reproducibility # Generate a data frame with random data for athletes athlete_data &lt;- data.frame( athlete_id = 1:100, # 100 athletes name = paste0(&quot;Athlete&quot;, 1:100), age = sample(18:40, 100, replace = TRUE), sport = sample(c(&quot;Basketball&quot;, &quot;Soccer&quot;, &quot;Tennis&quot;, &quot;Swimming&quot;, &quot;Running&quot;), 100, replace = TRUE), country = sample(c(&quot;USA&quot;, &quot;Kenya&quot;, &quot;China&quot;, &quot;Brazil&quot;, &quot;Germany&quot;), 100, replace = TRUE), score = round(runif(100, 60, 100), 1) # Scores between 60 and 100 ) # Introduce 15% missing values randomly across the entire data frame total_cells &lt;- prod(dim(athlete_data)) # Total number of cells missing_cells &lt;- round(0.15 * total_cells) # 15% of cells will have missing values # Randomly select indices to be set to NA missing_indices &lt;- sample(seq_len(total_cells), missing_cells) # Convert the data frame to a matrix for easier manipulation of specific indices athlete_data_matrix &lt;- as.matrix(athlete_data) athlete_data_matrix[missing_indices] &lt;- NA # Convert back to a data frame athlete_data &lt;- as.data.frame(athlete_data_matrix) # View the dataset head(athlete_data) ## athlete_id name age sport country score ## 1 1 Athlete1 32 Basketball Germany 95.6 ## 2 2 Athlete2 36 Tennis &lt;NA&gt; 92.5 ## 3 3 Athlete3 &lt;NA&gt; &lt;NA&gt; USA 89.9 ## 4 4 Athlete4 20 &lt;NA&gt; Brazil 66.2 ## 5 5 &lt;NA&gt; 27 Tennis Kenya 65.0 ## 6 &lt;NA&gt; Athlete6 35 Running Germany 99.0 Lets count the all null values. The is.na() from base R is used to identify the null values while the sum() sums up all the identified null values sum(is.na(athlete_data)) ## [1] 90 Alternatively, the total null values from the columns can be counted. The sapply() function along with the is.na() and sum() are used. # Count the number of null values in each column null_counts &lt;- sapply(athlete_data, function(x) sum(is.na(x))) # Display the counts of null values null_counts ## athlete_id name age sport country score ## 17 18 10 14 18 13 Lets now delete the rows containing the null values by the na.omit() and rename the data frame athlete_data_clean athlete_data_clean &lt;- na.omit(athlete_data) Confirm the operation by counting the null values in each column and overall # Count the null values by column null_count_by_column &lt;- sapply(athlete_data_clean, function(x) sum(is.na(x))) null_count_by_column ## athlete_id name age sport country score ## 0 0 0 0 0 0 # Overall null count overall_null_count &lt;- sum(is.na(athlete_data_clean)) print(paste(&quot;Overall Null Count: &quot;, overall_null_count)) ## [1] &quot;Overall Null Count: 0&quot; Now that we have seen how null values can be handled by deleting the rows with the null values, therefore lets perform imputation on the original data set. The null values will not be deleted however, they will be replaced by filling the null values with the previous or next value We will be using the original athlete_data that has null values. The tidyr package will be used which can be installed by; # If not already installed run the command below on the console install.packages(&quot;tidyr&quot;) Count the null values once again and view sample of the data set # Count the null values by column null_count_by_column &lt;- sapply(athlete_data, function(x) sum(is.na(x))) null_count_by_column ## athlete_id name age sport country score ## 17 18 10 14 18 13 # View the sample of the data - first 6 records head(athlete_data) ## athlete_id name age sport country score ## 1 1 Athlete1 32 Basketball Germany 95.6 ## 2 2 Athlete2 36 Tennis &lt;NA&gt; 92.5 ## 3 3 Athlete3 &lt;NA&gt; &lt;NA&gt; USA 89.9 ## 4 4 Athlete4 20 &lt;NA&gt; Brazil 66.2 ## 5 5 &lt;NA&gt; 27 Tennis Kenya 65.0 ## 6 &lt;NA&gt; Athlete6 35 Running Germany 99.0 Now fill the null values with the previous values. Specificall the sport. athlete_id and the age columns # Import the package library(tidyr) # Fill the null value with the previous value athlete_data_filled &lt;- athlete_data %&gt;% fill(athlete_id, age, sport, .direction = &quot;down&quot;) # Count the null values by column null_count_by_column &lt;- sapply(athlete_data_filled, function(x) sum(is.na(x))) null_count_by_column ## athlete_id name age sport country score ## 0 18 0 0 18 13 # View the first few rows head(athlete_data_filled) ## athlete_id name age sport country score ## 1 1 Athlete1 32 Basketball Germany 95.6 ## 2 2 Athlete2 36 Tennis &lt;NA&gt; 92.5 ## 3 3 Athlete3 36 Tennis USA 89.9 ## 4 4 Athlete4 20 Tennis Brazil 66.2 ## 5 5 &lt;NA&gt; 27 Tennis Kenya 65.0 ## 6 5 Athlete6 35 Running Germany 99.0 You can see the target columns(age, athlete_id and sport) are filled and have zero null values. In the fill() function, there is an argument, .direction that specify the direction in which to fill the missing value. In this case the value of the argument is \"down\" because we are filling based on the previous value. It can be either \"up\", \"downup\"(based on previous the later) or \"updown\"(based on later then previous). Practical exercise Download the Detailed NFL Play-by-Play Data 2009-2018 data from here. Read the data set and subset to have Date , Drive, time, TimeUnder, TimeSecs columns. Name the data resultant data frame subset_df1. Perform the following operations on subset_df1. Count the total null values Create a copy of the data set and rename it subset_df1_copy. Drop rows that contain any null value. Create another copy, subset_df1_copy2 and impute the missing values based on the previous value. .direction=down. Count the null values on the copy data set 8.2.1.2.2 Dealing with Outliers Outliers are values that that are completely different from the other data points in the data set. They may be extremely smaller or extremely larger than the values in the data set. Also outliers can present some non-realistic values for instance a body temperature of \\(143^0C\\) or an athlete height of 0.23 meters. They are brought about by; Measurement error like the human error when collecting the data. When the sample picked does not accurately represent the population being studied for instance soil samples picked near a cow shed will have high levels of acidity. There might be a mixture of distribution brought about collecting data from two distinct populations. Nevertheless, outliers can indicate new behaviors in the data set, especially when the outliers values are the most recently collected data. When cleaning data, it is advisable to remove outliers as they can have a significant impact on the aftermath analysis for instance; skewed and biased results, misleading interpretations, increased range and standard deviation(gives false variability of the data), and erroneous assumptions. Let’s generate a random data set of health of villagers(like blood pressure, heart rate, body temperature etc) and use it to detect outliers within Generate a random data set # Set seed for reproducibility set.seed(123) # Create a data frame with health-related metrics for villagers health_data &lt;- data.frame( villager_id = 1:100, body_temperature = c(rnorm(95, mean = 98.6, sd = 1), rnorm(5, mean = 102, sd = 1)), # Include 5 outliers blood_pressure = c(rnorm(95, mean = 120, sd = 10), rnorm(5, mean = 180, sd = 5)), # Include 5 outliers heart_rate = c(rnorm(95, mean = 70, sd = 8), rnorm(5, mean = 110, sd = 5)), # Include 5 outliers cholesterol = c(rnorm(95, mean = 200, sd = 30), rnorm(5, mean = 300, sd = 10)) # Include 5 outliers ) # View the first few rows of the dataset head(health_data) ## villager_id body_temperature blood_pressure heart_rate cholesterol ## 1 1 98.03952 112.8959 87.59048 178.5427 ## 2 2 98.36982 122.5688 80.49930 177.4193 ## 3 3 100.15871 117.5331 67.87884 171.8438 ## 4 4 98.67051 116.5246 74.34555 168.4246 ## 5 5 98.72929 110.4838 66.68528 186.8852 ## 6 6 100.31506 119.5497 66.19002 209.9354 Find outliers using IQR(Interquartile Range) Any values larger than \\(1.5xIQR\\) added to the upper quartile or smaller than the \\(1.5xIQR\\) subtracted to the lower quartile. # Function to identify outliers using IQR identify_outliers_iqr &lt;- function(data_column) { Q1 &lt;- quantile(data_column, 0.25, na.rm = TRUE) Q3 &lt;- quantile(data_column, 0.75, na.rm = TRUE) IQR_value &lt;- IQR(data_column, na.rm = TRUE) lower_bound &lt;- Q1 - 1.5 * IQR_value upper_bound &lt;- Q3 + 1.5 * IQR_value return(data_column &lt; lower_bound | data_column &gt; upper_bound) } # Apply IQR method to identify outliers in each health metric health_data$outlier_body_temp_iqr &lt;- identify_outliers_iqr(health_data$body_temperature) health_data$outlier_blood_pressure_iqr &lt;- identify_outliers_iqr(health_data$blood_pressure) health_data$outlier_heart_rate_iqr &lt;- identify_outliers_iqr(health_data$heart_rate) health_data$outlier_cholesterol_iqr &lt;- identify_outliers_iqr(health_data$cholesterol) # View the data with outlier flags for body temperature health_data[health_data$outlier_body_temp_iqr, c(&quot;villager_id&quot;, &quot;body_temperature&quot;)] ## villager_id body_temperature ## 96 96 101.3997 ## 97 97 104.1873 ## 98 98 103.5326 ## 99 99 101.7643 Find the outliers using the z-score method. The Z-score method standardizes the data to have a mean of 0 and a standard deviation of 1. Outliers are values that have a z-score of greater than 3 or less than -3. # Function to identify outliers using the Z-score identify_outliers_zscore &lt;- function(data_column) { z_scores &lt;- scale(data_column) return(abs(z_scores) &gt; 3) } # Apply Z-score method to identify outliers in each health metric health_data$outlier_body_temp_zscore &lt;- identify_outliers_zscore(health_data$body_temperature) health_data$outlier_blood_pressure_zscore &lt;- identify_outliers_zscore(health_data$blood_pressure) health_data$outlier_heart_rate_zscore &lt;- identify_outliers_zscore(health_data$heart_rate) health_data$outlier_cholesterol_zscore &lt;- identify_outliers_zscore(health_data$cholesterol) # View the data with Z-score outlier flags for blood pressure health_data[health_data$outlier_blood_pressure_zscore, c(&quot;villager_id&quot;, &quot;blood_pressure&quot;)] ## villager_id blood_pressure ## 96 96 189.9861 ## 97 97 183.0035 ## 98 98 173.7436 ## 99 99 176.9442 ## 100 100 174.0726 Practical exercise Download the Brazil’s House of Deputies Reimbursements from here. Read the data set, identify numerical columns that contain the outliers and remove them 8.2.1.2.3 Data Transformation Techniques Normalization and Standardization Normalization and standardization are both data preprocessing techniques that adjust the range of input values to make them easier to analyze and process. They are often interchangeably however, they have different applications and techniques. To be specific; Normalization involves rescaling data within a specific range for instance between 0 and 1 where the maximum value will be 1 and the minimum be 0, all other values will range between zero and 1. Also, -1 and 1, 0 and 100 are among the most common ranges where statistician normalizes their data. This technique is useful where the distribution of the data is unknown or not normal but its likely to be affected by outliers due to its restricted range. On the contrary, Standardization involves rescaling the data values to have a mean of zero and a standard deviation of one. This technique is useful when the distribution of the data is known and normally distributed. It is less likely to be affected by outliers because it does not have a restricted range. Z-score is of the most popular method of standardization. Here is the formula to calculate z-score; \\[Z = {value-mean\\over{standard deviation}}\\] Lets normalize/standardize a vector v1, v1&lt;- c(1200,34567,3456,12,3456,0985,1211) in three easy steps Min-Max Scaling(normalization) The vector will be scaled in a range of 0 to 1 using a preProcess function from the caret package. # Load the library library(caret) ## Loading required package: lattice ## ## Attaching package: &#39;caret&#39; ## The following object is masked from &#39;package:purrr&#39;: ## ## lift v1 &lt;- c(1200,34567,3456,12,3456,0985,1211) # train the preprocesser(scaler) minMaxScaler &lt;- preProcess(as.data.frame(v1), method=c(&quot;range&quot;)) # Apply the scaler v1_scaled &lt;- predict(minMaxScaler, as.data.frame(v1)) v1_scaled ## v1 ## 1 0.03437997 ## 2 1.00000000 ## 3 0.09966720 ## 4 0.00000000 ## 5 0.09966720 ## 6 0.02815801 ## 7 0.03469831 Using the scale() function(standardization) # Create the vector v1 &lt;- c(1200,34567,3456,12,3456,0985,1211) v1_scaled &lt;- as.data.frame(scale(v1)) v1_scaled ## V1 ## 1 -0.4175944 ## 2 2.2556070 ## 3 -0.2368546 ## 4 -0.5127711 ## 5 -0.2368546 ## 6 -0.4348191 ## 7 -0.4167131 Log transform This is used especially when the data is unevenly distributed for instance being skewed by “stretching out” the tail of a right-skewed distribution making it more symmetric. # Create the vector v1 &lt;- c(1200,34567,3456,12,3456,0985,1211) log_scaled_v1 &lt;- log(as.data.frame(v1)) log_scaled_v1 ## v1 ## 1 7.090077 ## 2 10.450655 ## 3 8.147867 ## 4 2.484907 ## 5 8.147867 ## 6 6.892642 ## 7 7.099202 Encoding Categorical Variables Encoding is the process of converting categorical variables to numerical variables in data preprocessing. Machine learning models only understand numerical values, therefore encoding is a crucial step in data preprocessing. The most common methods are one-hot encoding and label encoding. We will discuss how to perform them in R One-hot encoding One hot encoding represents each category as a separate column and converts the categorical values into a 1 or 0 depending where the category is represented. This avoids the assumption of ordinality by the machine learning models. Lets perform one hot encoding on a color vector library(caret) # Sample categorical variable color_df &lt;- data.frame( Color = c(&quot;Red&quot;, &quot;Blue&quot;, &quot;Green&quot;, &quot;Red&quot;, &quot;Blue&quot;) ) # dummify the data color_dmy &lt;- dummyVars(&quot; ~ .&quot;, data = color_df) new_color_df &lt;- data.frame(predict(color_dmy, newdata = color_df)) new_color_df ## ColorBlue ColorGreen ColorRed ## 1 0 0 1 ## 2 1 0 0 ## 3 0 1 0 ## 4 0 0 1 ## 5 1 0 0 Alternatively you can use the model_matrix, a built-in R function # Using model.matrix() to one-hot encode color_df$Color &lt;- as.factor(color_df$Color) # Convert Color to a factor one_hot_encoded &lt;- model.matrix(~ Color - 1, color_df) # View the encoded matrix one_hot_encoded ## ColorBlue ColorGreen ColorRed ## 1 0 0 1 ## 2 1 0 0 ## 3 0 1 0 ## 4 0 0 1 ## 5 1 0 0 ## attr(,&quot;assign&quot;) ## [1] 1 1 1 ## attr(,&quot;contrasts&quot;) ## attr(,&quot;contrasts&quot;)$Color ## [1] &quot;contr.treatment&quot; One-hot encoding is preferred in cases where the categories are nominal for example colors, countries since it does not assume any order between the categories Label encoding This method assigns a unique integer to each category. This method is simpler but it may impose unintended ordinality. # Sample categorical variable color_df &lt;- data.frame( Color = c(&quot;Red&quot;, &quot;Blue&quot;, &quot;Green&quot;, &quot;Red&quot;, &quot;Blue&quot;) ) # Label encoding color_df$Color_encoded &lt;- as.numeric(as.factor(color_df$Color)) # View the label encoded data color_df ## Color Color_encoded ## 1 Red 3 ## 2 Blue 1 ## 3 Green 2 ## 4 Red 3 ## 5 Blue 1 Label encoding is preferred where the categories are ordinal for instance education levels, living standards, age groups etc. Categories that have a natural order Practical exercise Using the same Brazil’s House of Deputies data set, encode the deputy_state and the political_party to have numerical values. 8.2.2 Hands-On Exercises In this course you will be required to use the NHANES data set from the NHANES package. Install the NHANES package by; install.packages(&quot;NHANES&quot;) then import the library and the data set # Load the library library(NHANES) # Load the data set data(&quot;NHANES&quot;) thereafter you are required to answer the following questions; Count the total number of missing values in each column in the data set and find which column has the most null values sapply(NHANES, function(x) sum(is.na(x))) Remove the rows that have any missing values and store the results in a new data set For columns with numeric data, impute missing values by replacing them with the mean or median of the column For categorical columns (such as Gender or SmokeNow), impute missing values using the mode. For the BMI and Pulse columns, calculate the Interquartile Range (IQR) and dentify any data points that fall outside the 1.5 * IQR range (both lower and upper bounds) as outliers Remove rows where outliers are found in the BMI and Pulse columns Apply Min-Max normalization to the BMI and Pulse columns, rescaling the data between 0 and 1 Use one-hot encoding to convert the Gender column into a numerical format (i.e., create separate columns for male and female) "],["time-series-analysis.html", "Chapter 9 Time Series Analysis 9.1 Introduction to Time Series Data 9.2 Basic Time Series Concepts 9.3 Basic Time Series Forecasting 9.4 Hands-On Exercises", " Chapter 9 Time Series Analysis 9.1 Introduction to Time Series Data The time series data refers to a sequence of data points collected or recorded at regular time intervals. Each data has a specific time stamps and the data is always dependent on the previous time and after. The order of the rows doesn’t matter but the timestamp does, this is what is referred to as temporal ordering. Here are some of the distinct characteristics of time series data; Trend: the time series data tend to show long-term increase or decrease over a period of time. Temporal Dependence: In time series, the current data values are often influenced by the previous data values and may impact the future ones. Seasonality: some of the time series data often exhibit repeating patterns at regular intervals for instance daily, monthly and annually. Autocorrelation: current values can be correlated with future or previous time points. Stationarity: Time series is often stationary if its statistical properties for instance mean and variance remain constant over time. Time series has several applications in the industry, here are some of its applications; Forecasting; predicting future values based on the previous and current values. Anomaly detection; identify outliers or any unusual patterns over a certain period of time. Seasonality; Find and analyze recurring patterns. Trend Analysis; Identify trends or patterns over a certain period of time. Used in economic and financial analysis to predict economic indicators such as GDP, exchange rates and inflation rates. Measuring natural phenomena like measuring rainfall in weather forecasting. 9.2 Basic Time Series Concepts Components of Time Series The above graph represents an example of a time series data. To understand the underlying structure in time series, it is broken down into three components; trend, seasonality and noise. These components characterize the pattern and behavior of data over time. Trend; This will show the general direction of data whether it is upward(increasing) or downward(decreasing). They indicate long-term movement depicting overall growth or decline. The above chart shows that there was an overall growth(upward trend) over the year Seasonality; It is the predictable pattern that appear regularly. In the chart above there is a quarterly rise and drop of values. Cycles; represents the fluctuations that don’t have a fixed period. Noise; its is the residual variability of data that has no explanations by the factors affecting the trend. The variability is always small compared to the trend and cycle. Lets use the R inbuilt data set, AirPassengers to decompose the time series data into trend, seasonality … # Load the data data(&quot;AirPassengers&quot;) head(AirPassengers) ## [1] 112 118 132 129 121 135 # Decompose the air passengers time series decomposed_ts &lt;- decompose(AirPassengers) # Plotting will be done later # decomposed_ts # uncomment to show the data Practical exercise In this course, you will be required to download the amazon stock prices prediction data set from here Solution library(dplyr) # Load the data amazon_stocks &lt;- read.csv(&quot;data/amazon_trends.csv&quot;) # Ensure the data is ordered by date (if necessary) amazon_stocks &lt;- amazon_stocks %&gt;% arrange(Date) # Convert to time series object ts_data &lt;- ts(amazon_stocks$Google_Trends, frequency = 365) # Decompose the time series data decomposed_data &lt;- decompose(ts_data) ________________________________________________________________________________ Decompose the time series data set into trend, seasonal and residual components. Visualization of Time Series Data Visualization is a crucial step in the time series analysis process as it enables; the researcher to analyze the important concepts in the data such as trend, seasonality and noise the analyst to track perfomance over time to diagnose alien behaviors like sudden spikes and presence of outliers the analyst to communicate insights to the non-technical stake holders. Lets visualize the time series data of the AirPassengers. plot.ts(AirPassengers, main = &quot;AirPasengers Time Series &quot;, ylab = &quot;Passengers&quot;, xlab = &quot;Time&quot;, col = &quot;blue&quot;, lwd = 2) Lets now visualize the decomposed time series. plot(decomposed_ts) The number of Air Passengers has increased from 1950 to 1960. There is an upward trend. Now lets repeat the process using the ggplot2 library. # Load the library library(ggplot2) # Convert the air passengers to a dataframe df_airpassengers &lt;- data.frame( # Month = as.Date(time(AirPassengers)), # Extracting the time component Month = seq(from = as.Date(&quot;1949-01-31&quot;), to = as.Date(&quot;1960-12-31&quot;), by = &quot;month&quot;), Passengers = as.numeric(AirPassengers) # Extracting the passenger counts ) head(df_airpassengers) ## Month Passengers ## 1 1949-01-31 112 ## 2 1949-03-03 118 ## 3 1949-03-31 132 ## 4 1949-05-01 129 ## 5 1949-05-31 121 ## 6 1949-07-01 135 # Plot the data ggplot(df_airpassengers, aes(x = Month, y = Passengers)) + geom_line(color = &quot;blue&quot;, size = 1) + # Line for the time series data labs(title = &quot;Air Passengers Time Series Data&quot;, # Title x = &quot;Month&quot;, # X-axis label y = &quot;Passengers&quot;) + # Y-axis label theme_minimal() # Apply a minimal theme ## Warning: Using `size` aesthetic for lines was deprecated in ## ggplot2 3.4.0. ## ℹ Please use `linewidth` instead. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see ## where this warning was generated. Now lets visualize the decomposed time series (AirPassengers) data using the ggplot2 library; create a data frame from the decomposed data # Create a data frame with all the components df_decomposed &lt;- data.frame( Date = seq(from = as.Date(&quot;1949-01-31&quot;), to = as.Date(&quot;1960-12-31&quot;), by = &quot;month&quot;), Observed = as.numeric(AirPassengers), Trend = as.numeric(decomposed_ts$trend), Seasonal = as.numeric(decomposed_ts$seasonal), Residual = as.numeric(decomposed_ts$random) ) # Remove the null values df_decomposed &lt;- na.omit(df_decomposed) head(df_decomposed) ## Date Observed Trend Seasonal Residual ## 7 1949-07-31 148 126.7917 63.83081 -42.622475 ## 8 1949-08-31 148 127.2500 62.82323 -42.073232 ## 9 1949-10-01 136 127.9583 16.52020 -8.478535 ## 10 1949-10-31 119 128.5833 -20.64268 11.059343 ## 11 1949-12-01 104 129.0000 -53.59343 28.593434 ## 12 1949-12-31 118 129.7500 -28.61995 16.869949 Practical exercise Using the amazon stock prices prediction data set, plot the data to identify time series patterns and trends Solution library(dplyr) # Load the data amazon_stocks &lt;- read.csv(&quot;data/amazon_trends.csv&quot;) # Ensure the data is ordered by date (if necessary) amazon_stocks &lt;- amazon_stocks %&gt;% arrange(Date) # Convert to time series object ts_data &lt;- ts(amazon_stocks$Google_Trends, frequency = 365) # Decompose the time series data decomposed_data &lt;- decompose(ts_data) # Plot the decomposed data plot(decomposed_data) ________________________________________________________________________________ 9.3 Basic Time Series Forecasting 9.3.1 Moving Averages Moving average is a statistical calculations used to analyze values over a specific period of time. Its main focus is to smooth out the short term fluctuations that makes it easier to identify the underlying trends in the data. It smoothens the time series data by averaging the the values over a sliding window. There are two types of Moving Averages, namely; Simple Moving Averages(SMA) Weighted Moving Averages (WMA) Moving Average can also be referred to as the rolling mean. In this course we will demonstrate how to calculate and plot the Simple Moving Average using the rollmean() and filter() functions from the zoo and the Base R respectively. Load the required libraries # Load the libraries library(ggplot2) # for plotting library(zoo) # to access `rollmean` function Load the data and calulate the moving average using the rollmean() function. Since the data was collected in monthly basis, a yearly moving average will be calculated. # Load the AirPassengers dataset data(&quot;AirPassengers&quot;) # Calculate 12-month moving average using rollmean() from zoo package moving_avg &lt;- rollmean(AirPassengers, k = 12, # 12- Month Moving Average fill = NA) # Create a data frame combining the original data and moving average df_airpassengers &lt;- data.frame( Month = as.Date(time(AirPassengers)), Passengers = as.numeric(AirPassengers), MovingAvg = as.numeric(moving_avg) ) # View the first few rows of the data frame head(df_airpassengers, 10) ## Month Passengers MovingAvg ## 1 1949-01-01 112 NA ## 2 1949-02-01 118 NA ## 3 1949-03-01 132 NA ## 4 1949-04-01 129 NA ## 5 1949-05-01 121 NA ## 6 1949-06-01 135 126.6667 ## 7 1949-07-01 148 126.9167 ## 8 1949-08-01 148 127.5833 ## 9 1949-09-01 136 128.3333 ## 10 1949-10-01 119 128.8333 Now plot the time series data along with the moving average # Plot original data and moving average ggplot(df_airpassengers, aes(x = Month)) + geom_line(aes(y = Passengers), color = &quot;blue&quot;, size = 1) + # Original data geom_line(aes(y = MovingAvg), color = &quot;red&quot;, size = 1.2) + # Moving average labs(title = &quot;AirPassengers - Moving Average (12 months)&quot;, y = &quot;Number of Passengers&quot;, x = &quot;Month&quot;) + theme_minimal() ## Warning: Removed 11 rows containing missing values or values ## outside the scale range (`geom_line()`). The blue line represents the original time series data The red line represents the Moving Average data, you can see that the line is smoother. Let’s repeat the process but this time we use the filter() function from the Base R. Remember dplyr also has filter() function. To speficify use stats::filter(). # Calculate the Moving Average moving_avg_filter &lt;- stats::filter(AirPassengers, rep(1/12, 12), sides = 2) # Add the moving average to the existing data frame df_airpassengers$MovingAvg_Filter &lt;- as.numeric(moving_avg_filter) # Plot original data and moving average calculated by filter() ggplot(df_airpassengers, aes(x = Month)) + geom_line(aes(y = Passengers), color = &quot;blue&quot;, size = 1) + # Original data geom_line(aes(y = MovingAvg_Filter), color = &quot;green&quot;, size = 1.2) + # Moving average from filter labs(title = &quot;AirPassengers - Moving Average (12 months, filter())&quot;, y = &quot;Number of Passengers&quot;, x = &quot;Month&quot;) + theme_minimal() The green line represents the simple moving average over a 12-month sliding window Practical exercise Apply 30-day moving averages on the amazon stock. Solution library(dplyr) library(zoo) library(ggplot2) # Load the data amazon_stocks &lt;- read.csv(&quot;data/amazon_trends.csv&quot;) # Ensure the data is ordered by date (if necessary) amazon_stocks &lt;- amazon_stocks %&gt;% arrange(Date) # Calculate 30-day moving average (approximate for monthly) amazon_stocks &lt;- amazon_stocks %&gt;% mutate(moving_avg_30 = rollmean(Google_Trends, k = 30, fill = NA)) # Plot the original data and moving average ggplot(amazon_stocks, aes(x = Date)) + geom_line(aes(y = Google_Trends, color = &quot;Original&quot;), size = 1) + geom_line(aes(y = moving_avg_30, color = &quot;30-Day Moving Avg&quot;), size = 1) + labs(title = &quot;Stock Price with 30-Day Moving Average&quot;, x = &quot;Date&quot;, y = &quot;Stock Price&quot;) + scale_color_manual(values = c(&quot;Original&quot; = &quot;blue&quot;, &quot;30-Day Moving Avg&quot; = &quot;red&quot;)) + theme_minimal() + theme(legend.title = element_blank()) ## `geom_line()`: Each group consists of only one ## observation. ## ℹ Do you need to adjust the group aesthetic? ## Warning: Removed 29 rows containing missing values or values ## outside the scale range (`geom_line()`). ## `geom_line()`: Each group consists of only one ## observation. ## ℹ Do you need to adjust the group aesthetic? Work on it later- Copy the guide ________________________________________________________________________________ 9.3.2 ARIMA model ARIMA stands for AutoRegressive Integrated Moving Average that is defined in three parameters namely; p, d and q where; AR(p) Autoregression: This utilizes the relationship between the current values and the previous one where the current ones are dependent on the previous. I(d) Integration: It entails subtracting the current observations of a series with its previous values d number of times. This is done to make the time series stationary. MA(q) Moving Average: A model that uses the dependency between an observation and a residual error from a moving average model applied to lagged observations. A moving average component depicts the error of the model as a combination of previous error terms. The order q represents the number of terms to be included in the model In summary, p, d and q represents the number of autoregressive (AR) terms, the number of differences required to make the series stationary and the number of moving averages(MA) respectively. These values are determined using the following diagonstic tools; Autocorrelation Function(ACF): identifies the MA terms Partial Autocorrelation Function (PACF): identifies the number of AR terms. Differencing: determines the value of d need to make the series stationary. R has auto.arima() function from the forecast library designed to create the ARIMA model. The forecast library can be installed by; install.packages(&quot;forecast&quot;) Lets use the AirPassengers data to design our ARIMA model. Step 1: Check stationarity To create an ARIMA model the time series data, in this case the AirPassengers, must be stationary i.e have a constant mean and variance. Otherwise, differencing is applied to make it stationary. The diff() is used to this. # Load necessary library library(forecast) ## Registered S3 method overwritten by &#39;quantmod&#39;: ## method from ## as.zoo.data.frame zoo # Load the AirPassengers data set if not loaded already # Make the data stationary by differencing diff_passengers &lt;- diff(log(AirPassengers)) Step 2: Use the ACF and PACF Plots to Identify ARIMA Parameters The ACF is used to find the correlation between the time series and the lagged versions((previous) of itself. Thats the q (MA) parameter. The PACF finds the correlation between the time series and the lagged versions of itself but after removing the effect of the intermediate lags. Thats the p (AR) parameter Step 3: Plot the ACF and PACF The acf() and pacf functions are used to generate the plots. # Plot the ACF and PACF of the differenced data par(mfrow = c(1, 2)) # Set up for side-by-side plots # ACF plot acf(diff_passengers, main = &quot;ACF of Differenced AirPassengers&quot;) # PACF plot pacf(diff_passengers, main = &quot;PACF of Differenced AirPassengers&quot;) &lt;-Interpret the plots-&gt; The AirPassengers data set is not stationary, therefore the data will be transformed by log-transform before applying differencing # Log transformation to stabilize variance log_passengers &lt;- log(AirPassengers) # Difference the log-transformed data to make it stationary diff_passengers &lt;- diff(log_passengers) # Plot the differenced series to check stationarity plot(diff_passengers, main = &quot;Differenced Log of AirPassengers&quot;, ylab = &quot;Differenced Log(Passengers)&quot;, xlab = &quot;Time&quot;) Step 4: Fit the ARIMA model The auto.arima() function can be used to automate the process of selecting the p, d, q parameters. # Auto-identify ARIMA parameters and fit the model fit_arima &lt;- auto.arima(log_passengers) # Display the summary of the fitted ARIMA model summary(fit_arima) ## Series: log_passengers ## ARIMA(0,1,1)(0,1,1)[12] ## ## Coefficients: ## ma1 sma1 ## -0.4018 -0.5569 ## s.e. 0.0896 0.0731 ## ## sigma^2 = 0.001371: log likelihood = 244.7 ## AIC=-483.4 AICc=-483.21 BIC=-474.77 ## ## Training set error measures: ## ME RMSE MAE MPE MAPE MASE ## Training set 0.0005730622 0.03504883 0.02626034 0.01098898 0.4752815 0.2169522 ## ACF1 ## Training set 0.01443892 Lets fit the ARIMA model with of order 1 1 1 to represent p d q respectively. # Fit ARIMA(1,1,1) model fit_manual_arima &lt;- arima(log_passengers, order = c(1, 1, 1)) # Display the summary of the fitted ARIMA model summary(fit_manual_arima) ## ## Call: ## arima(x = log_passengers, order = c(1, 1, 1)) ## ## Coefficients: ## ar1 ma1 ## -0.5780 0.8482 ## s.e. 0.1296 0.0865 ## ## sigma^2 estimated as 0.01027: log likelihood = 124.31, aic = -242.63 ## ## Training set error measures: ## ME RMSE MAE MPE MAPE MASE ## Training set 0.008294478 0.1009741 0.08585615 0.1401158 1.557282 0.9477905 ## ACF1 ## Training set 0.010984 Step 5: Forecast with the ARIMA model The fitted ARIMA model can now be used for forecasting the future values using the arima() function. # Forecast for the next 24 months (2 years) forecast_arima &lt;- forecast(fit_arima, h = 24) # Plot the forecast plot(forecast_arima, main = &quot;ARIMA Forecast for AirPassengers&quot;, xlab = &quot;Time&quot;, ylab = &quot;Log(Passengers)&quot;) The blue part of the plot represents the forecasted values. Practical exercise Fit the ARIMA model to the Amazon Stock Prices Prediction data and interpret the results. 9.4 Hands-On Exercises You will be required to download the Electricity Price &amp; Demand 2018-2023 data set from here Perform time series on the total demand on one of the csv files provided from visualization to forecasting "],["capstone-project.html", "Chapter 10 Capstone Project 10.1 Introduction 10.2 Project Planning 10.3 Project Execution 10.4 Project Presentations and Feedback", " Chapter 10 Capstone Project 10.1 Introduction 10.1.1 Project Overview and Objectives The main objective of this capstone project is to allow the student apply all the skills and techniques learned through the camp in a comprehensive real-world analysis. It greatly emphasizes on teamwork, critical thinking and creativity 10.1.2 Group Formation Participants are divided into 5 groups, each consisting of 5 members. Each group is assigned a different data set. These are the proposed data sets and their links to download them: Healthcare Data set https://www.kaggle.com/datasets/imtkaggleteam/mental-health?select=1-+mental-illnesses-prevalence.csv Retail Sale data https://www.kaggle.com/datasets/manjeetsingh/retaildataset Financial Transaction Data https://www.kaggle.com/datasets/apoorvwatsky/bank-transaction-data https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud Social Media Data https://www.kaggle.com/datasets/datasnaek/youtube-new?select=USvideos.csv https://www.kaggle.com/datasets/datasnaek/youtube-new?select=GBvideos.csv Environmental Data set https://www.kaggle.com/datasets/sevgisarac/temperature-change 10.2 Project Planning 10.2.1 Defining the problem statement Using your assigned data set, you will be required to; define a clear problem statement to address. narrow down on the scope of the project to make it manageable within the given timeframe outline your project goals and the expected outcomes 10.2.2 Planning the Analysis WorkFlow In your groups, you are required to plan the analysis including: Data Cleaning and Preprocessing You will identify potential issues for instance missing values and outliers ,then plan how to address them Explanatory Data Analysis Plan how to conduct the initial exploration of the data set to understand the data structure and key patterns Model Selection and Application Discuss on the potential statistical or machine learning models that could be applied to the data Evaluation and Interpretation Plan how to evaluate the model’s performance and interpret the results. 10.3 Project Execution 10.3.1 Part 1 10.3.1.1 Data Cleaning and Preprocessing The groups will begin by cleaning and preparing the data sets including; Handling missing values Correcting inconsistencies in the data set Ensuring data is ready for analysis The instructirs will provide support and guidance as needed, answering questions and help in troubleshooting issues 10.3.1.2 Explanatory Data Analysis(EDA) The groups will then; Perform EDA to uncover insights, identify trends and explore the relationships between variables Visualize the data using ggplot2 or any other tool to better understand the data. Document their findings and decide on the next step in the analysis 10.3.2 Part 2 10.3.2.1 Model Building and Analysis Each group is required to; Select and apply an appropriate statistical or machine learning model to address their research questions Train, test and fine-tune the model as needed. Emphasize on the interpreting the model outputs, understanding the implications and ensuring the results align with the project goals 10.3.2.2 Evaluation and Insights The groups will then; evaluate the performance of their models using relevant metrics (e.g., accuracy, RMSE, precision, recall). discuss on the insights gained from the analysis and how these insights could be applied in a real-world context. prepare visualizations and summary reports to effectively communicate their findings. 10.4 Project Presentations and Feedback Each group will finally present their project that will include; An overview of the data set and the problem statement. The steps taken during data cleaning, EDA, and model building. The key findings, model performance, and final insights. Visualizations and any actionable recommendations based on the analysis. "],["datasets.html", "Chapter 11 Datasets", " Chapter 11 Datasets tweets Dataset from the rtweet Package “Credit Card Fraud” dataset from R package creditcard The “Groceries” from the R package comes arules “pima” from MASS package airquality (find the package) “NHANES” from “NHANES” package Bengaluru Restaurants dataset - https://www.kaggle.com/datasets/mrmars1010/restaurants-dataset-bengaluru Car Sales data set - https://github.com/balsaedi/R_programming/blob/main/data/car_sales.csv Furniture Sales Data set - https://www.kaggle.com/datasets/rajagrawal7089/furniture-sales-data Boston Housing Data set - https://www.kaggle.com/datasets/fedesoriano/the-boston-houseprice-data Amazon Stock Price Prediction - https://www.kaggle.com/datasets/aenes95/amazon-stock-price-prediction Electricity Price &amp; Demand 2018-2023 - https://www.kaggle.com/datasets/joebeachcapital/nsw-australia-electricity-demand-2018-2023/data Groundhog Day Forecasts and Temperatures - https://www.kaggle.com/datasets/groundhogclub/groundhog-day Amazon Fine Food Reviews data - https://www.kaggle.com/datasets/snap/amazon-fine-food-reviews Detailed NFL Play-by-Play Data 2009-2018 - https://www.kaggle.com/datasets/maxhorowitz/nflplaybyplay2009to2016 Brazil’s House of Deputies Reimbursements - https://www.kaggle.com/datasets/epattaro/brazils-house-of-deputies-reimbursements "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
