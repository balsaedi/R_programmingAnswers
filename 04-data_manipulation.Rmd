# Data Manipulation
## Basic Data Manipulation 
### Introduction to Data Manipulation
Data Manipulation is the adjusting, organizing and transforming of the raw data is not a more useful and suitable format for data analysis. These are some of the reasons that make data manipulation mandatory in the data analysis process; 

1. **Improves the data quality**  
Raw data may be incomplete, messy, containing irrelevant information, errors ,or duplicates that need to be cleaned and rectified. This will ensure the data is reliable thereby preventing incorrect conclusions or decisions. 

2. **Making Data Usable** 
Sometimes data is collected from different sources that is not ready for analysis. Data Manipulation will transform the data into a structured and consistent format for easy analysis. 

3. **Enhancing Data Exploration**  
By cleaning the data, analysts explore the data thereby understanding different concepts of the data. 

4. **Enabling Complex Analysis**  
Some types of analysis require data to be in specific format or structure, for instance the time series analysis require data to be sorted out by date. 

5. **Supporting Decision Making**  
Data Manipulation ensures that the data that is fed into the system is timely, accurate and reliable for informed decision-making models and relevant reports
<br>

These are the key tasks in the data manipulation;

- **Cleaning**: by removing inaccurate and incomplete data entries.
- **Filtering** the data by selecting certain rows or columns based on a certain criteria. 
- **Reshaping**: Changing the structure of the data for instance pivoting.
- **Merging**: Combine multiple data sets into one. 
- **Transforming**: Modify existing data by mathematical or logical operations. 
- **Aggregation**: Summarizing the data by performing operations like sum,average and count. 

### Subsetting and Filtering Data:

**Subsetting** is a data management strategy that involves creating a coherent slice data from different data set for specific use cases. This topic will better be explained practically, therefore we will use the titanic data set. The data set contains information about the passengers on the Titanic, including their age, gender, passenger class, whether they survived and other details. Since the titanic dataset is absent in baseR, the  `titanic` library will be installed by;
```
install.packages("titanic")
```

load the library 
```{r}
library("titanic")
```

The data set will indexed using different indexing techniques such as indexing of a single element, row and column indexing. 

First we load the data set and view the first few records before indexing
```{r}
data("titanic_train")

titanic <- titanic_train 
head(titanic) # view the first few rows of the titanic data set
```

Subsetting by indexing can be done in three different ways that include 

i. Extracting a Row
ii. Extracting a column
iii. Extracting a Single Element


1. **Extract a row**  

When subsetting to extract data for a single row the square brackets `[ ]` will be used with the position of the index you want to extract. Lets extract all the information of the 10th passenger.
```{r}
titanic[10, ] # note the comma after the index 10
```
Also, more indices can be subsetted in the format `[i:j, ]` where the `i` is the starting index while `j` is the ending index respectively. Lets extract the information by subsetting the titanic data from index 7 to 10. 
```{r}
titanic[7:10, ]
```
2. **Extract a column** 

When subsetting to extract data for a single column the square brackets `[ ]` will be used as before, with the position of the index or column name you want to extract. Lets extract all the information of the column `Name`.
```
titanic[, "Name"] # note the comma before "Name"
```
![](images/dataset_images/titanic_names.png)

An index of the column can be used in place of the column name. For instance, the column, "PassengerId" is the first column therefore its index will be `1`. Lets subset the column by calling the index.
```
titanic[, 1] # note the comma before the column index
```
![](images/dataset_images/titanic_passengers_id.png)

3. **Extracting a single element**  

A single element that has a defined position in a data frame, both the row index and the column name/index are called. 
```
dataframe[row_index, column index/name]
```

Lets extract the age of the Name of the fifth passenger. 
```{r}
titanic[5, "Name"]
```

Instead of using the column name. Lets use the column index. In the above context, the column "Name" appears at index(is the fourth column). 
```{r}
titanic[5, 4]
```


Subsetting a data set can be done by **filtering** data based on logical conditions to extract rows that meet certain criteria. They involve comparisons operators such as `>, <, ==, !=` or logical operators like `&`(and), `|`(or), `!` (not). 

In this titanic data set we:-

1. **Filter based on a single condition**


Lets find the passengers who survived on the titanic.
```{r}
survivors <- titanic[titanic$Survived == 1, ]
head(survivors) # view the first few rows of survivors
```
The above data set consists of titanic passengers who survived.

Who were the passengers who boarded the first class on the Titan?
```{r}
first_class_passengers <- titanic[titanic$Pclass == 1, ]
head(first_class_passengers)
```
The above examples, the extracted data set met a single condition. 

2. **Filtering based on Multiple Conditions**


Data can be subsetted by filtering based on more than one condition. To demonstrate this, lets find the female passengers who survived. Here there are two conditions;-

- the passenger must be a female,
- the passenger must have survived. 

The resultant data set must meet the above conditions
```{r}
female_survivors <- titanic[titanic$Sex == "female" & titanic$Survived == 1, ]

head(female_survivors) #view the first few rows 
```
Lets also add one more condition, the female survivor must be under 18. 
```{r}
minor_female_survivors <- titanic[titanic$Sex == "female" & 
                                    titanic$Survived == 1 &
                                    titanic$Age < 18, ] # comma should be after the conditons

head(minor_female_survivors)
```

3. **Filtering using Negation**  

The `!=` sign a logical operator that is used to negate a condition. Lets use it to find the passengers who did not survive. 
```{r}
non_survivors <- titanic[titanic$Survived != 1, ]
tail(non_survivors) # view the last few records 
```
Alternatively you can use 
```
non_survivors <- titanic[!titanic$Survived == 1, ]
```

Also, lets find the passengers who were not in the third class 
```{r}
not_third_class <- titanic[titanic$Pclass != 3, ]
head(not_third_class)
```

Alternatively
```
not_third_class <- titanic[!titanic$Pclass == 3, ]
```

### Sorting Data
Sorting is the ordering of elements in a data set (vectors, lists, matrix and data frames) based on a particular criteria. This is a fundamental operation data analysis, as it enables data organization in a meaningful way for easier visualization and interpretation. These are the several functions in Base R that are used in sorting;-

1. `sort()`  

- Lets create a vector v with five elements
- Sort the elements in a descending order

```{r}
v = c(43, 82, 11, 73, 34) # Create a vector
v1 = sort(v, decreasing = TRUE) #sort the elements in a descending order
v1
```

- to order the same vector in an ascending order the `decreasing` argument is set to `FALSE`. 
```{r}
v = c(43, 82, 11, 73, 34) # Create a vector
v2 = sort(v, decreasing = FALSE) #sort the elements in an ascending order
v2
```

- Also character vectors can be sorted in alphabetical order for instance lets sort the the names, `"Alice", "Charlie", "Bob"` in the alphabetical order. 
```{r}
names <- c("Alice", "Charlie", "Bob")
sorted_names <- sort(names)
sorted_names
```

- Alternatively, the names can be ordered in the reverse alphabetical order when the `decreasing` argument is set to `TRUE`.
```{r}
names <- c("Alice", "Charlie", "Bob", "Zach")
names_1 <- sort(names, decreasing = TRUE) # order in reverse alphabetical order
names_1
```

2. `order()`  

This function returns the indices that would sort the vectors. For instance lets sort the vector `v = c(43, 82, 11, 73, 34)` in an ascending order(from smallest to the largest). The smallest number in this case is 11, therefore, the `order()` function will return 1 while 82 is the largest(5th smallest) number in this case, it will be returned as 5.  

```{r}
v = c(43, 82, 11, 73, 34)
order(v, decreasing = FALSE)
```

Lets repeat the process but this time we order the indices of the vector `v` in a descending order
```{r}
v = c(43, 82, 11, 73, 34)
order(v, decreasing = TRUE)
```
You can see that index `2` which has a value `82` on the original vector comes first while index `3` which has a value of `11` comes last

The default value for the `decreasing` argument is `FALSE`.
```{r}
v = c(43, 82, 11, 73, 34)
order(v) # no `decreasing` argument
```

3. `rank()`

Returns of the rank of the element in a vector, list. The smallest element is ranked as 1(in this case its 11) while largest element is ranked last(82 is ranked 5 here)

```{r}
v = c(43, 82, 11, 73, 34)
rank(v, ties.method = "average", na.last = TRUE)
```

4. `rev()`

This function simply reverse the order of elements. The first element in a vector will be last while the last one will be first. 

```{r}
v = c(43, 82, 11, 73, 34)
rev(v)
```

5. **Sorting Data Frames**

A data frame can be sorted in descending/ascending order of a certain column. For instance, we will sort the titanic data set in the order of age in ascending order.

```{r}
titanic_by_age <- titanic[order(titanic$Age), ]
head(titanic_by_age)
```

Sorting the titanic data by age in descending order, `-` will be added infront of argument `titanic$Age` to be `-titanic$Age`

```{r}
titanic_by_age <- titanic[order(-titanic$Age), ] # note the - sign
head(titanic_by_age)
```

Also, dataframes can be sorted based by multiple columns. Lets sort the titanic data set by Passenger class (`Pclass`) in ascending order and by age in descending order at once.

```{r}
# Pclass in ascending order, Age in descending order
titanic_sorted_by_class_and_age <- titanic[order(titanic$Pclass, -titanic$Age), ]

head(titanic_sorted_by_class_and_age)
```

### Basic Data Cleaning
**Data Cleaning** is the process of fixing, removing incorrect, incomplete or otherwise problematic data from a data set. This is a crucial step in data analysis as it leads to more reliable analyses and insights. 

Here some data cleaning techniques used; 

1. **Handling Missing Data**  

The null values are identified by `is.na()` function. If there exists null values, they are removed by `na.omit()` function. The null values can also be replaced by appropriate substitutes  such as the mean, median, zero value or a placeholder, this is referred to as imputation.  
Lets create a vector `myvector` will null values, identify the null values, remove/impute them. 
```{r}
# Create a vector that has missing values
my_vector <- c(12, 43, NA, 32, 65, 11, NA, NA, 34, 98, 57) # NA is the missing value

# Identify existence of the null values
is.na(my_vector)

# Count the null values 
sum(is.na(my_vector))

# remove null values 
clean_vector <- na.omit(my_vector)
clean_vector

# impute missing values
my_vector[is.na(my_vector)] <- mean(my_vector, na.rm = TRUE)
my_vector
```

2. **Removing Duplicates**

In a raw data set there may exist some duplicated entries/records/rows that will give false results in data analysis thereby leading to poor insights and decision-making. The duplicates are identified by `duplicated()` function. If there exists any duplicates, they are removed by subsetting or calling  `unique()` function.  

Lets create a data frame with duplicate values and remove them 

```{r}
# Creating a simple data frame
data <- data.frame(
  Position = c(1, 2, 3, 4, 5, 3),  # Notice the duplicate position
  Name = c("Alice", "Bob", "Charlie", "David", "Eva", "Charlie"),
  Age = c(25, 30, NA, 40, 29, NA),  # NA represents null values
  Score = c(85, 90, 88, NA, 92, 88)  # NA represents null values
) # Position 3 is duplicated

# Display the data frame
print(data)

print("CLEAN DATA")

# Remove the duplicate row
clean_data <- unique(data)
clean_data
```

Also the duplicated rows can be removed by 
```{r}
clean_data2 <- data[!duplicated(data), ]
clean_data2
```

3. **Handling Outliers**

Outliers are extreme values that differ from most other data points in a data set. hey can be detected by identifying the upper bound and lower bound using boxplots. They are removed by;
```
clean_data <- raw_data[raw_data$column < upper_bound & raw_data$column > lower_bound, ]
```

4. **Standardizing Data**

There are numerous ways that data can be standardized such ensuring a consistent date format across the data set and correcting case. 

Lets create a vector with different date formats and make it consistent
```{r}
# Creating a vector with different date formats
date_vector <- c("2023-08-01", "01/08/2023", "August 1, 2023", "20230801", "08-01-2023")

# Converting the date_vector to a consistent format
clean_date_vector <- as.Date(date_vector, format="%Y-%m-%d")

# Display the clean date vector
print(clean_date_vector)
```

The `lubridate` package can do it better
```{r}
# Load necessary library
library(lubridate)

# Correcting each date format using lubridate functions
clean_date_vector <- c(
  as.Date(date_vector[1], format="%Y-%m-%d"),       # "2023-08-01"
  as.Date(date_vector[2], format="%d/%m/%Y"),       # "01/08/2023"
  as.Date(date_vector[3], format="%B %d, %Y"),      # "August 1, 2023"
  as.Date(date_vector[4], format="%Y%m%d"),         # "20230801"
  as.Date(date_vector[5], format="%m-%d-%Y")        # "08-01-2023"
)

# Display the clean date vector
print(clean_date_vector)

```


<span style="color: green">**Practical exercise**</span>

You will use the car sales data set found [here](https://github.com/balsaedi/R_programming/blob/main/data/car_sales.csv) to;

i. Delete all the records that have any null value
ii. Remove duplicated observations

_______________________________________________________________________
<span style="color: brown;">**Solution**</span> 

You will use the car sales data set found [here](https://github.com/balsaedi/R_programming/blob/main/data/car_sales.csv) to;

i. Delete all the records that have any null value
```{r}
# Read the data sets 
car_sales <- read.csv("data/car_sales.csv") # file path can be the whole link
# i. Delete all the records that have any null value
car_sales_1 <- na.omit(car_sales)
```

ii. Remove duplicated observations
```{r}
# ii. Remove duplicated observations
car_sales_2 <- car_sales_1[!duplicated(car_sales_1), ]
```

<span style="color: brown;">**________________________________________________________________________________**</span>

### Hands-on Exercises
Download the Bengaluru Restaurants Dataset from [here](https://www.kaggle.com/datasets/mrmars1010/restaurants-dataset-bengaluru) and use it to for the activities below. 

1. **Subsetting and Filtering**

- Subset the data set to include only restaurants with a rating of 4.0 and above.
- Filter out restaurants that have zero reviews or null ratings.

2. **Sorting**

- Sort the data set by restaurant ratings in descending order.
- Sort the data set by the number of reviews a restaurant received, with the highest reviews appearing last

3. **Data Cleaning**

- Identify and remove rows with missing or null values in any key columns (e.g., ratings, cost, cuisine type).

_______________________________________________________________________
<span style="color: brown;">**Solution**</span> 

Download the Bengaluru Restaurants Dataset from [here](https://www.kaggle.com/datasets/mrmars1010/restaurants-dataset-bengaluru) and use it to for the activities below. 

1. **Subsetting and Filtering**

- Subset the data set to include only restaurants with a rating of 4.0 and above.
```{r}
df <- read.csv("data/Bengaluru_Restaurants.csv")
# Subsetting and Filtering
## Subset the data set to include only restaurants with a rating of 4.0 and above.
high_rated_restaurants <- df[df$rating >= 4.0, ]

head(high_rated_restaurants[, c("name", "rating") ]) # show the restaurants 
```

- Filter out restaurants that have zero reviews or null ratings.
```{r}
## Filter out restaurants that have no reviews or ratings.
# Filter out restaurants with no reviews or ratings
filtered_restaurants <- df[df$numberOfReviews>0 & !is.na(df$rating), ]

head(filtered_restaurants[, c("name", "rating", "numberOfReviews") ])
```

2. **Sorting**

- Sort the data set by restaurant ratings in descending order.
```{r}
# Sort the restaurants by ratings in descending order
sorted_restaurants <- df[order(-df$rating), ]

# Display the sorted data
head(sorted_restaurants[, c("name", "rating")])
```

- Sort the data set by the number of reviews a restaurant received, with the highest reviews appearing last.
```{r}
# Sort the restaurants by no of reviews in ascending order
sorted_restaurants <- df[order(df$numberOfReviews), ]

# Display the sorted data
tail(sorted_restaurants[, c("name", "numberOfReviews")])
```

3. **Data Cleaning**

- Identify and remove rows with missing or null values in any key columns (e.g., ratings, cost, cuisine type).
```{r}
sum(is.na(df)) # count the null values

# Count the null values 
complete_data <- na.omit(df)

# Confirm the operation
sum(is.na(complete_data))
```

<span style="color: brown;">**________________________________________________________________________________**</span>

## Data Manipulation with Dplyr
### Introduction to Dplyr package 
Dplyr is a package designed for data manipulation equipped with a set of intuitive functions to perform tasks like filtering rows, selecting columns, rearranging data and summarizing information. The package is part of a larger library, `tidyverse`. The **tidyverse** package is a package designed for data science that share an underlying design philosophy, grammar and data structures. The packages within the tidyverse are widely used for data manipulation, exploration, and visualization in R. Here are some of the core packages in tidyverse; 

- `ggplot2`
- `dplyr`
- `tidyr`
- `readr`
- `purrr`
- `tibble`

The tidyverse package is installed by 
```
install.packages("tidyverse")
```

To invoke the package into the system, the below command is invoked
```{r}
library(tidyverse)
```
In this course, we will discuss on `dplyr` since it is an essential tool in data analysis. If you want to use dplyr alone then it can be installed by; 
```
install.packages("dplyr")
```

To load the library into the system;
```{r}
library(dplyr)
```

### Key Functions in dplyr
There are some functions that are use by data scientist when working with `dplyr`, they are referred to as dplyr verbs. To explain these verbs better, we will use an example data set to explain. A good example is the famous iris data set, it is always used by beginners in data science. The data set contains measurements of various characteristics of iris flowers. These characteristics include sepal length, sepal width, petal length, and petal width. There are three species of iris flowers in the data set: setosa, versicolor, and virginica. 
The data will be invoked to R before assessment and wrangling;

Lets load the iris data and explore the first few records. 
```{r}
data("iris")
head(iris)
```

### select

This dplyr verb is used when selecting or dropping specific columns. In this lesson we will find the iris column names and select two of them using `select`. 
```{r}
data(iris)

# Find the column names 
colnames(iris)
```

Remember the data frame to work on need to be specified in the arguments such that 

```
selected_data = select(data_frame, col1, col2, col3)
```

Therefore, we will select the columns; Species, Petal length and petal width. 

```{r}
# Load the required libraries 
library(dplyr)

# Load the iris data 
data(iris)

selected_iris_data = select(iris,
                       Petal.Length, Petal.Width, Species)

# view the first few rows of the selected data
head(selected_iris_data)
```
The three selected columns are displayed in the data frame above. 

Specific columns can be dropped by putting `-` before the column name as
```
# Drop specified columns 
remaining_data = select(data_frame, -col1_to_drop, -col2_to_drop)
```
In this lesson, we will drop petal length, petal width and Species columns;
```{r}
# Load the required libraries 
library(dplyr)

# Load the iris data 
data(iris)

# Drop some columns 
remaining_iris_data = select(iris,
                       -Petal.Length, -Petal.Width, -Species)

# view the first few rows of the selected data
head(remaining_iris_data)
```

<span style="color: green;">**Practical exercise**</span>

You will be required to use the `car_sales` data set from https://raw.githubusercontent.com/insaid2018/Term-1/master/Data/Projects/car_sales.csv. Read the data using `read.csv` and select the `car, price, body, mileage, engV, engType, year, model`. Save the data frame from the selected columns as `selected_cars_df`. Show the first few rows of the `selected_cars_df`. 

_______________________________________________________________________
<span style="color: brown;">**Solution**</span> 

```{r}
library(dplyr)

# Load the data 
filepath = "https://raw.githubusercontent.com/insaid2018/Term-1/master/Data/Projects/car_sales.csv"
car_sales <- read.csv(filepath)

# select the required columns 
selected_cars_df <- select(car_sales, 
                           car, price, body, mileage, engV, engType, year, model # mention the columns
                           )
# Show the first few rows of the data set 
head(selected_cars_df)
```

<span style="color: brown;">**________________________________________________________________________________**</span>

### filter
Is a verb/function from `dplyr` used to filter records in a data frame based on a specific condition. It allows the analyst to retrieve the records he/she is interested in and work easier with the subset. 
With `filter()`, the data frame and the condition are passed as a arguments;
```
# Filtering rows where a certain column meets a condition
filtered_data = filter(data_frame, 
column_name > 5 # This is the condition)
```

Lets select the species 'setosa' from the iris data set 
```{r}
# Load the required libraries 
library(dplyr)

# Load the iris data 
data(iris)

# Filter to select Setosa 
setosa_iris = filter(iris, # the data frame
                     Species == "setosa" # the condition
                     )
# First few records of setosa data
head(setosa_iris)
```

Records with sepal width of more than 3.0 can be filtered. Here is how we achieve such a subset
```{r}
# Load the required libraries 
library(dplyr)

# Load the iris data 
data(iris)

# Filtered to select records with more than 3.0 sepal width
wide_sepal_iris = filter(iris, #the data frame
                         Sepal.Width>3.0 # the condition
                         )

head(wide_sepal_iris)
```

<span style="color: green;">**Practical exercise**</span>

With the `car_sales` data set that you used above, use `filter()` function to get the cars that were sold from the year 2008 to present and name them `latest_car_sales`. Count the number of observations made and show the first few rows. 

_______________________________________________________________________
<span style="color: brown;">**Solution**</span> 

```{r}
library(dplyr)

# Load the data 
filepath = "https://raw.githubusercontent.com/insaid2018/Term-1/master/Data/Projects/car_sales.csv"
car_sales <- read.csv(filepath)

# Filter to find cars sold from 2008
latest_car_sales = filter(car_sales, 
                          year>=2008)

# Count the observations made. Use nrow function
nrow(latest_car_sales)

# Show the first few rows
head(latest_car_sales)
```

<span style="color: brown;">**________________________________________________________________________________**</span>

### arrange 
This is `dplyr` verb/function used for sorting rows by rearranging in a specific order. here is how to use `arrange()` function;
```
arranged_data = arrange(data_frame, column_name)
```
This allows the analyst to arrange the data in a default ascending order. To arrange in a descending order a `desc()` function is added as;
```
# Note the additional desc function
arranged_data = arrange(data_frame, desc(column_name))
```

Now lets order the iris data in an ascending order based on Petal length and view the first 6 records with the shortest petal. 
```{r}
# Load the required libraries 
library(dplyr)

# Load the iris data 
data(iris)

# Sort the data 
by_petal_length = arrange(iris, # data frame 
                          Petal.Length # order by column
                          )

# View the data 
head(by_petal_length)
```

Lets repeat the same process but now we order the data in a descending order.
```{r}
# Sort the data 
by_petal_length = arrange(iris, # data frame 
                          desc(Petal.Length) # order by column
                          )

# View the data 
head(by_petal_length)
```

<span style="color: green;">**Practical exercise**</span>

Arrange the columns in the `car_sales` data set according to `mileage` in descending order. Show the last few rows

_______________________________________________________________________
<span style="color: brown;">**Solution**</span> 

```{r}
library(dplyr)

# Load the data 
filepath = "https://raw.githubusercontent.com/insaid2018/Term-1/master/Data/Projects/car_sales.csv"
car_sales <- read.csv(filepath)

# Order according to mileage in descending order
mileage_order = arrange(car_sales, 
                        desc(mileage))

# Show the last few rows of the data set
tail(mileage_order)
```

<span style="color: brown;">**________________________________________________________________________________**</span>

### mutate
`mutate()` is a dplyr verb used to modifying the existing variables or creating new variables in a data set. 
In this case we can calculate the log off Sepal length in the iris data 
```{r}
# Load the required libraries 
library(dplyr)

# Load the iris data 
data(iris)

# modify Sepal.Length
new_iris = mutate(iris, Sepal.Length=log(Sepal.Length))

head(new_iris)
```

Additionally, we can create an entirely new variable by `mutate()`. In this case we will find the ratio between petal length and petal width. The new variable will be called "Petal.Length.Width.Ratio"
```{r}
# Load the required libraries 
library(dplyr)

# Load the iris data 
data(iris)

# Create a new column in the data set 
new_iris = mutate(iris,
                  Petal.Length.Width.Ratio = Petal.Length/Petal.Width)

head(new_iris)
```

The "Petal.Length.Width.Ratio" is found by dividing the Petal.Length and the Petal.Width variables. 

<span style="color: green;">**Practical exercise**</span>

Using the `car_sales` data set, create a new column, `"distance_covered_km"`, calculated from the `mileage`. Just multiply `mileage` with `1.609`. Show the first few rows of the mutated data frame. 

_______________________________________________________________________
<span style="color: brown;">**Solution**</span> 

```{r}
library(dplyr)

# Load the data 
filepath = "https://raw.githubusercontent.com/insaid2018/Term-1/master/Data/Projects/car_sales.csv"
car_sales <- read.csv(filepath)

# Create new column "distance_covered_km
mutated_car_sales = mutate(car_sales,
                           distance_covered_km = mileage * 1.609)

# Show first few rows 
head(mutated_car_sales)
```

<span style="color: brown;">**________________________________________________________________________________**</span>

### Summarise
To calculate summary statistics such as average, median and maximum the `summarise()` is used. This function collapses multiple rows into a summary row. For instance calculating the mean Petal width;
```{r}
# Load the required libraries 
library(dplyr)

# Load the iris data 
data(iris)

# Calculate the mean petal width
summarise(iris, 
          mean_petal_width=mean(Petal.Width))
```

To find the mean petal width for each iris species;

- the iris data will be grouped by species
- a mean value for each group will be calculated
```{r}
# Load the required libraries 
library(dplyr)

# Load the iris data 
data(iris)

# To find the mean petal width for each iris species;
# - the iris data will be grouped by species
# - a mean value for each group will be calculated
grouped_iris = group_by(iris, Species)
mean_petal_widths = summarise(grouped_iris, mean_value=mean(Petal.Width))
mean_petal_widths
```

<span style="color: green;">**Practical exercise**</span>

You will be required to use the `car_sales` data set once again. Calculate the descriptive statistics using `summarise()` command. 

_______________________________________________________________________
<span style="color: brown;">**Solution**</span> 

```{r}
library(dplyr)

# Load the data 
filepath = "https://raw.githubusercontent.com/insaid2018/Term-1/master/Data/Projects/car_sales.csv"
car_sales <- read.csv(filepath)

# Calculate the summary statistics
summarise(car_sales)
```

### group_by 
The `group_by()` is a function used to group records in a data frame by one or more variables. It allows the analyst to create a group based on a certain criteria. It is always chained together with `summarise()`. 

Lets group the iris data based on the Species variable and find the mean of each variable;
```{r}
# Load the required libraries 
library(dplyr)

# Load the iris data 
data(iris)

# Group the iris based on their Species 
iris_groups = group_by(iris, Species) %>%
  summarise(across(everything(), mean, na.rm = TRUE))
head(iris_groups)
```

This groupings allow the analyst to retrieve insights at more base level and uncover more insights that could not have been possible when analyzing the entire data set

<span style="color: green;">**Practical exercise**</span>

Use the `car_sales` data set provided before to work on this activity. Load the data and group the sales by `model` to get the sum of every quantitative feature/variable. Name the resultant data frame, `car_sales_by_model`. Display the resultant data frame. **Hint**: Use `across(where(is.numeric), sum)` as an argument to summarise instead of `across(everything(), mean, na.rm = TRUE)` to find the sum of quantitative variables. 

_______________________________________________________________________
<span style="color: brown;">**Solution**</span> 

```{r}
library(dplyr)

# Load the data 
filepath = "https://raw.githubusercontent.com/insaid2018/Term-1/master/Data/Projects/car_sales.csv"
car_sales <- read.csv(filepath)

# Group the sales by model
car_sales_by_model = group_by(car_sales, model)%>%
  summarise(across(where(is.numeric), sum))

# Print out the dataframe 
car_sales_by_model
```

<span style="color: brown;">**________________________________________________________________________________**</span>

## Chaining
Chaining is the process of combining several operations together using the `%>%` or forward pipe operator. The chained workflow succeeds each other until the whole process is done. To understand chaining, the mtcars(Motor Trend cars) data set will be used. Mtcars is also a well-known data set containing several attributes of 32 different cars from 1974. Here's a brief explanation of the variables in the mtcars data set:

1. `mpg`: Miles per gallon (fuel efficiency).
2. `cyl`: Number of cylinders.
3. `disp`: Displacement (cubic inches).
4. `hp`: Horsepower.
5. `drat`: Rear axle ratio.
6. `wt`: Weight (in 1000 lbs).
7. `qsec`: Quarter mile time (in seconds).
8. `vs`: Engine type (0 = V-shaped, 1 = Straight).
9. `am`: Transmission type (0 = Automatic, 1 = Manual).
10. `gear`: Number of forward gears.
11. `carb`: Number of carburetors.

Lets start by loading the data into the program and view its first few records;
```{r}
data(mtcars)
head(mtcars)
```

Lets `select` 6 most important columns in this analysis
```{r}
# Load the library
library(dplyr)

# Load the data
data(mtcars)

# Lets `select` 6 most important columns in this analysis
cars1 = mtcars %>% select(mpg, cyl, disp, hp, qsec, am)

head(cars1)
```

Lets now `filter` to find vehicles with an automatic transmission type. The `filter` verb will be chained to `select` verb with `%>%`. 
```{r}
# Load the library
library(dplyr)

# Load the data
data(mtcars)

# Selct and filter chained together
cars2 = mtcars %>%select(mpg, cyl, disp, hp, qsec, am) %>%
  filter(am==0)
head(cars2)
```

All these vehicles are of automatic transmission type, lets rank them according to the horsepower in descending order. 
```{r}
# Load the library
library(dplyr)

# Load the data
data(mtcars)

# Select, filter and arrange chained together
cars3= mtcars %>%select(mpg, cyl, disp, hp, qsec, am, wt) %>%
  filter(am==0) %>%
  arrange(desc(hp))

head(cars3)
```

A new column of weight in 1000kgs (wt_1000kgs) can be created by diving weight in 1000lbs by 2.20462. `mutate` verb will be chained also.
```{r}
# Load the library
library(dplyr)

# Load the data
data(mtcars)

# Multiple chains 
cars4= mtcars %>%select(mpg, cyl, disp, hp, qsec, am, wt) %>%
  filter(am==0) %>%
  arrange(desc(hp)) %>%
  mutate(wt_1000kgs=wt/2.20462)

head(cars4)
```

The above process has explained how chained works in `dplyr`. Many functions/processed can be chained together to manipulate data to the desired output. The next section will apply chaining to biology and be used to answer a few questions that will cement your understanding in R as a biologist. 

### Hands-on Exercises
You will be required to download the Furniture Sales Dataset from [here](https://www.kaggle.com/datasets/rajagrawal7089/furniture-sales-data). Use the data set to answer the questions below;  
1. Perform the following tasks using a combination of `select()`, `filter()`, `arrange()`, `mutate()`, and `group_by()`.

  i. Select the columns `"category"`, `"sales"`, and `"profit_margin"`. Filter the data set to include only orders with a `"sales"` amount greater than 25, and arrange the data by `"profit_margin"` in descending order.
  ii. Group the data by `"category"` and calculate the total `sales` and `profit_margin` for each category. Arrange the results by total sales in ascending order.  
<br>
2. Answer the following questions by chaining two or three `dplyr` verbs:

  i. What store type had products made of fabric material sold? 
  ii.  What is the average `"profit margin"` for each `"category"` of products?

_______________________________________________________________________
<span style="color: brown;">**Solution**</span> 

Load the data and libraries
```{r}
library(dplyr)

# Load the data 
furniture_sales <- read.csv("data/Furniture.csv")
```
1. Perform the following tasks using a combination of `select()`, `filter()`, `arrange()`, `mutate()`, and `group_by()`.

i. Select the columns `"category"`, `"sales"`, and `"profit_margin"`. Filter the data set to include only orders with a `"sales"` amount greater than 25, and arrange the data by `"profit_margin"` in descending order.
```{r}
high_sales <- furniture_sales %>% 
  select(category, sales, profit_margin)%>%
  filter(sales > 25)%>%
  arrange(desc(profit_margin))
```

ii. Group the data by `"category"` and calculate the total `sales` and `profit_margin` for each category. Arrange the results by total sales in ascending order.  
```{r}
sales_by_category <- furniture_sales %>%
  group_by(category) %>%
  summarise(total_sales = sum(sales), total_profit_margin = sum(profit_margin)) %>%
  arrange(desc(total_sales))

# View the result
print(sales_by_category)
```

2. Answer the following questions by chaining two or three dplyr verbs:

i. What store type had products made of fabric material sold?
```{r}
fabric_stores <- furniture_sales %>%
  select(store_type, material) %>% # select relevant columns
  filter(material=="Fabric")

unique(fabric_stores$store_type)
```
 Both the Online and Retail stores had products made of Fabric materials 
 
ii. What is the average `"profit margin"` for each `"category"` of products?
```{r}
profit_margin_by_category <- furniture_sales %>% 
  select(category, profit_margin) %>% # Select relevant columns
  group_by(category) %>%
  summarise(total_profit_margin = sum(profit_margin))

profit_margin_by_category
```

<span style="color: brown;">**________________________________________________________________________________**</span>